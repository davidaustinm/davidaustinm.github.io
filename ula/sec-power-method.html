<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2022-08-08T13:56:13-04:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US">
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Finding eigenvectors numerically</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="Understanding Linear Algebra">
<meta property="book:author" content=" David Austin ">
<script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script><script>window.MathJax = {
  tex: {
    inlineMath: [['\\(','\\)']],
    tags: "none",
    tagSide: "right",
    tagIndent: ".8em",
    packages: {'[+]': ['base', 'extpfeil', 'ams', 'amscd', 'newcommand', 'knowl']}
  },
  options: {
    ignoreHtmlClass: "tex2jax_ignore|ignore-math",
    processHtmlClass: "process-math",
    renderActions: {
        findScript: [10, function (doc) {
            document.querySelectorAll('script[type^="math/tex"]').forEach(function(node) {
                var display = !!node.type.match(/; *mode=display/);
                var math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                var text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = {node: text, delim: '', n: 0};
                math.end = {node: text, delim: '', n: 0};
                doc.math.push(math);
            });
        }, '']
    },
  },
  chtml: {
    scale: 0.88,
    mtextInheritFont: true
  },
  loader: {
    load: ['input/asciimath', '[tex]/extpfeil', '[tex]/amscd', '[tex]/newcommand', '[pretext]/mathjaxknowl3.js'],
    paths: {pretext: "https://pretextbook.org/js/lib"},
  },
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><script>// Make *any* pre with class 'sagecell-sage' an executable Sage cell
// Their results will be linked, only within language type
sagecell.makeSagecell({inputLocation: 'pre.sagecell-sage',
                       linked: true,
                       languages: ['sage'],
                       evalButtonText: 'Evaluate (Sage)'});
</script><script async="" src="https://cse.google.com/cse.js?cx=015103900096539427448:ngwuia10qci"></script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.13/pretext.js"></script><script>miniversion=0.674</script><script src="https://pretextbook.org/js/0.13/pretext_add_on.js?x=1"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/banner_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/toc_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/knowls_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/style_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/colors_blue_grey.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/setcolors.css" rel="stylesheet" type="text/css">
<!--** eBookCongig is necessary to configure interactive       **-->
<!--** Runestone components to run locally in reader's browser **-->
<!--** No external communication:                              **-->
<!--**     log level is 0, Runestone Services are disabled     **-->
<script type="text/javascript">
eBookConfig = {};
eBookConfig.useRunestoneServices = false;
eBookConfig.host = 'http://127.0.0.1:8000';
eBookConfig.course = 'PTX Course: Title Here';
eBookConfig.basecourse = 'PTX Base Course';
eBookConfig.isLoggedIn = false;
eBookConfig.email = '';
eBookConfig.isInstructor = false;
eBookConfig.logLevel = 0;
eBookConfig.username = '';
eBookConfig.readings = null;
eBookConfig.activities = null;
eBookConfig.downloadsEnabled = false;
eBookConfig.allow_pairs = false;
eBookConfig.enableScratchAC = false;
eBookConfig.build_info = "";
eBookConfig.python3 = null;
eBookConfig.acDefaultLanguage = 'python';
eBookConfig.runestone_version = '5.0.1';
eBookConfig.jobehost = '';
eBookConfig.proxyuri_runs = '';
eBookConfig.proxyuri_files = '';
eBookConfig.enable_chatcodes =  false;
</script>
<!--*** Runestone Services ***-->
<script type="text/javascript" src="https://runestone.academy/cdn/runestone/6.2.1/runtime.b0f8547c48f16a9f.bundle.js"></script><script type="text/javascript" src="https://runestone.academy/cdn/runestone/6.2.1/637.d54be67956c5c660.bundle.js"></script><script type="text/javascript" src="https://runestone.academy/cdn/runestone/6.2.1/runestone.0e9550fe42760516.bundle.js"></script><link rel="stylesheet" type="text/css" href="https://runestone.academy/cdn/runestone/6.2.1/637.fafafbd97df8a0d1.css">
<link rel="stylesheet" type="text/css" href="https://runestone.academy/cdn/runestone/6.2.1/runestone.e4d5592da655219f.css">
</head>
<body class="pretext-book ignore-math has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\newcommand{\avec}{{\mathbf a}}
\newcommand{\bvec}{{\mathbf b}}
\newcommand{\cvec}{{\mathbf c}}
\newcommand{\dvec}{{\mathbf d}}
\newcommand{\dtil}{\widetilde{\mathbf d}}
\newcommand{\evec}{{\mathbf e}}
\newcommand{\fvec}{{\mathbf f}}
\newcommand{\nvec}{{\mathbf n}}
\newcommand{\pvec}{{\mathbf p}}
\newcommand{\qvec}{{\mathbf q}}
\newcommand{\svec}{{\mathbf s}}
\newcommand{\tvec}{{\mathbf t}}
\newcommand{\uvec}{{\mathbf u}}
\newcommand{\vvec}{{\mathbf v}}
\newcommand{\wvec}{{\mathbf w}}
\newcommand{\xvec}{{\mathbf x}}
\newcommand{\yvec}{{\mathbf y}}
\newcommand{\zvec}{{\mathbf z}}
\newcommand{\rvec}{{\mathbf r}}
\newcommand{\mvec}{{\mathbf m}}
\newcommand{\zerovec}{{\mathbf 0}}
\newcommand{\onevec}{{\mathbf 1}}
\newcommand{\real}{{\mathbb R}}
\newcommand{\twovec}[2]{\left[\begin{array}{r}#1 \\ #2
\end{array}\right]}
\newcommand{\ctwovec}[2]{\left[\begin{array}{c}#1 \\ #2
\end{array}\right]}
\newcommand{\threevec}[3]{\left[\begin{array}{r}#1 \\ #2 \\ #3
\end{array}\right]}
\newcommand{\cthreevec}[3]{\left[\begin{array}{c}#1 \\ #2 \\ #3
\end{array}\right]}
\newcommand{\fourvec}[4]{\left[\begin{array}{r}#1 \\ #2 \\ #3 \\ #4
\end{array}\right]}
\newcommand{\cfourvec}[4]{\left[\begin{array}{c}#1 \\ #2 \\ #3 \\ #4
\end{array}\right]}
\newcommand{\fivevec}[5]{\left[\begin{array}{r}#1 \\ #2 \\ #3 \\
#4 \\ #5 \\ \end{array}\right]}
\newcommand{\cfivevec}[5]{\left[\begin{array}{c}#1 \\ #2 \\ #3 \\
#4 \\ #5 \\ \end{array}\right]}
\newcommand{\mattwo}[4]{\left[\begin{array}{rr}#1 \amp #2 \\ #3 \amp #4 \\ \end{array}\right]}
\newcommand{\laspan}[1]{\text{Span}\{#1\}}
\newcommand{\bcal}{{\cal B}}
\newcommand{\ccal}{{\cal C}}
\newcommand{\scal}{{\cal S}}
\newcommand{\wcal}{{\cal W}}
\newcommand{\ecal}{{\cal E}}
\newcommand{\coords}[2]{\left\{#1\right\}_{#2}}
\newcommand{\gray}[1]{\color{gray}{#1}}
\newcommand{\lgray}[1]{\color{lightgray}{#1}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\row}{\text{Row}}
\newcommand{\col}{\text{Col}}
\renewcommand{\row}{\text{Row}}
\newcommand{\nul}{\text{Nul}}
\newcommand{\var}{\text{Var}}
\newcommand{\corr}{\text{corr}}
\newcommand{\len}[1]{\left|#1\right|}
\newcommand{\bbar}{\overline{\bvec}}
\newcommand{\bhat}{\widehat{\bvec}}
\newcommand{\bperp}{\bvec^\perp}
\newcommand{\xhat}{\widehat{\xvec}}
\newcommand{\vhat}{\widehat{\vvec}}
\newcommand{\uhat}{\widehat{\uvec}}
\newcommand{\what}{\widehat{\wvec}}
\newcommand{\Sighat}{\widehat{\Sigma}}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\definecolor{fillinmathshade}{gray}{0.9}
\newcommand{\fillinmath}[1]{\mathchoice{\colorbox{fillinmathshade}{$\displaystyle     \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\textstyle        \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptstyle      \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptscriptstyle\phantom{\,#1\,}$}}}
\)</span></div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="ula.html"><span class="title">Understanding Linear Algebra</span></a></h1>
<p class="byline">David Austin</p>
</div>
<div class="searchwrapper" role="search"><div class="gcse-search"></div></div>
</div></div>
<nav id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3">
<a class="index-button toolbar-item button" href="index-1.html" title="Index">Index</a><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="sec-gaussian-revisited.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="chap5.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="chap6.html" title="Next">Next</a></span>
</div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="sec-gaussian-revisited.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="chap5.html" title="Up">Up</a><a class="next-button button toolbar-item" href="chap6.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link frontmatter">
<a href="frontmatter.html" data-scroll="frontmatter" class="internal"><span class="title">Front Matter</span></a><ul>
<li><a href="dedication-1.html" data-scroll="dedication-1" class="internal">Dedication</a></li>
<li><a href="colophon-1.html" data-scroll="colophon-1" class="internal">Colophon</a></li>
<li><a href="preface-1.html" data-scroll="preface-1" class="internal">Our goals</a></li>
</ul>
</li>
<li class="link">
<a href="chap1.html" data-scroll="chap1" class="internal"><span class="codenumber">1</span> <span class="title">Systems of equations</span></a><ul>
<li><a href="sec-expect.html" data-scroll="sec-expect" class="internal">What can we expect</a></li>
<li><a href="sec-finding-solutions.html" data-scroll="sec-finding-solutions" class="internal">Finding solutions to linear systems</a></li>
<li><a href="sec-sage-introduction.html" data-scroll="sec-sage-introduction" class="internal">Computation with Sage</a></li>
<li><a href="sec-pivots.html" data-scroll="sec-pivots" class="internal">Pivots and their influence on solution spaces</a></li>
</ul>
</li>
<li class="link">
<a href="chap2.html" data-scroll="chap2" class="internal"><span class="codenumber">2</span> <span class="title">Vectors, matrices, and linear combinations</span></a><ul>
<li><a href="sec-vectors-lin-combs.html" data-scroll="sec-vectors-lin-combs" class="internal">Vectors and linear combinations</a></li>
<li><a href="sec-matrices-lin-combs.html" data-scroll="sec-matrices-lin-combs" class="internal">Matrix multiplication and linear combinations</a></li>
<li><a href="sec-span.html" data-scroll="sec-span" class="internal">The span of a set of vectors</a></li>
<li><a href="sec-linear-dep.html" data-scroll="sec-linear-dep" class="internal">Linear independence</a></li>
<li><a href="sec-linear-trans.html" data-scroll="sec-linear-trans" class="internal">Matrix transformations</a></li>
<li><a href="sec-transforms-geom.html" data-scroll="sec-transforms-geom" class="internal">The geometry of matrix transformations</a></li>
</ul>
</li>
<li class="link">
<a href="chap3.html" data-scroll="chap3" class="internal"><span class="codenumber">3</span> <span class="title">Invertibility, bases, and coordinate systems</span></a><ul>
<li><a href="sec-matrix-inverse.html" data-scroll="sec-matrix-inverse" class="internal">Invertibility</a></li>
<li><a href="sec-bases.html" data-scroll="sec-bases" class="internal">Bases and coordinate systems</a></li>
<li><a href="sec-jpeg.html" data-scroll="sec-jpeg" class="internal">Image compression</a></li>
<li><a href="sec-determinants.html" data-scroll="sec-determinants" class="internal">Determinants</a></li>
<li><a href="sec-subspaces.html" data-scroll="sec-subspaces" class="internal">Subspaces</a></li>
</ul>
</li>
<li class="link">
<a href="chap4.html" data-scroll="chap4" class="internal"><span class="codenumber">4</span> <span class="title">Eigenvalues and eigenvectors</span></a><ul>
<li><a href="sec-eigen-intro.html" data-scroll="sec-eigen-intro" class="internal">An introduction to eigenvalues and eigenvectors</a></li>
<li><a href="sec-eigen-find.html" data-scroll="sec-eigen-find" class="internal">Finding eigenvalues and eigenvectors</a></li>
<li><a href="sec-eigen-diag.html" data-scroll="sec-eigen-diag" class="internal">Diagonalization, similarity, and powers of a matrix</a></li>
<li><a href="sec-dynamical.html" data-scroll="sec-dynamical" class="internal">Dynamical systems</a></li>
<li><a href="sec-stochastic.html" data-scroll="sec-stochastic" class="internal">Markov chains and Google's PageRank algorithm</a></li>
</ul>
</li>
<li class="link">
<a href="chap5.html" data-scroll="chap5" class="internal"><span class="codenumber">5</span> <span class="title">Linear algebra and computing</span></a><ul>
<li><a href="sec-gaussian-revisited.html" data-scroll="sec-gaussian-revisited" class="internal">Gaussian elimination revisited</a></li>
<li><a href="sec-power-method.html" data-scroll="sec-power-method" class="active">Finding eigenvectors numerically</a></li>
</ul>
</li>
<li class="link">
<a href="chap6.html" data-scroll="chap6" class="internal"><span class="codenumber">6</span> <span class="title">Orthogonality and Least Squares</span></a><ul>
<li><a href="sec-dot-product.html" data-scroll="sec-dot-product" class="internal">The dot product</a></li>
<li><a href="sec-transpose.html" data-scroll="sec-transpose" class="internal">Orthogonal complements and the matrix transpose</a></li>
<li><a href="sec-orthogonal-bases.html" data-scroll="sec-orthogonal-bases" class="internal">Orthogonal bases and projections</a></li>
<li><a href="sec-gram-schmidt.html" data-scroll="sec-gram-schmidt" class="internal">Finding orthogonal bases</a></li>
<li><a href="sec-least-squares.html" data-scroll="sec-least-squares" class="internal">Orthogonal least squares</a></li>
</ul>
</li>
<li class="link">
<a href="chap7.html" data-scroll="chap7" class="internal"><span class="codenumber">7</span> <span class="title">The Spectral Theorem and singular value decompositions</span></a><ul>
<li><a href="sec-symmetric-matrices.html" data-scroll="sec-symmetric-matrices" class="internal">Symmetric matrices and variance</a></li>
<li><a href="sec-quadratic-forms.html" data-scroll="sec-quadratic-forms" class="internal">Quadratic forms</a></li>
<li><a href="sec-pca.html" data-scroll="sec-pca" class="internal">Principal Component Analysis</a></li>
<li><a href="sec-svd-intro.html" data-scroll="sec-svd-intro" class="internal">Singular Value Decompositions</a></li>
<li><a href="sec-svd-uses.html" data-scroll="sec-svd-uses" class="internal">Using Singular Value Decompositions</a></li>
</ul>
</li>
<li class="link backmatter"><a href="backmatter.html" data-scroll="backmatter" class="internal"><span class="title">Back Matter</span></a></li>
<li class="link"><a href="app-sage-reference.html" data-scroll="app-sage-reference" class="internal"><span class="codenumber">A</span> <span class="title">Sage Reference</span></a></li>
<li class="link"><a href="index-1.html" data-scroll="index-1" class="internal"><span class="title">Index</span></a></li>
<li class="link"><a href="colophon-2.html" data-scroll="colophon-2" class="internal"><span class="title">Colophon</span></a></li>
</ul></nav><div class="extras"><nav><a class="pretext-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content"><section class="section" id="sec-power-method"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">5.2</span> <span class="title">Finding eigenvectors numerically</span>
</h2>
<section class="introduction" id="introduction-26"><p id="p-5267">We have typically found eigenvalues of a square matrix <span class="process-math">\(A\)</span> as the roots of the characteristic polynomial <span class="process-math">\(\det(A-\lambda I) = 0\)</span> and the associated eigenvectors as the null space <span class="process-math">\(\nul(A-\lambda I)\text{.}\)</span>  Unfortunately, this approach is not practical when we are working with large matrices. First, finding the charactertic polynomial of a large matrix requires considerable computation, as does finding the roots of that polynomial.  Second, finding the null space of a singular matrix is plagued by numerical problems, as we will see in the preview activity.</p>
<p id="p-5268">For this reason, we will explore a technique called the <em class="emphasis">power method</em> that finds numerical approximations to the eigenvalues and eigenvectors of a square matrix.</p>
<article class="exploration project-like" id="exploration-20"><h3 class="heading">
<span class="type">Preview Activity</span><span class="space"> </span><span class="codenumber">5.2.1</span><span class="period">.</span>
</h3>
<p id="p-5269">Let's recall some earlier observations about eigenvalues and eigenvectors.</p>
<ol class="lower-alpha">
<li id="li-3609"><p id="p-5270">How are the eigenvalues and associated eigenvectors of <span class="process-math">\(A\)</span> related to those of <span class="process-math">\(A^{-1}\text{?}\)</span></p></li>
<li id="li-3610"><p id="p-5271">How are the eigenvalues and associated eigenvectors of <span class="process-math">\(A\)</span> related to those of <span class="process-math">\(A-3I\text{?}\)</span></p></li>
<li id="li-3611"><p id="p-5272">If <span class="process-math">\(\lambda\)</span> is an eigenvalue of <span class="process-math">\(A\text{,}\)</span> what can we say about the pivot positions of <span class="process-math">\(A-\lambda
I\text{?}\)</span></p></li>
<li id="li-3612"><p id="p-5273">Suppose that <span class="process-math">\(A = \left[\begin{array}{rr}
0.8 \amp 0.4 \\
0.2 \amp 0.6 \\
\end{array}\right]
\text{.}\)</span>  Explain how we know that <span class="process-math">\(1\)</span> is an eigenvalue of <span class="process-math">\(A\)</span> and then explain why the following Sage computation is incorrect. <pre class="ptx-sagecell sagecell-sage" id="sage-141"><script type="text/x-sage">A = matrix(2,2,[0.8, 0.4, 0.2, 0.6])
I = matrix(2,2,[1, 0, 0, 1])	  
(A-I).rref()
</script></pre></p></li>
<li id="li-3613"><p id="p-5274">Suppose that <span class="process-math">\(\xvec_0 = \twovec{1}{0}\text{,}\)</span> and we define a sequence <span class="process-math">\(\xvec_{k+1} = A\xvec_k\text{;}\)</span>  in other words, <span class="process-math">\(\xvec_{k} = A^k\xvec_0\text{.}\)</span>  What happens to <span class="process-math">\(\xvec_k\)</span> as <span class="process-math">\(k\)</span> grows increasingly large?</p></li>
<li id="li-3614"><p id="p-5275">Explain how the eigenvalues of <span class="process-math">\(A\)</span> are responsible for the behavior noted in the previous question.</p></li>
</ol></article></section><section class="subsection" id="subsection-79"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">5.2.1</span> <span class="title">The power method</span>
</h3>
<p id="p-5283">Our goal is to find a technique that produces numerical approximations to the eigenvalues and associated eigenvectors of a matrix <span class="process-math">\(A\text{.}\)</span>  We begin by searching for the eigenvalue having the largest absolute value, which is called the <em class="emphasis">dominant</em> eigenvalue.  The next two examples demonstrate this technique. </p>
<article class="example example-like" id="example-50"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">5.2.1</span><span class="period">.</span>
</h4>
<p id="p-5284">Let's begin with the positive stochastic matrix <span class="process-math">\(A=\left[\begin{array}{rr}
0.7 \amp 0.6 \\
0.3 \amp 0.4 \\
\end{array}\right]
\text{.}\)</span> We spent quite a bit of time studying this type of matrix in <a href="sec-stochastic.html" class="internal" title="Section 4.5: Markov chains and Google's PageRank algorithm">Section 4.5</a>; in particular, we saw that any Markov chain will converge to the unique steady state vector.  Let's rephrase this statement in terms of the eigenvectors of <span class="process-math">\(A\text{.}\)</span></p>
<p id="p-5285">This matrix has eigenvalues <span class="process-math">\(\lambda_1 = 1\)</span> and <span class="process-math">\(\lambda_2 =0.1\)</span> so the dominant eigenvalue is <span class="process-math">\(\lambda_1 = 1\text{.}\)</span> The associated eigenvectors are <span class="process-math">\(\vvec_1 =
\twovec{2}{1}\)</span> and <span class="process-math">\(\vvec_2 = \twovec{-1}{1}\text{.}\)</span> Suppose we begin with the vector</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\xvec_0 = \twovec{1}{0} = \frac13 \vvec_1 - \frac13 \vvec_2
\end{equation*}
</div>
<p class="continuation">and find</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\begin{aligned}
\xvec_1 \amp {}={} A\xvec_0 = \frac13 \vvec_1 - \frac13(0.1)
\vvec_2 \\
\xvec_2 \amp {}={} A^2\xvec_0 = \frac13 \vvec_1 - \frac13(0.1)^2
\vvec_2 \\
\xvec_3 \amp {}={} A^3\xvec_0 = \frac13 \vvec_1 - \frac13(0.1)^3
\vvec_2 \\
\amp \vdots \\
\xvec_k \amp {}={} A^k\xvec_0 = \frac13 \vvec_1 - \frac13(0.1)^k
\vvec_2 \\
\end{aligned}
\end{equation*}
</div>
<p class="continuation">and so forth.  Notice that the powers <span class="process-math">\(0.1^k\)</span> become increasingly small as <span class="process-math">\(k\)</span> grows so that <span class="process-math">\(\xvec_k\approx
\frac13\vvec_1\)</span> when <span class="process-math">\(k\)</span> is large.  Therefore, the vectors <span class="process-math">\(\xvec_k\)</span> become increasingly close to a vector in the eigenspace <span class="process-math">\(E_1\text{,}\)</span> the eigenspace associated to the dominant eigenvalue.  If we did not know the eigenvector <span class="process-math">\(\vvec_1\text{,}\)</span> we could use a Markov chain in this way to find a basis vector for <span class="process-math">\(E_1\text{,}\)</span> which is essentially how the Google PageRank algorithm works.</p></article><article class="example example-like" id="example-51"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">5.2.2</span><span class="period">.</span>
</h4>
<p id="p-5286">Let's now look at the matrix <span class="process-math">\(A = \left[\begin{array}{rr}
2 \amp 1 \\ 1 \amp 2 \\ \end{array}\right] \text{,}\)</span> which has eigenvalues <span class="process-math">\(\lambda_1=3\)</span> and <span class="process-math">\(\lambda_2 = 1\text{.}\)</span>  The dominant eigenvalue is <span class="process-math">\(\lambda_1=3\text{,}\)</span> and the associated eigenvectors are <span class="process-math">\(\vvec_1 = \twovec{1}{1}\)</span> and <span class="process-math">\(\vvec_{2} =
\twovec{-1}{1}\text{.}\)</span>  Once again, begin with the vector <span class="process-math">\(\xvec_0 = \twovec{1}{0}=\frac12 \vvec_1 - \frac12 \vvec_2\)</span> so that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\begin{aligned}
\xvec_1 \amp {}={} A\xvec_0 = 3\frac12 \vvec_1 - \frac12
\vvec_2 \\
\xvec_2 \amp {}={} A^2\xvec_0 = 3^2\frac13 \vvec_1 - \frac12
\vvec_2 \\
\xvec_3 \amp {}={} A^3\xvec_0 = 3^3\frac13 \vvec_1 - \frac12
\vvec_2 \\
\amp \vdots \\
\xvec_k \amp {}={} A^k\xvec_0 = 3^k\frac13 \vvec_1 - \frac12
\vvec_2\text{.} \\
\end{aligned}
\end{equation*}
</div>
<div class="sidebyside"><div class="sbsrow" style="margin-left:0%;margin-right:0%;">
<div class="sbspanel top" style="width:60%;"><p id="p-5287">As the figure shows, the vectors <span class="process-math">\(\xvec_k\)</span> are stretched by a factor of <span class="process-math">\(3\)</span> in the <span class="process-math">\(\vvec_1\)</span> direction and not at all in the <span class="process-math">\(\vvec_2\)</span> direction. Consequently, the vectors <span class="process-math">\(\xvec_k\)</span> become increasingly long, but their direction becomes closer to the direction of the eigenvector <span class="process-math">\(\vvec_1=\twovec{1}{1}\)</span> associated to the dominant eigenvalue.</p></div>
<div class="sbspanel top" style="width:40%;"><img src="external/images/numerical-power.svg" role="img" class="contained"></div>
</div></div>
<p id="p-5288">To find an eigenvector associated to the dominant eigenvalue, we will prevent the length of the vectors <span class="process-math">\(\xvec_k\)</span> from growing arbitrarily large by multiplying by an appropriate scaling constant.  Here is one way to do this. Given the vector <span class="process-math">\(\xvec_k\text{,}\)</span> we identify its component having the largest absolute value and call it <span class="process-math">\(m_k\text{.}\)</span>  We then define <span class="process-math">\(\overline{\xvec}_k = \frac{1}{m_k} \xvec_k\text{,}\)</span> which means that the component of <span class="process-math">\(\overline{\xvec}_k\)</span> having the largest absolute value is <span class="process-math">\(1\text{.}\)</span></p>
<p id="p-5289">For example, beginning with <span class="process-math">\(\xvec_0 = \twovec{1}{0}\text{,}\)</span> we find <span class="process-math">\(\xvec_1 = A\xvec_{0} =
\twovec{2}{1}\text{.}\)</span>  The component of <span class="process-math">\(\xvec_1\)</span> having the largest absolute value is <span class="process-math">\(m_1=2\)</span> so we multiply by <span class="process-math">\(\frac{1}{m_1} = \frac12\)</span> to obtain <span class="process-math">\(\overline{\xvec}_1 =
\twovec{1}{\frac12}\text{.}\)</span>  Then <span class="process-math">\(\xvec_2 = A\overline{\xvec}_1 =
\twovec{\frac52}{2}\text{.}\)</span>  Now the component having the largest absolute value is <span class="process-math">\(m_2=\frac52\)</span> so we multiply by <span class="process-math">\(\frac25\)</span> to obtain <span class="process-math">\(\overline{\xvec}_2 = \twovec{1}{\frac45}\text{.}\)</span></p>
<div class="sidebyside"><div class="sbsrow" style="margin-left:0%;margin-right:0%;">
<div class="sbspanel top" style="width:60%;"><p id="p-5290">The resulting sequence of vectors <span class="process-math">\(\overline{\xvec}_k\)</span> is shown in the figure.  Notice how the vectors <span class="process-math">\(\overline{\xvec}_k\)</span> now approach the eigenvector <span class="process-math">\(\vvec_1\text{,}\)</span> which gives us a way to find the eigenvector <span class="process-math">\(\vvec=\twovec{1}{1}\text{.}\)</span> This is the <em class="emphasis">power method</em> for finding an eigenvector associated to the dominant eigenvalue of a matrix. </p></div>
<div class="sbspanel top" style="width:40%;"><img src="external/images/numerical-power-norm.svg" role="img" class="contained"></div>
</div></div></article><article class="activity project-like" id="activity-65"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">5.2.2</span><span class="period">.</span>
</h4>
<p id="p-5291">Let's begin by considering the matrix <span class="process-math">\(A = \left[\begin{array}{rr}
0.5 \amp 0.2 \\
0.4 \amp 0.7 \\
\end{array}\right]\)</span> and the initial vector <span class="process-math">\(\xvec_0 = \twovec{1}{0}\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-142"><script type="text/x-sage">
</script></pre></p>
<ol class="lower-alpha">
<li id="li-3621"><p id="p-5292">Compute the vector <span class="process-math">\(\xvec_1 =
A\xvec_0\text{.}\)</span></p></li>
<li id="li-3622"><p id="p-5293">Find <span class="process-math">\(m_1\text{,}\)</span> the component of <span class="process-math">\(\xvec_1\)</span> that has the largest absolute value.  Then form <span class="process-math">\(\overline{\xvec}_1 =
\frac 1{m_1} \xvec_1\text{.}\)</span>  Notice that the component having the largest absolute value of <span class="process-math">\(\overline{\xvec}_1\)</span> is <span class="process-math">\(1\text{.}\)</span></p></li>
<li id="li-3623"><p id="p-5294">Find the vector <span class="process-math">\(\xvec_2 = A\overline{\xvec}_1\text{.}\)</span> Identify the component <span class="process-math">\(m_2\)</span> of <span class="process-math">\(\xvec_2\)</span> having the largest absolute value.  Then form <span class="process-math">\(\overline{\xvec}_2 =
\frac1{m_2}\overline{\xvec}_1\)</span> to obtain a vector in which the component with the largest absolute value is <span class="process-math">\(1\text{.}\)</span></p></li>
<li id="li-3624">
<p id="p-5295">The Sage cell below defines a function that implements the power method.  Define the matrix <span class="process-math">\(A\)</span> and initial vector <span class="process-math">\(\xvec_0\)</span> below.  The command <code class="code-inline tex2jax_ignore">power(A, x0,	N)</code> will print out the multiplier <span class="process-math">\(m\)</span> and the vectors <span class="process-math">\(\overline{\xvec}_k\)</span> for <span class="process-math">\(N\)</span> steps of the power method. <pre class="ptx-sagecell sagecell-sage" id="sage-143"><script type="text/x-sage">def power(A, x, N):
    for i in range(N):
        x = A*x
        m = max([comp for comp in x], key=abs).numerical_approx(digits=14)
        x = 1/float(m)*x
        print (m, x)

### Define the matrix A and initial vector x0 below
A =
x0 =
power(A, x0, 20)
</script></pre></p>
<p id="p-5296">How does this computation identify an eigenvector of the matrix <span class="process-math">\(A\text{?}\)</span></p>
</li>
<li id="li-3625"><p id="p-5297">What is the corresponding eigenvalue of this eigenvector?</p></li>
<li id="li-3626"><p id="p-5298">How do the values of the multipliers <span class="process-math">\(m_k\)</span> tell us the eigenvalue associated to the eigenvector we have found?</p></li>
<li id="li-3627"><p id="p-5299">Consider now the matrix <span class="process-math">\(A=\left[\begin{array}{rr}
-5.1 \amp 5.7 \\
-3.8 \amp 4.4 \\
\end{array}\right]
\text{.}\)</span> Use the power method to find the dominant eigenvalue of <span class="process-math">\(A\)</span> and an associated eigenvector.</p></li>
</ol></article><p id="p-5308">Notice that the power method gives us not only an eigenvector <span class="process-math">\(\vvec\)</span> but also its associated eigenvalue.  As in the activity, consider the matrix <span class="process-math">\(A=\left[\begin{array}{rr}
-5.1 \amp 5.7 \\
-3.8 \amp 4.4 \\
\end{array}\right]
\text{,}\)</span> which has eigenvector <span class="process-math">\(\vvec=\twovec{3}{2}\text{.}\)</span>  The first component has the largest absolute value so we multiply by <span class="process-math">\(\frac13\)</span> to obtain <span class="process-math">\(\overline{\vvec}=\twovec{1}{\frac23}\text{.}\)</span>  When we multiply by <span class="process-math">\(A\text{,}\)</span> we have <span class="process-math">\(A\overline{\vvec} = \twovec{-1.30}{-0.86}\text{.}\)</span> Notice that the first component still has the largest absolute value so that the multiplier <span class="process-math">\(m=-1.3\)</span> is the eigenvalue <span class="process-math">\(\lambda\)</span> corresponding to the eigenvector.  This demonstrates the fact that the multipliers <span class="process-math">\(m_k\)</span> approach the eigenvalue <span class="process-math">\(\lambda\)</span> having the largest absolute value.</p>
<p id="p-5309">Notice that the power method requires us to choose an initial vector <span class="process-math">\(\xvec_0\text{.}\)</span>  For most choices, this method will find the eigenvalue having the largest absolute value.  However, an unfortunate choice of <span class="process-math">\(\xvec_0\)</span> may not.  For instance, if we had chosen <span class="process-math">\(\xvec_0 = \vvec_2\)</span> in our example above, the vectors in the sequence <span class="process-math">\(\xvec_k =
A^k\xvec_0=\lambda_2^k\vvec_2\)</span> will not detect the eigenvector <span class="process-math">\(\vvec_1\text{.}\)</span>  However, it usually happens that our initial guess <span class="process-math">\(\xvec_0\)</span> has some contribution from <span class="process-math">\(\vvec_1\)</span> that enables us to find it.</p>
<p id="p-5310">The power method, as presented here, will fail for certain unlucky matrices.  This is examined in <a href="" class="xref" data-knowl="./knowl/exercise-power-method.html" title="Exercise 5.2.4.5">Exercise 5.2.4.5</a> along with a means to improve the power method to work for all matrices.</p></section><section class="subsection" id="subsection-80"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">5.2.2</span> <span class="title">Finding other eigenvalues</span>
</h3>
<p id="p-5311">The power method gives a technique for finding the dominant eigenvalue of a matrix.  We can modify the method to find the other eigenvalues as well.</p>
<article class="activity project-like" id="activity-66"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">5.2.3</span><span class="period">.</span>
</h4>
<p id="p-5312">The key to finding the eigenvalue of <span class="process-math">\(A\)</span> having the smallest absolute value is to note that the eigenvectors of <span class="process-math">\(A\)</span> are the same as those of <span class="process-math">\(A^{-1}\text{.}\)</span></p>
<ol class="lower-alpha">
<li id="li-3635"><p id="p-5313">If <span class="process-math">\(\vvec\)</span> is an eigenvector of <span class="process-math">\(A\)</span> with associated eigenvector <span class="process-math">\(\lambda\text{,}\)</span> explain why <span class="process-math">\(\vvec\)</span> is an eigenvector of <span class="process-math">\(A^{-1}\)</span> with associated eigenvalue <span class="process-math">\(\lambda^{-1}\text{.}\)</span></p></li>
<li id="li-3636"><p id="p-5314">Explain why the eigenvalue of <span class="process-math">\(A\)</span> having the smallest absolute value is the reciprocal of the dominant eigenvalue of <span class="process-math">\(A^{-1}\text{.}\)</span></p></li>
<li id="li-3637"><p id="p-5315">Explain how to use the power method applied to <span class="process-math">\(A^{-1}\)</span> to find the eigenvalue of <span class="process-math">\(A\)</span> having the smallest absolute value.</p></li>
<li id="li-3638"><p id="p-5316">If we apply the power method to <span class="process-math">\(A^{-1}\text{,}\)</span> we begin with an intial vector <span class="process-math">\(\xvec_0\)</span> and generate the sequence <span class="process-math">\(\xvec_{k+1} = A^{-1}\xvec_k\text{.}\)</span>  It is not computationally efficient to compute <span class="process-math">\(A^{-1}\text{,}\)</span> however, so instead we solve the equation <span class="process-math">\(A\xvec_{k+1} =
\xvec_k\text{.}\)</span>  Explain why an <span class="process-math">\(LU\)</span> factorization of <span class="process-math">\(A\)</span> is useful for implementing the power method applied to <span class="process-math">\(A^{-1}\text{.}\)</span></p></li>
<li id="li-3639"><p id="p-5317">The following Sage cell defines a command called <code class="code-inline tex2jax_ignore">inverse_power</code> that applies the power method to <span class="process-math">\(A^{-1}\text{.}\)</span>  That is, <code class="code-inline tex2jax_ignore">inverse_power(A, x0, N)</code> prints the vectors <span class="process-math">\(\xvec_k\text{,}\)</span> where <span class="process-math">\(\xvec_{k+1} =
A^{-1}\xvec_k\text{,}\)</span> and multipliers <span class="process-math">\(\frac{1}{m_k}\text{,}\)</span> which approximate the eigenvalue of <span class="process-math">\(A\text{.}\)</span>  Use it to find the eigenvalue of <span class="process-math">\(A=\left[\begin{array}{rr}
-5.1 \amp 5.7 \\
-3.8 \amp 4.4 \\
\end{array}\right]\)</span> having the smallest absolute value. <pre class="ptx-sagecell sagecell-sage" id="sage-144"><script type="text/x-sage">def inverse_power(A, x, N):
    for i in range(N):
        x = A \ x
        m = max([comp for comp in x], key=abs).numerical_approx(digits=14)
        x = 1/float(m)*x
        print (1/float(m), x)
### define the matrix A and vector x0
A =
x0 =
inverse_power(A, x0, 20)
</script></pre></p></li>
<li id="li-3640"><p id="p-5318">The inverse power method only works if <span class="process-math">\(A\)</span> is invertible.  If <span class="process-math">\(A\)</span> is not invertible, what is its eigenvalue having the smallest absolute value?</p></li>
<li id="li-3641"><p id="p-5319">Use the power method and the inverse power method to find the eigenvalues and associated eigenvectors of the matrix <span class="process-math">\(A = \left[\begin{array}{rr}
-0.23 \amp -2.33 \\
-1.16 \amp 1.08 \\
\end{array}\right]
\text{.}\)</span></p></li>
</ol></article><p id="p-5328">With the power method and the inverse power method, we can now find the eigenvalues of a matrix <span class="process-math">\(A\)</span> having the largest and smallest absolute values.  With one more modification, we can find all the eigenvalues of <span class="process-math">\(A\text{.}\)</span></p>
<article class="activity project-like" id="activity-67"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">5.2.4</span><span class="period">.</span>
</h4>
<p id="p-5329">Remember that the absolute value of a number tells us how far that number is from <span class="process-math">\(0\)</span> on the real number line.  We may therefore think of the inverse power method as telling us the eigenvalue closest to <span class="process-math">\(0\text{.}\)</span></p>
<ol class="lower-alpha">
<li id="li-3649"><p id="p-5330">If <span class="process-math">\(\vvec\)</span> is an eigenvalue of <span class="process-math">\(A\)</span> with associated eigenvalue <span class="process-math">\(\lambda\text{,}\)</span> explain why <span class="process-math">\(\vvec\)</span> is an eigenvector of <span class="process-math">\(A - sI\)</span> where <span class="process-math">\(s\)</span> is some scalar.</p></li>
<li id="li-3650"><p id="p-5331">What is the eigenvalue of <span class="process-math">\(A-sI\)</span> associated to the eigenvector <span class="process-math">\(\vvec\text{?}\)</span></p></li>
<li id="li-3651"><p id="p-5332">Explain why the eigenvalue of <span class="process-math">\(A\)</span> closest to <span class="process-math">\(s\)</span> is the eigenvalue of <span class="process-math">\(A-sI\)</span> closest to <span class="process-math">\(0\text{.}\)</span></p></li>
<li id="li-3652"><p id="p-5333">Explain why applying the inverse power method to <span class="process-math">\(A-sI\)</span> gives the eigenvalue of <span class="process-math">\(A\)</span> closest to <span class="process-math">\(s\text{.}\)</span></p></li>
<li id="li-3653">
<p id="p-5334">Consider the matrix <span class="process-math">\(A = \left[\begin{array}{rrrr}
3.6 \amp 1.6 \amp 4.0 \amp 7.6 \\
1.6 \amp 2.2 \amp 4.4 \amp 4.1 \\
3.9 \amp 4.3 \amp 9.0 \amp 0.6 \\
7.6 \amp 4.1 \amp 0.6 \amp 5.0 \\
\end{array}\right]
\text{.}\)</span> If we use the power method and inverse power method, we find two eigenvalues, <span class="process-math">\(\lambda_1=16.35\)</span> and <span class="process-math">\(\lambda_2=0.75\text{.}\)</span>  Viewing these eigenvalues on a number line, we know that the other eigenvalues lie in the range between <span class="process-math">\(-\lambda_1\)</span> and <span class="process-math">\(\lambda_1\text{,}\)</span> as shaded in <a href="" class="xref" data-knowl="./knowl/fig-numerical-power-line.html" title="Figure 5.2.3">Figure 5.2.3</a>.</p>
<figure class="figure figure-like" id="fig-numerical-power-line"><div class="sidebyside"><div class="sbsrow" style="margin-left:12.5%;margin-right:12.5%;"><div class="sbspanel top" style="width:100%;"><img src="external/images/numerical-power-line.svg" role="img" class="contained"></div></div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">5.2.3<span class="period">.</span></span><span class="space"> </span>The range of eigenvalues of <span class="process-math">\(A\text{.}\)</span></figcaption></figure><p id="p-5335">The Sage cell below has a function <code class="code-inline tex2jax_ignore">find_closest_eigenvalue(A, s, x, N)</code> that implements <span class="process-math">\(N\)</span> steps of the inverse power method using the matrix <span class="process-math">\(A-sI\)</span> and an initial vector <span class="process-math">\(x\text{.}\)</span>  This function prints approximations to the eigenvalue of <span class="process-math">\(A\)</span> closest to <span class="process-math">\(s\)</span> and its associated eigenvector.  By trying different values of <span class="process-math">\(s\)</span> in the shaded regions of the number line shown in <a href="" class="xref" data-knowl="./knowl/fig-numerical-power-line.html" title="Figure 5.2.3">Figure 5.2.3</a>, find the other two eigenvalues of <span class="process-math">\(A\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-145"><script type="text/x-sage">def find_closest_eigenvalue(A, s, x, N):
    B = A-s*identity_matrix(A.nrows())
    for i in range(N):
        x = B \ x
        m = max([comp for comp in x], key=abs).numerical_approx(digits=14)
        x = 1/float(m)*x
        print (1/float(m)+s, x)
### define the matrix A and vector x0
A =
x0 =
find_closest_eigenvalue(A, 2, x0, 20)
</script></pre></p>
</li>
<li id="li-3654"><p id="p-5336">Write a list of the four eigenvalues of <span class="process-math">\(A\)</span> in increasing order.</p></li>
</ol></article><p id="p-5344">There are some restrictions on the matrices to which this technique applies as we have assumed that the eigenvalues of <span class="process-math">\(A\)</span> are real and distinct.  If <span class="process-math">\(A\)</span> has repeated or complex eigenvalues, this technique will need to be modified, as explored in some of the exercises.</p></section><section class="subsection" id="subsection-81"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">5.2.3</span> <span class="title">Summary</span>
</h3>
<p id="p-5345">We have explored the power method as a tool for numerically approximating the eigenvalues and eigenvectors of a matrix.</p>
<ul class="disc">
<li id="li-3661"><p id="p-5346">After choosing an initial vector <span class="process-math">\(\xvec_0\text{,}\)</span> we define the sequence <span class="process-math">\(\xvec_{k+1}=A\xvec_k\text{.}\)</span>  As <span class="process-math">\(k\)</span> grows larger, the direction of the vectors <span class="process-math">\(\xvec_k\)</span> closely approximates the direction of the eigenspace corresponding to the eigenvalue <span class="process-math">\(\lambda_1\)</span> having the largest absolute value.</p></li>
<li id="li-3662"><p id="p-5347">We normalize the vectors <span class="process-math">\(\xvec_k\)</span> by multiplying by <span class="process-math">\(\frac{1}{m_k}\text{,}\)</span> where <span class="process-math">\(m_k\)</span> is the component having the largest absolute value.  In this way, the vectors <span class="process-math">\(\overline{\xvec}_k\)</span> approach an eigenvector associated to <span class="process-math">\(\lambda_1\text{,}\)</span> and the multipliers <span class="process-math">\(m_k\)</span> approach the eigenvalue <span class="process-math">\(\lambda_1\text{.}\)</span></p></li>
<li id="li-3663"><p id="p-5348">To find the eigenvalue having the smallest absolute value, we apply the power method using the matrix <span class="process-math">\(A^{-1}\text{.}\)</span></p></li>
<li id="li-3664"><p id="p-5349">To find the eigenvalue closest to some number <span class="process-math">\(s\text{,}\)</span> we apply the power method using the matrix <span class="process-math">\((A-sI)^{-1}\text{.}\)</span></p></li>
</ul></section><section class="exercises" id="exercises-21"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">5.2.4</span> <span class="title">Exercises</span>
</h3>
<p id="p-5350">This Sage cell has the commands <code class="code-inline tex2jax_ignore">power</code>, <code class="code-inline tex2jax_ignore">inverse_power</code>, and <code class="code-inline tex2jax_ignore">find_closest_eigenvalue</code> that we have developed in this section.  After evaluating this cell, these commands will be available in any other cell on this page. <pre class="ptx-sagecell sagecell-sage" id="sage-146"><script type="text/x-sage">def power(A, x, N):
    for i in range(N):
        x = A*x
        m = max([comp for comp in x], key=abs).numerical_approx(digits=14)
        x = 1/float(m)*x
        print (m, x)
def find_closest_eigenvalue(A, s, x, N):
    B = A-s*identity_matrix(A.nrows())
    for i in range(N):
        x = B \ x
        m = max([comp for comp in x], key=abs).numerical_approx(digits=14)
        x = 1/float(m)*x
        print (1/float(m)+s, x)
def inverse_power(A, x, N):
    find_closest_eigenvalue(A, 0, x, N)
</script></pre></p>
<article class="exercise exercise-like" id="exercise-189"><h4 class="heading"><span class="codenumber">1<span class="period">.</span></span></h4>
<p id="p-5351">Suppose that <span class="process-math">\(A\)</span> is a matrix having eigenvalues <span class="process-math">\(-3\text{,}\)</span> <span class="process-math">\(-0.2\text{,}\)</span> <span class="process-math">\(1\text{,}\)</span> and <span class="process-math">\(4\text{.}\)</span></p>
<ol class="lower-alpha">
<li id="li-3665"><p id="p-5352">What are the eigenvalues of <span class="process-math">\(A^{-1}\text{?}\)</span></p></li>
<li id="li-3666"><p id="p-5353">What are the eigenvalues of <span class="process-math">\(A+7I\text{?}\)</span></p></li>
</ol></article><article class="exercise exercise-like" id="exercise-190"><h4 class="heading"><span class="codenumber">2<span class="period">.</span></span></h4>
<p id="p-5360">Use the commands <code class="code-inline tex2jax_ignore">power</code>, <code class="code-inline tex2jax_ignore">inverse_power</code>, and <code class="code-inline tex2jax_ignore">find_closest_eigenvalue</code> to approximate the eigenvalues and associated eigenvectors of the following matrices. <pre class="ptx-sagecell sagecell-sage" id="sage-147"><script type="text/x-sage">
</script></pre></p>
<ol class="lower-alpha">
<li id="li-3671"><p id="p-5361"><span class="process-math">\(A= \left[\begin{array}{rr}
-2 \amp -2 \\
-8 \amp -2 \\
\end{array}\right]
\text{.}\)</span></p></li>
<li id="li-3672"><p id="p-5362"><span class="process-math">\(A= \left[\begin{array}{rr}
0.6 \amp 0.7 \\
0.5 \amp 0.2 \\
\end{array}\right]
\text{.}\)</span></p></li>
<li id="li-3673"><p id="p-5363"><span class="process-math">\(A= \left[\begin{array}{rrrr}
1.9  \amp -16.0 \amp  -13.0 \amp 27.0 \\
-2.4 \amp  20.3 \amp  4.6 \amp -17.7 \\
-0.51 \amp -11.7 \amp -1.4 \amp  13.1  \\
-2.1 \amp  15.3 \amp  6.9 \amp -20.5 \\
\end{array}\right]
\text{.}\)</span></p></li>
</ol></article><article class="exercise exercise-like" id="exercise-191"><h4 class="heading"><span class="codenumber">3<span class="period">.</span></span></h4>
<p id="p-5373">Use the techniques we have seen in this section to find the eigenvalues of the matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A= \left[\begin{array}{rrrrr}
-14.6 \amp 9.0 \amp -14.1 \amp 5.8 \amp  13.0 \\
27.8 \amp -4.2 \amp  16.0 \amp 0.9 \amp -21.3 \\
-5.5 \amp 3.4 \amp  3.4 \amp  3.3 \amp  1.1 \\
-25.4 \amp 11.3 \amp -15.4 \amp 4.7 \amp  20.3 \\
-33.7 \amp 14.8 \amp -22.5 \amp 9.7 \amp  26.6 \\
\end{array}\right]\text{.}
\end{equation*}
</div>
<p class="continuation"><pre class="ptx-sagecell sagecell-sage" id="sage-148"><script type="text/x-sage">A = matrix(5,5, [-14.6,  9.0, -14.1, 5.8,  13.0,
                  27.8, -4.2,  16.0, 0.9, -21.3,
                  -5.5,  3.4,   3.4, 3.3,   1.1,
                 -25.4, 11.3, -15.4, 4.7,  20.3,
                 -33.7, 14.8, -22.5, 9.7,  26.6])
</script></pre></p></article><article class="exercise exercise-like" id="exercise-192"><h4 class="heading"><span class="codenumber">4<span class="period">.</span></span></h4>
<p id="p-5376">Consider the matrix <span class="process-math">\(A = \left[\begin{array}{rr}
0 \amp -1 \\
-4 \amp 0 \\
\end{array}\right]
\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-149"><script type="text/x-sage">
</script></pre></p>
<ol class="lower-alpha">
<li id="li-3680"><p id="p-5377">Describe what happens if we apply the power method and the inverse power method using the initial vector <span class="process-math">\(\xvec_0 =
\twovec{1}{0}\text{.}\)</span></p></li>
<li id="li-3681"><p id="p-5378">Find the eigenvalues of this matrix and explain this observed behavior.</p></li>
<li id="li-3682"><p id="p-5379">How can we apply the techniques of this section to find the eigenvalues of <span class="process-math">\(A\text{?}\)</span></p></li>
</ol></article><article class="exercise exercise-like" id="exercise-power-method"><h4 class="heading"><span class="codenumber">5<span class="period">.</span></span></h4>
<p id="p-5388">We have seen that the matrix <span class="process-math">\(A = \left[\begin{array}{rr}
1 \amp 2 \\
2 \amp 1 \\
\end{array}\right]\)</span> has eigenvalues <span class="process-math">\(\lambda_1 = 3\)</span> and <span class="process-math">\(\lambda_2=-1\)</span> and associated eigenvectors <span class="process-math">\(\vvec_1 =
\twovec{1}{1}\)</span> and <span class="process-math">\(\vvec_2=\twovec{-1}{1}\text{.}\)</span></p>
<ol class="lower-alpha">
<li id="li-3689"><p id="p-5389">Describe what happens when we apply the inverse power method using the initial vector <span class="process-math">\(\xvec_0 = \twovec{1}{0}\text{.}\)</span></p></li>
<li id="li-3690"><p id="p-5390">Explain why this is happening and provide a contrast with how the power method usually works.</p></li>
<li id="li-3691"><p id="p-5391">How can we modify the power method to give the dominant eigenvalue in this case?</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-194"><h4 class="heading"><span class="codenumber">6<span class="period">.</span></span></h4>
<p id="p-5400">Suppose that <span class="process-math">\(A\)</span> is a <span class="process-math">\(2\times2\)</span> matrix with eigenvalues <span class="process-math">\(4\)</span> and <span class="process-math">\(-3\)</span> and that <span class="process-math">\(B\)</span> is a <span class="process-math">\(2\times2\)</span> matrix with eigenvalues <span class="process-math">\(4\)</span> and <span class="process-math">\(1\text{.}\)</span> If we apply the power method to find the dominant eigenvalue of these matrices to the same degree of accuracy, which matrix will require more steps in the algorithm?  Explain your response.</p></article><article class="exercise exercise-like" id="exercise-195"><h4 class="heading"><span class="codenumber">7<span class="period">.</span></span></h4>
<p id="p-5403">Suppose that we apply the power method to the matrix <span class="process-math">\(A\)</span> with an initial vector <span class="process-math">\(\xvec_0\)</span> and find the eigenvalue <span class="process-math">\(\lambda=3\)</span> and eigenvector <span class="process-math">\(\vvec\text{.}\)</span> Suppose that we then apply the power method again with a different initial vector and find the same eigenvalue <span class="process-math">\(\lambda=3\)</span> but a different eigenvector <span class="process-math">\(\wvec\text{.}\)</span>  What can we conclude about the matrix <span class="process-math">\(A\)</span> in this case?</p></article><article class="exercise exercise-like" id="exercise-196"><h4 class="heading"><span class="codenumber">8<span class="period">.</span></span></h4>
<p id="p-5406">The power method we have developed only works if the matrix has real eigenvalues. Suppose that <span class="process-math">\(A\)</span> is a <span class="process-math">\(2\times2\)</span> matrix that has a complex eigenvalue <span class="process-math">\(\lambda = 2+3i\text{.}\)</span>  What would happen if we apply the power method to <span class="process-math">\(A\text{?}\)</span></p></article><article class="exercise exercise-like" id="exercise-197"><h4 class="heading"><span class="codenumber">9<span class="period">.</span></span></h4>
<p id="p-5409">Consider the matrix <span class="process-math">\(A=\left[\begin{array}{rr}
1 \amp 1 \\
0 \amp 1 \\
\end{array}\right]
\text{.}\)</span></p>
<ol class="lower-alpha">
<li id="li-3698"><p id="p-5410">Find the eigenvalues and associated eigenvectors of <span class="process-math">\(A\text{.}\)</span></p></li>
<li id="li-3699"><p id="p-5411">Make a prediction about what happens if we apply the power method and the inverse power method to find eigenvalues of <span class="process-math">\(A\text{.}\)</span></p></li>
<li id="li-3700"><p id="p-5412">Verify your prediction using Sage. <pre class="ptx-sagecell sagecell-sage" id="sage-150"><script type="text/x-sage">
</script></pre></p></li>
</ol></article></section></section></div></main>
</div>
</body>
</html>
