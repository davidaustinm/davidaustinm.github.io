<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2021-06-18T15:34:31-04:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Orthogonal bases and projections</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<script src="https://sagecell.sagemath.org/embedded_sagecell.js"></script><script>window.MathJax = {
  tex: {
    inlineMath: [['\\(','\\)']],
    tags: "none",
    useLabelIds: true,
    tagSide: "right",
    tagIndent: ".8em",
    packages: {'[+]': ['base', 'extpfeil', 'ams', 'amscd', 'newcommand', 'knowl']}
  },
  options: {
    ignoreHtmlClass: "tex2jax_ignore",
    processHtmlClass: "has_am",
  },
  chtml: {
    scale: 0.88,
    mtextInheritFont: true
  },
  loader: {
    load: ['input/asciimath', '[tex]/extpfeil', '[tex]/amscd', '[tex]/newcommand', '[pretext]/mathjaxknowl3.js'],
    paths: {pretext: "https://pretextbook.org/js/lib"},
  },
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><script>$(function () {
    // Make *any* div with class 'sagecell-sage' an executable Sage cell
    // Their results will be linked, only within language type
    sagecell.makeSagecell({inputLocation: 'div.sagecell-sage',
                           linked: true,
                           languages: ['sage'],
                           evalButtonText: 'Evaluate (Sage)'});
});
</script><script async="" src="https://cse.google.com/cse.js?cx=015103900096539427448:ngwuia10qci"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.min.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/0.13/pretext.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/0.13/pretext_add_on.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script xmlns:svg="http://www.w3.org/2000/svg">sagecellEvalName='Evaluate (Sage)';
</script><link xmlns:svg="http://www.w3.org/2000/svg" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/pretext.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/pretext_add_on.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/banner_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/toc_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/knowls_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/style_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/colors_blue_grey.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/setcolors.css" rel="stylesheet" type="text/css">
<!-- 2019-10-12: Temporary - CSS file for experiments with styling --><link xmlns:svg="http://www.w3.org/2000/svg" href="developer.css" rel="stylesheet" type="text/css">
</head>
<body class="mathbook-book has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div xmlns:svg="http://www.w3.org/2000/svg" id="latex-macros" class="hidden-content" style="display:none">\(\newcommand{\avec}{{\mathbf a}}
\newcommand{\bvec}{{\mathbf b}}
\newcommand{\cvec}{{\mathbf c}}
\newcommand{\dvec}{{\mathbf d}}
\newcommand{\dtil}{\widetilde{\mathbf d}}
\newcommand{\evec}{{\mathbf e}}
\newcommand{\fvec}{{\mathbf f}}
\newcommand{\nvec}{{\mathbf n}}
\newcommand{\pvec}{{\mathbf p}}
\newcommand{\qvec}{{\mathbf q}}
\newcommand{\svec}{{\mathbf s}}
\newcommand{\tvec}{{\mathbf t}}
\newcommand{\uvec}{{\mathbf u}}
\newcommand{\vvec}{{\mathbf v}}
\newcommand{\wvec}{{\mathbf w}}
\newcommand{\xvec}{{\mathbf x}}
\newcommand{\yvec}{{\mathbf y}}
\newcommand{\zvec}{{\mathbf z}}
\newcommand{\rvec}{{\mathbf r}}
\newcommand{\mvec}{{\mathbf m}}
\newcommand{\zerovec}{{\mathbf 0}}
\newcommand{\onevec}{{\mathbf 1}}
\newcommand{\real}{{\mathbb R}}
\newcommand{\twovec}[2]{\left[\begin{array}{r}#1 \\ #2
\end{array}\right]}
\newcommand{\ctwovec}[2]{\left[\begin{array}{c}#1 \\ #2
\end{array}\right]}
\newcommand{\threevec}[3]{\left[\begin{array}{r}#1 \\ #2 \\ #3
\end{array}\right]}
\newcommand{\cthreevec}[3]{\left[\begin{array}{c}#1 \\ #2 \\ #3
\end{array}\right]}
\newcommand{\fourvec}[4]{\left[\begin{array}{r}#1 \\ #2 \\ #3 \\ #4
\end{array}\right]}
\newcommand{\cfourvec}[4]{\left[\begin{array}{c}#1 \\ #2 \\ #3 \\ #4
\end{array}\right]}
\newcommand{\fivevec}[5]{\left[\begin{array}{r}#1 \\ #2 \\ #3 \\
#4 \\ #5 \\ \end{array}\right]}
\newcommand{\cfivevec}[5]{\left[\begin{array}{c}#1 \\ #2 \\ #3 \\
#4 \\ #5 \\ \end{array}\right]}
\newcommand{\mattwo}[4]{\left[\begin{array}{rr}#1 \amp #2 \\ #3 \amp #4 \\ \end{array}\right]}
\newcommand{\laspan}[1]{\text{Span}\{#1\}}
\newcommand{\bcal}{{\cal B}}
\newcommand{\ccal}{{\cal C}}
\newcommand{\scal}{{\cal S}}
\newcommand{\wcal}{{\cal W}}
\newcommand{\ecal}{{\cal E}}
\newcommand{\coords}[2]{\left\{#1\right\}_{#2}}
\newcommand{\gray}[1]{\color{gray}{#1}}
\newcommand{\lgray}[1]{\color{lightgray}{#1}}
\newcommand{\rank}{\text{rank}}
\newcommand{\row}{\text{Row}}
\newcommand{\col}{\text{Col}}
\renewcommand{\row}{\text{Row}}
\newcommand{\nul}{\text{Nul}}
\newcommand{\var}{\text{Var}}
\newcommand{\corr}{\text{corr}}
\newcommand{\len}[1]{\left|#1\right|}
\newcommand{\bbar}{\overline{\bvec}}
\newcommand{\bhat}{\widehat{\bvec}}
\newcommand{\bperp}{\bvec^\perp}
\newcommand{\xhat}{\widehat{\xvec}}
\newcommand{\vhat}{\widehat{\vvec}}
\newcommand{\uhat}{\widehat{\uvec}}
\newcommand{\what}{\widehat{\wvec}}
\newcommand{\Sighat}{\widehat{\Sigma}}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="ula.html"><span class="title">Understanding Linear Algebra</span></a></h1>
<p class="byline">David Austin</p>
</div>
<div class="searchwrapper" role="search"><div class="gcse-search"></div></div>
</div></div>
<nav xmlns:svg="http://www.w3.org/2000/svg" id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3">
<a class="index-button toolbar-item button" href="index-1.html" title="Index">Index</a><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="sec-transpose.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="chap6.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="sec-gram-schmidt.html" title="Next">Next</a></span>
</div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="sec-transpose.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="chap6.html" title="Up">Up</a><a class="next-button button toolbar-item" href="sec-gram-schmidt.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div xmlns:svg="http://www.w3.org/2000/svg" id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link frontmatter">
<a href="frontmatter.html" data-scroll="frontmatter"><span class="title">Front Matter</span></a><ul>
<li><a href="dedication-1.html" data-scroll="dedication-1">Dedication</a></li>
<li><a href="colophon-1.html" data-scroll="colophon-1">Colophon</a></li>
<li><a href="preface-1.html" data-scroll="preface-1">Our goals</a></li>
</ul>
</li>
<li class="link">
<a href="chap1.html" data-scroll="chap1"><span class="codenumber">1</span> <span class="title">Systems of equations</span></a><ul>
<li><a href="sec-expect.html" data-scroll="sec-expect">What can we expect</a></li>
<li><a href="sec-finding-solutions.html" data-scroll="sec-finding-solutions">Finding solutions to systems of linear equations</a></li>
<li><a href="sec-sage-introduction.html" data-scroll="sec-sage-introduction">Computation with Sage</a></li>
<li><a href="sec-pivots.html" data-scroll="sec-pivots">Pivots and their influence on solution spaces</a></li>
</ul>
</li>
<li class="link">
<a href="chap2.html" data-scroll="chap2"><span class="codenumber">2</span> <span class="title">Vectors, matrices, and linear combinations</span></a><ul>
<li><a href="sec-vectors-lin-combs.html" data-scroll="sec-vectors-lin-combs">Vectors and linear combinations</a></li>
<li><a href="sec-matrices-lin-combs.html" data-scroll="sec-matrices-lin-combs">Matrix multiplication and linear combinations</a></li>
<li><a href="sec-span.html" data-scroll="sec-span">The span of a set of vectors</a></li>
<li><a href="sec-linear-dep.html" data-scroll="sec-linear-dep">Linear independence</a></li>
<li><a href="sec-linear-trans.html" data-scroll="sec-linear-trans">Matrix transformations</a></li>
<li><a href="sec-transforms-geom.html" data-scroll="sec-transforms-geom">The geometry of matrix transformations</a></li>
</ul>
</li>
<li class="link">
<a href="chap3.html" data-scroll="chap3"><span class="codenumber">3</span> <span class="title">Invertibility, bases, and coordinate systems</span></a><ul>
<li><a href="sec-matrix-inverse.html" data-scroll="sec-matrix-inverse">Invertibility</a></li>
<li><a href="sec-bases.html" data-scroll="sec-bases">Bases and coordinate systems</a></li>
<li><a href="sec-jpeg.html" data-scroll="sec-jpeg">Image compression</a></li>
<li><a href="sec-determinants.html" data-scroll="sec-determinants">Determinants</a></li>
<li><a href="sec-subspaces.html" data-scroll="sec-subspaces">Subspaces of \(\real^p\)</a></li>
</ul>
</li>
<li class="link">
<a href="chap4.html" data-scroll="chap4"><span class="codenumber">4</span> <span class="title">Eigenvalues and eigenvectors</span></a><ul>
<li><a href="sec-eigen-intro.html" data-scroll="sec-eigen-intro">An introduction to eigenvalues and eigenvectors</a></li>
<li><a href="sec-eigen-find.html" data-scroll="sec-eigen-find">Finding eigenvalues and eigenvectors</a></li>
<li><a href="sec-eigen-diag.html" data-scroll="sec-eigen-diag">Diagonalization, similarity, and powers of a matrix</a></li>
<li><a href="sec-dynamical.html" data-scroll="sec-dynamical">Dynamical systems</a></li>
<li><a href="sec-stochastic.html" data-scroll="sec-stochastic">Markov chains and Google's PageRank algorithm</a></li>
</ul>
</li>
<li class="link">
<a href="chap5.html" data-scroll="chap5"><span class="codenumber">5</span> <span class="title">Linear algebra and computing</span></a><ul>
<li><a href="sec-gaussian-revisited.html" data-scroll="sec-gaussian-revisited">Gaussian elimination revisited</a></li>
<li><a href="sec-power-method.html" data-scroll="sec-power-method">Finding eigenvectors numerically</a></li>
</ul>
</li>
<li class="link">
<a href="chap6.html" data-scroll="chap6"><span class="codenumber">6</span> <span class="title">Orthogonality and Least Squares</span></a><ul>
<li><a href="sec-dot-product.html" data-scroll="sec-dot-product">The dot product</a></li>
<li><a href="sec-transpose.html" data-scroll="sec-transpose">Orthogonal complements and the matrix tranpose</a></li>
<li><a href="sec-orthogonal-bases.html" data-scroll="sec-orthogonal-bases" class="active">Orthogonal bases and projections</a></li>
<li><a href="sec-gram-schmidt.html" data-scroll="sec-gram-schmidt">Finding orthogonal bases</a></li>
<li><a href="sec-least-squares.html" data-scroll="sec-least-squares">Orthogonal least squares</a></li>
</ul>
</li>
<li class="link">
<a href="chap7.html" data-scroll="chap7"><span class="codenumber">7</span> <span class="title">The Spectral Theorem and singular value decompositions</span></a><ul>
<li><a href="sec-symmetric-matrices.html" data-scroll="sec-symmetric-matrices">Symmetric matrices and variance</a></li>
<li><a href="sec-quadratic-forms.html" data-scroll="sec-quadratic-forms">Quadratic forms</a></li>
<li><a href="sec-pca.html" data-scroll="sec-pca">Principal Component Analysis</a></li>
<li><a href="sec-svd-intro.html" data-scroll="sec-svd-intro">Singular Value Decompositions</a></li>
<li><a href="sec-svd-uses.html" data-scroll="sec-svd-uses">Using Singular Value Decompositions</a></li>
</ul>
</li>
<li class="link backmatter"><a href="backmatter.html" data-scroll="backmatter"><span class="title">Back Matter</span></a></li>
<li class="link"><a href="index-1.html" data-scroll="index-1"><span class="title">Index</span></a></li>
</ul></nav><div class="extras"><nav><a class="mathbook-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content"><section xmlns:svg="http://www.w3.org/2000/svg" class="section" id="sec-orthogonal-bases"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">6.3</span> <span class="title">Orthogonal bases and projections</span>
</h2>
<section class="introduction" id="introduction-30"><p id="p-5776">We know that a linear system \(A\xvec=\bvec\) is inconsistent when \(\bvec\) is not in \(\col(A)\text{,}\) the column space of \(A\text{.}\)  In <a href="sec-least-squares.html" class="internal" title="Section 6.5: Orthogonal least squares">Section 6.5</a>, we'll develop a strategy for dealing with inconsistent systems by finding \(\bhat\text{,}\) the vector in \(\col(A)\) that is closest to \(\bvec\text{.}\)  The equation \(A\xvec=\bhat\) is then consistent and its solution set can provide us with useful information about the original system.</p>
<p id="p-5777">In this section and the next, we'll develop some techniques that enable us to find \(\bhat\text{,}\) the vector in a given subspace \(W\) that is closest to a given vector \(\bvec\text{.}\)</p>
<article class="exploration project-like" id="preview-orthogonal-basis"><h6 class="heading">
<span class="type">Preview Activity</span><span class="space"> </span><span class="codenumber">6.3.1</span><span class="period">.</span>
</h6>
<p id="p-5778">For this activity, it will be helpful to recall the distributive property of dot products:</p>
<div class="displaymath">
\begin{equation*}
\vvec\cdot(c_1\wvec_1+c_2\wvec_2) = c_1\vvec\cdot\wvec_1 +
c_2\vvec\cdot\wvec_2\text{.}
\end{equation*}
</div>
<p class="continuation">We'll work with the basis of \(\real^2\) formed by the vectors</p>
<div class="displaymath">
\begin{equation*}
\wvec_1=\twovec12,\hspace{24pt}
\wvec_2=\twovec{-2}1\text{.}
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-3961"><p id="p-5779">Verify that the vectors \(\wvec_1\) and \(\wvec_2\) are orthogonal.</p></li>
<li id="li-3962"><p id="p-5780">Suppose that \(\bvec =\twovec74\) and find the dot products \(\wvec_1\cdot\bvec\) and \(\wvec_2\cdot\bvec\text{.}\)</p></li>
<li id="li-3963">
<p id="p-5781">We would like to express \(\bvec\) as a linear combination of \(\wvec_1\) and \(\wvec_2\text{,}\) which means that we need to find weights \(c_1\) and \(c_2\) such that</p>
<div class="displaymath">
\begin{equation*}
\bvec = c_1\wvec_1 + c_2\wvec_2\text{.}
\end{equation*}
</div>
<p class="continuation">To find the weight \(c_1\text{,}\) dot both sides of this expression with \(\wvec_1\text{:}\)</p>
<div class="displaymath">
\begin{equation*}
\bvec\cdot\wvec_1 = (c_1\wvec_1 +
c_2\wvec_2)\cdot\wvec_1\text{,}
\end{equation*}
</div>
<p class="continuation">and apply the distributive property.</p>
</li>
<li id="li-3964"><p id="p-5782">In a similar fashion, find the weight \(c_2\text{.}\)</p></li>
<li id="li-3965"><p id="p-5783">Verify that \(\bvec = c_1\wvec_1+c_2\wvec_2\) using the weights you have found.</p></li>
</ol></article><p id="p-5784">We frequently ask to write a given vector as a linear combination of given basis vectors.  In the past, we have done this by solving a linear system.  The preview activity illustrates how this task can be simplified when the basis vectors are orthogonal to one another.  We'll explore this and other uses of orthogonal bases in this section.</p></section><section class="subsection" id="subsection-88"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">6.3.1</span> <span class="title">Orthogonal sets</span>
</h3>
<p id="p-5785">The preview activity dealt with a basis of \(\real^2\) formed by two orthogonal vectors.  We will more generally consider a set of orthogonal vectors, as described in the next definition.</p>
<article class="definition definition-like" id="definition-26"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">6.3.1</span><span class="period">.</span>
</h6>
<p id="p-5786">By an <em class="emphasis">orthogonal set</em> of vectors, we mean a set of nonzero vectors each of which is orthogonal to the others.</p></article><article class="example example-like" id="example-orthogonal-basis"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">6.3.2</span><span class="period">.</span>
</h6>
<p id="p-5787">The 3-dimensional vectors</p>
<div class="displaymath">
\begin{equation*}
\wvec_1 = \threevec1{-1}1,\hspace{24pt}
\wvec_2 = \threevec1{1}0,\hspace{24pt}
\wvec_3 = \threevec1{-1}{-2}.
\end{equation*}
</div>
<p class="continuation">form an orthogonal set, which can be verified by computing</p>
<div class="displaymath">
\begin{equation*}
\begin{array}{rcl}
\wvec_1\cdot\wvec_2 \amp {}={} \amp 0 \\
\wvec_1\cdot\wvec_3 \amp {}={} \amp 0 \\
\wvec_2\cdot\wvec_3 \amp {}={} \amp 0\text{.} \\
\end{array}
\end{equation*}
</div>
<p class="continuation">Notice that this set of vectors forms a basis for \(\real^3\text{.}\)</p></article><article class="example example-like" id="example-orthogonal-set"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">6.3.3</span><span class="period">.</span>
</h6>
<p id="p-5788">The vectors</p>
<div class="displaymath">
\begin{equation*}
\wvec_1 = \fourvec1111,\hspace{24pt}
\wvec_2 = \fourvec11{-1}{-1},\hspace{24pt}
\wvec_3 = \fourvec1{-1}1{-1}
\end{equation*}
</div>
<p class="continuation">form an orthogonal set of 4-dimensional vectors.  Since there are only three vectors, this set does not form a basis for \(\real^4\text{.}\)  It does, however, form a basis for a 3-dimensional subspace \(W\) of \(\real^4\text{.}\)</p></article><p id="p-5789">Suppose that a vector \(\bvec\) is a linear combination of an orthogonal set of vectors \(\wvec_1,\wvec_2,\ldots,\wvec_n\text{;}\)  that is, suppose that</p>
<div class="displaymath">
\begin{equation*}
c_1\wvec_1 + c_2\wvec_2 + \ldots + c_n\wvec_n = \bvec.
\end{equation*}
</div>
<p class="continuation">Just as in the preview activity, we can find the weight \(c_1\) by dotting both sides with \(\wvec_1\) and applying the distributive property of dot products:</p>
<div class="displaymath">
\begin{align*}
(c_1\wvec_1 + c_2\wvec_2 + \ldots + c_n\wvec_n)\cdot\wvec_1
\amp = \bvec\cdot\wvec_1\\
c_1\wvec_1\cdot\wvec_1 + c_2\wvec_2\cdot\wvec_1 +\ldots +
c_n\wvec_n\cdot\wvec_1  \amp = \bvec\cdot\wvec_1\\
c_1\wvec_1\cdot\wvec_1 \amp = \bvec\cdot\wvec_1\\
c_1 \amp = 
\frac{\bvec\cdot\wvec_1}{\wvec_1\cdot\wvec_1}\text{.}
\end{align*}
</div>
<p class="continuation">Notice how the presence of an orthogonal set causes most of the terms in the sum to vanish.  In the same way, we find that</p>
<div class="displaymath">
\begin{equation*}
c_i = \frac{\bvec\cdot\wvec_i}{\wvec_i\cdot\wvec_i}
\end{equation*}
</div>
<p class="continuation">so that</p>
<div class="displaymath">
\begin{equation*}
\bvec = \frac{\bvec\cdot\wvec_1}{\wvec_1\cdot\wvec_1}\wvec_1 +
\frac{\bvec\cdot\wvec_2}{\wvec_2\cdot\wvec_2}\wvec_2 +
\ldots +
\frac{\bvec\cdot\wvec_n}{\wvec_n\cdot\wvec_n}\wvec_n\text{.}
\end{equation*}
</div>
<p id="p-5790">We'll record this fact in the following proposition.</p>
<article class="proposition theorem-like" id="prop-orthog-lincomb"><h6 class="heading">
<span class="type">Proposition</span><span class="space"> </span><span class="codenumber">6.3.4</span><span class="period">.</span>
</h6>
<p id="p-5791">If a vector \(\bvec\) is a linear combination of an orthogonal set of vectors \(\wvec_1,\wvec_2,\ldots,\wvec_n\text{,}\) then</p>
<div class="displaymath">
\begin{equation*}
\bvec = \frac{\bvec\cdot\wvec_1}{\wvec_1\cdot\wvec_1}\wvec_1 +
\frac{\bvec\cdot\wvec_2}{\wvec_2\cdot\wvec_2}\wvec_2 +
\ldots +
\frac{\bvec\cdot\wvec_n}{\wvec_n\cdot\wvec_n}\wvec_n\text{.}
\end{equation*}
</div></article><p id="p-5792">Using this proposition, we can see that an orthogonal set of vectors must be linearly independent.  Suppose, for instance, that \(\wvec_1,\wvec_2,\ldots,\wvec_n\) is a set of nonzero orthogonal vectors and that one of the vectors is a linear combination of the others, say,</p>
<div class="displaymath">
\begin{equation*}
\wvec_3 = c_1\wvec_1 + c_2\wvec_2\text{.}
\end{equation*}
</div>
<p class="continuation">We therefore know that</p>
<div class="displaymath">
\begin{equation*}
\wvec_3 =
\frac{\wvec_3\cdot\wvec_1}{\wvec_1\cdot\wvec_1}\wvec_1 +
\frac{\wvec_3\cdot\wvec_2}{\wvec_2\cdot\wvec_1}\wvec_2
= \zerovec\text{,}
\end{equation*}
</div>
<p class="continuation">which cannot happen since we know that \(\wvec_3\) is nonzero. This tells us that</p>
<article class="proposition theorem-like" id="prop-orthog-lin-indep"><h6 class="heading">
<span class="type">Proposition</span><span class="space"> </span><span class="codenumber">6.3.5</span><span class="period">.</span>
</h6>
<p id="p-5793">An orthogonal set of vectors \(\wvec_1,\wvec_2,\ldots,\wvec_n\) is linearly independent.</p></article><p id="p-5794">If the vectors in an orthogonal set have dimension \(m\text{,}\) they form a linearly independent set in \(\real^m\) and are therefore a basis for the subspace \(W=\laspan{\vvec_1,\vvec_2,\ldots,\vvec_n}\text{.}\)  If there are \(m\) vectors in the orthogonal set, they form a basis for \(\real^m\text{.}\)</p>
<article class="activity project-like" id="activity-75"><h6 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">6.3.2</span><span class="period">.</span>
</h6>
<p id="p-5795">Consider the vectors</p>
<div class="displaymath">
\begin{equation*}
\wvec_1 = \threevec1{-1}1,\hspace{24pt}
\wvec_2 = \threevec1{1}0,\hspace{24pt}
\wvec_3 = \threevec1{-1}{-2}.
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-3966">
<p id="p-5796">Verify that this set forms an orthogonal set of \(3\)-dimensional vectors.</p>
<div class="sagecell-sage" id="sage-157"><script type="text/x-sage">
</script></div>
</li>
<li id="li-3967"><p id="p-5797">Explain why we now know that this set of vectors forms a basis for \(\real^3\text{.}\)</p></li>
<li id="li-3968"><p id="p-5798">Suppose that \(\bvec=\threevec24{-4}\text{.}\)  Find the weights \(c_1\text{,}\) \(c_2\text{,}\) and \(c_3\) that express \(\bvec\) as a linear combination \(\bvec=c_1\wvec_1 + c_2\wvec_2 + c_3\wvec_3\) using <a class="xref" data-knowl="./knowl/prop-orthog-lincomb.html" title="Proposition 6.3.4">Proposition 6.3.4</a>.</p></li>
<li id="li-3969">
<p id="p-5799">If we multiply a vector \(\vvec\) by a positive scalar \(s\text{,}\) the length of \(\vvec\) is also multiplied by \(s\text{;}\)  that is, \(\len{s\vvec} = s\len{\vvec}\text{.}\)</p>
<p id="p-5800">Using this observation, find a vector \(\uvec_1\) that is parallel to \(\wvec_1\) and has length 1. Such vectors are called <em class="emphasis">unit vectors</em>. <div class="sagecell-sage" id="sage-158"><script type="text/x-sage">
</script></div></p>
</li>
<li id="li-3970"><p id="p-5801">Similarly, find a unit vector \(\uvec_2\) that is parallel to \(\wvec_2\) and a unit vector \(\uvec_3\) that is parallel to \(\wvec_3\text{.}\)</p></li>
<li id="li-3971"><p id="p-5802">Construct the matrix \(Q=\begin{bmatrix}
\uvec_1 \amp \uvec_2 \amp \uvec_3
\end{bmatrix}\) and find the product \(Q^TQ\text{.}\)  Use <a class="xref" data-knowl="./knowl/prop-transpose-multiplication.html" title="Proposition 6.2.8">Proposition 6.2.8</a> to explain your result.</p></li>
</ol></article><p id="p-5817">This activity introduces an important way of modifying an orthogonal set so that the vectors in the set have unit length. Recall that we may multiply any nonzero vector \(\wvec\) by a scalar so that the new vector has length 1. For instance, we know that, if \(s\) is a positive scalar, then \(\len{s\wvec} = s\len{\wvec}\text{.}\)  To obtain a vector \(\uvec\) having unit length, we want</p>
<div class="displaymath">
\begin{equation*}
\len{\uvec} = \len{s\wvec} = s\len{\wvec} = 1
\end{equation*}
</div>
<p class="continuation">so that \(s=1/\len{\wvec}\text{.}\)  Therefore,</p>
<div class="displaymath">
\begin{equation*}
\uvec = \frac{1}{\len{\wvec}}\wvec
\end{equation*}
</div>
<p class="continuation">becomes a unit vector parallel to \(\wvec\text{.}\)</p>
<p id="p-5818">Orthogonal sets in which the vectors have unit length are called <em class="emphasis">orthonormal</em> and are especially convenient.</p>
<article class="definition definition-like" id="definition-27"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">6.3.6</span><span class="period">.</span>
</h6>
<p id="p-5819">An <em class="emphasis">orthonormal</em> set is an orthogonal set of vectors each of which has unit length.</p></article><article class="example example-like" id="example-39"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">6.3.7</span><span class="period">.</span>
</h6>
<p id="p-5820">The vectors</p>
<div class="displaymath">
\begin{equation*}
\uvec_1=\twovec{1/\sqrt{2}}{1/\sqrt{2}},\hspace{24pt}
\uvec_2=\twovec{-1/\sqrt{2}}{1/\sqrt{2}}
\end{equation*}
</div>
<p class="continuation">are an orthonormal set of vectors in \(\real^2\) and form an orthonormal basis for \(\real^2\text{.}\)</p>
<p id="p-5821">If we form the matrix</p>
<div class="displaymath">
\begin{equation*}
Q=\begin{bmatrix}
\uvec_1 \amp \uvec_2
\end{bmatrix}
= \begin{bmatrix}
1/\sqrt{2} \amp -1/\sqrt{2} \\
1/\sqrt{2} \amp 1/\sqrt{2} \\
\end{bmatrix}\text{,}
\end{equation*}
</div>
<p class="continuation">we find that \(Q^TQ = I\) since <a class="xref" data-knowl="./knowl/prop-transpose-multiplication.html" title="Proposition 6.2.8">Proposition 6.2.8</a> tells us that</p>
<div class="displaymath">
\begin{equation*}
Q^TQ = \begin{bmatrix}
\uvec_1\cdot\uvec_1 \amp \uvec_1\cdot\uvec_2 \\
\uvec_2\cdot\uvec_1 \amp \uvec_2\cdot\uvec_2 \\
\end{bmatrix}
=
\begin{bmatrix}
1 \amp 0 \\
0 \amp 1 \\
\end{bmatrix}
\end{equation*}
</div></article><p id="p-5822">The previous activity and example illustrate the next proposition.</p>
<article class="proposition theorem-like" id="prop-orthonormal-QTQ"><h6 class="heading">
<span class="type">Proposition</span><span class="space"> </span><span class="codenumber">6.3.8</span><span class="period">.</span>
</h6>
<p id="p-5823">If the columns of the \(m\times n\) matrix \(Q\) form an orthonormal set, then \(Q^TQ = I_n\text{,}\) the \(n\times n\) identity matrix.</p></article></section><section class="subsection" id="subsection-89"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">6.3.2</span> <span class="title">Orthgonal projections</span>
</h3>
<p id="p-5824">We now turn to an important problem that will appear in many forms in the rest of our investigations.  Suppose, as shown in <a class="xref" data-knowl="./knowl/fig-3d-orthog-proj.html" title="Figure 6.3.9">Figure 6.3.9</a>, that we have a subspace \(W\) of \(\real^m\) and a vector \(\bvec\) that is not in that subspace.  We would like to find the vector \(\bhat\) in \(W\) that is closest to \(\bvec\text{.}\)</p>
<figure class="figure figure-like" id="fig-3d-orthog-proj"><div class="sidebyside"><div class="sbsrow" style="margin-left:22.5%;margin-right:22.5%;"><div class="sbspanel top" style="width:100%;"><img src="images/3d-orthog-proj-3.svg" role="img" class="contained" alt=""></div></div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">6.3.9<span class="period">.</span></span><span class="space"> </span>Given a plane in \(\real^3\) and a vector \(\bvec\) not in the plane, we wish to find the vector \(\bhat\) in the plane that is closest to \(\bvec\text{.}\)</figcaption></figure><p id="p-5825">To get started, let's consider a simpler problem where we have a line \(L\) in \(\real^2\text{,}\) defined by the vector \(\wvec\text{,}\) and another vector \(\bvec\) that is not on the line, as shown on the left of <a class="xref" data-knowl="./knowl/fig-projection-line-a.html" title="Figure 6.3.10">Figure 6.3.10</a>.  We wish to find \(\bhat\text{,}\) the vector on the line that is closest to \(\bvec\text{,}\) as illustrated in the right of <a class="xref" data-knowl="./knowl/fig-projection-line-a.html" title="Figure 6.3.10">Figure 6.3.10</a>.</p>
<figure class="figure figure-like" id="fig-projection-line-a"><div class="sidebyside"><div class="sbsrow" style="margin-left:2.5%;margin-right:2.5%;">
<div class="sbspanel top" style="width:47.3684210526316%;"><img src="images/projection-line-1.svg" role="img" class="contained" alt=""></div>
<div class="sbspanel top" style="width:47.3684210526316%;"><img src="images/projection-line-4.svg" role="img" class="contained" alt=""></div>
</div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">6.3.10<span class="period">.</span></span><span class="space"> </span>Given a line \(L\) and a vector \(\bvec\text{,}\) we seek the vector \(\bhat\) on \(L\) that is closest to \(\bvec\text{.}\)</figcaption></figure><p id="p-5826">To find \(\bhat\text{,}\) we require that \(\bvec-\bhat\) be orthogonal to \(L\text{.}\)  For instance, if \(\yvec\) is another vector on the line, as shown in <a class="xref" data-knowl="./knowl/fig-projection-line-b.html" title="Figure 6.3.11">Figure 6.3.11</a>, then the Pythagorean theorem implies that</p>
<div class="displaymath">
\begin{equation*}
\len{\bvec-\yvec}^2 = |\bvec-\bhat|^2 +
|\bhat-\yvec|^2
\end{equation*}
</div>
<p class="continuation">which means that \(\len{\bvec-\yvec}\geq|\bvec-\bhat|\text{.}\) Therefore, \(\bhat\) is closer to \(\bvec\) than any other vector on the line \(L\text{.}\)</p>
<figure class="figure figure-like" id="fig-projection-line-b"><div class="sidebyside"><div class="sbsrow" style="margin-left:27.5%;margin-right:27.5%;"><div class="sbspanel top" style="width:100%;"><img src="images/projection-line-3.svg" role="img" class="contained" alt=""></div></div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">6.3.11<span class="period">.</span></span><span class="space"> </span>The vector \(\bhat\) is closer to \(\bvec\) than \(\yvec\) because \(\bvec-\bhat\) is orthogonal to \(L\text{.}\)</figcaption></figure><article class="definition definition-like" id="definition-28"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">6.3.12</span><span class="period">.</span>
</h6>
<p id="p-5827">Given a vector \(\bvec\) in \(\real^m\) and a subspace \(W\) of \(\real^m\text{,}\) the <em class="emphasis">orthogonal projection</em> of \(\bvec\) onto \(W\) is the vector \(\bhat\) in \(W\) that is closest to \(\bvec\text{.}\) It is characterized by the property that \(\bvec-\bhat\) is orthogonal to \(W\text{.}\)</p></article><article class="activity project-like" id="activity-76"><h6 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">6.3.3</span><span class="period">.</span>
</h6>
<p id="p-5828">This activity demonstrates how to determine the orthogonal projection of a vector onto a subspace of \(\real^m\text{.}\)</p>
<ol class="lower-alpha">
<li id="li-3984">
<p id="p-5829">Let's begin by considering a line \(L\text{,}\) defined by the vector \(\wvec=\twovec21\text{,}\) and a vector \(\bvec=\twovec24\) not on \(L\text{,}\) as illustrated in <a class="xref" data-knowl="./knowl/fig-projection-line-c.html" title="Figure 6.3.13">Figure 6.3.13</a>.</p>
<figure class="figure figure-like" id="fig-projection-line-c"><div class="sidebyside"><div class="sbsrow" style="margin-left:2.5%;margin-right:2.5%;">
<div class="sbspanel top" style="width:47.3684210526316%;"><img src="images/projection-line-4.svg" role="img" class="contained" alt=""></div>
<div class="sbspanel top" style="width:47.3684210526316%;"><img src="images/projection-line-2.svg" role="img" class="contained" alt=""></div>
</div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">6.3.13<span class="period">.</span></span><span class="space"> </span>Finding the orthogonal projection of \(\bvec\) onto the line defined by \(\wvec\text{.}\)</figcaption></figure><ol id="p-5830" class="lower-roman">
<li id="li-3985">
<p id="p-5831">To find \(\bhat\text{,}\) first notice that \(\bhat =
s\wvec\) for some scalar \(s\text{.}\) Since \(\bvec-\bhat = \bvec -
s\wvec\) is orthogonal to \(\wvec\text{,}\) what do we know about the dot product</p>
<div class="displaymath">
\begin{equation*}
(\bvec-s\wvec)\cdot\wvec\text{?}
\end{equation*}
</div>
</li>
<li id="li-3986"><p id="p-5832">Apply the distributive property of dot products to find the scalar \(s\text{.}\)  What is the vector \(\bhat\text{,}\) the orthogonal projection of \(\bvec\) onto \(L\text{?}\)</p></li>
<li id="li-3987">
<p id="p-5833">More generally, explain why the orthogonal projection of \(\bvec\) onto the line defined by \(\wvec\) is</p>
<div class="displaymath">
\begin{equation*}
\bhat=
\frac{\bvec\cdot\wvec}{\wvec\cdot\wvec}~\wvec\text{.} 
\end{equation*}
</div>
</li>
</ol>
</li>
<li id="li-3988">
<p id="p-5834">The same ideas apply more generally.  Suppose we have an orthogonal set of vectors \(\wvec_1=\threevec22{-1}\) and \(\wvec_2=\threevec102\) that define a plane \(W\) in \(\real^3\text{.}\) If \(\bvec=\threevec396\) another vector in \(\real^3\text{,}\) we seek the vector \(\bhat\) on the plane \(W\) closest to \(\bvec\text{.}\) As before, the vector \(\bvec-\bhat\) will be orthogonal to \(W\text{,}\) as illustrated in <a class="xref" data-knowl="./knowl/fig-3d-orthog.html" title="Figure 6.3.14">Figure 6.3.14</a>.</p>
<figure class="figure figure-like" id="fig-3d-orthog"><div class="sidebyside"><div class="sbsrow" style="margin-left:2.5%;margin-right:2.5%;">
<div class="sbspanel top" style="width:47.3684210526316%;"><img src="images/3d-orthog-proj-1.svg" role="img" class="contained" alt=""></div>
<div class="sbspanel top" style="width:47.3684210526316%;"><img src="images/3d-orthog-proj-2.svg" role="img" class="contained" alt=""></div>
</div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">6.3.14<span class="period">.</span></span><span class="space"> </span>Given a plane \(W\) defined by the orthogonal vectors \(\wvec_1\) and \(\wvec_2\) and another vector \(\bvec\text{,}\) we seek the vector \(\bhat\) on \(W\) closest to \(\bvec\text{.}\)</figcaption></figure><ol id="p-5835" class="lower-roman">
<li id="li-3989"><p id="p-5836">The vector \(\bvec-\bhat\) is orthogonal to \(W\text{.}\) What does this say about the dot products: \((\bvec-\bhat)\cdot\wvec_1\) and \((\bvec-\bhat)\cdot\wvec_2\text{?}\)</p></li>
<li id="li-3990">
<p id="p-5837">Since \(\bhat\) is in the plane \(W\text{,}\) we can write it as a linear combination \(\bhat = c_1\wvec_1 + c_2\wvec_2\text{.}\) Then</p>
<div class="displaymath">
\begin{equation*}
\bvec-\bhat = \bvec - (c_1\wvec_1+c_2\wvec_2)\text{.}
\end{equation*}
</div>
<p class="continuation">Find the weight \(c_1\) by dotting \(\bvec-\bhat\) with \(\wvec_1\) and applying the distributive property of dot products.  Similarly, find the weight \(c_2\text{.}\)</p>
</li>
<li id="li-3991"><p id="p-5838">What is the vector \(\bhat\text{,}\) the orthogonal projection of \(\wvec\) onto the plane \(W\text{?}\)</p></li>
</ol>
</li>
<li id="li-3992">
<p id="p-5839">Suppose that \(W\) is a subspace of \(\real^m\) with orthogonal basis \(\wvec_1,\wvec_2,\ldots,\wvec_n\) and that \(\bvec\) is a vector in \(\real^m\text{.}\)  Explain why the orthogonal projection of \(\bvec\) onto \(W\) is the vector</p>
<div class="displaymath">
\begin{equation*}
\bhat =
\frac{\bvec\cdot\wvec_1}{\wvec_1\cdot\wvec_1}~\wvec_1 + 
\frac{\bvec\cdot\wvec_2}{\wvec_2\cdot\wvec_2}~\wvec_2 +
\ldots + 
\frac{\bvec\cdot\wvec_n}{\wvec_n\cdot\wvec_n}~\wvec_n\text{.} 
\end{equation*}
</div>
</li>
<li id="li-3993">
<p id="p-5840">Suppose that \(\uvec_1,\uvec_2,\ldots,\uvec_n\) is an <em class="emphasis">orthonormal</em> basis for \(W\text{;}\)  that is, the vectors are orthogonal to one another and have unit length.  Explain why the orthogonal projection is</p>
<div class="displaymath">
\begin{equation*}
\bhat=
(\bvec\cdot\uvec_1)~\uvec_1 + 
(\bvec\cdot\uvec_2)~\uvec_2 +
\ldots +
(\bvec\cdot\uvec_n)~\uvec_n\text{.}
\end{equation*}
</div>
</li>
<li id="li-3994"><p id="p-5841">If \(Q=\begin{bmatrix}
\uvec_1 \amp \uvec_2 \amp \ldots \amp \uvec_n
\end{bmatrix}\) is the matrix whose columns are an orthonormal basis of \(W\text{,}\) use <a class="xref" data-knowl="./knowl/prop-transpose-multiplication.html" title="Proposition 6.2.8">Proposition 6.2.8</a> to explain why \(\bhat = QQ^T\bvec\text{.}\)</p></li>
</ol></article><p id="p-5868">In all the cases considered in the activity, we are looking for \(\bhat\text{,}\) the vector in a subspace \(W\) closest to a vector \(\bvec\text{,}\) which is found by requiring that \(\bvec-\bhat\) be orthogonal to \(W\text{.}\)  This means that \((\bvec-\bhat)\cdot\wvec = 0\) for any vector \(\wvec\) in \(W\text{.}\)</p>
<p id="p-5869">If we have an orthogonal basis \(\wvec_1,\wvec_2,\ldots,\wvec_n\) for \(W\text{,}\) then \(\bhat = c_1\wvec_1+c_w\wvec_2+\ldots c_n\wvec_n\text{.}\)  Therefore,</p>
<div class="displaymath">
\begin{align*}
(\bvec-\bhat)\cdot\wvec_i \amp = 0\\
\bvec\cdot\wvec_i \amp = \bhat\cdot\wvec_i\\
\bvec\cdot\wvec_i \amp =
(c_1\wvec_1+c_2\wvec_2+\ldots + c_n\wvec_n)\cdot\wvec_i\\
\bvec\cdot\wvec_i \amp = c_i\wvec_i\cdot\wvec_i\\
c_i \amp = \frac{\bvec\cdot\wvec_i}{\wvec_i\cdot\wvec_i}.
\end{align*}
</div>
<p class="continuation">This leads to the projection formula:</p>
<article class="proposition theorem-like" id="prop-proj-formula"><h6 class="heading">
<span class="type">Proposition</span><span class="space"> </span><span class="codenumber">6.3.15</span><span class="period">.</span><span class="space"> </span><span class="title">Projection formula.</span>
</h6>
<p id="p-5870">If \(W\) is a subspace of \(\real^m\) having an orthogonal basis \(\wvec_1,\wvec_2,\ldots, \wvec_n\) and \(\bvec\) is a vector in \(\real^m\text{,}\) then the orthogonal projection of \(\bvec\) onto \(W\) is</p>
<div class="displaymath">
\begin{equation*}
\bhat=
\frac{\bvec\cdot\wvec_1}{\wvec_1\cdot\wvec_1}~\wvec_1 + 
\frac{\bvec\cdot\wvec_2}{\wvec_2\cdot\wvec_2}~\wvec_2 +
\ldots +
\frac{\bvec\cdot\wvec_n}{\wvec_n\cdot\wvec_n}~\wvec_n\text{.}
\end{equation*}
</div></article><article class="assemblage assemblage-like" id="assemblage-12"><h6 class="heading"><span class="title">Caution.</span></h6>
<p id="p-5871">Remember that the projection formula given in <a class="xref" data-knowl="./knowl/prop-proj-formula.html" title="Proposition 6.3.15: Projection formula">Proposition 6.3.15</a> applies only when the basis \(\wvec_1,\wvec_2,\ldots,\wvec_n\) of \(W\) is <em class="emphasis">orthogonal</em>.</p></article><p id="p-5872">If we have an orthonormal basis \(\uvec_1,\uvec_2,\ldots,\uvec_n\) for \(W\text{,}\) the projection formula simplifies to</p>
<div class="displaymath">
\begin{equation*}
\bhat=
(\bvec\cdot\uvec_1)~\uvec_1 + 
(\bvec\cdot\uvec_2)~\uvec_2 +
\ldots + 
(\bvec\cdot\uvec_n)~\uvec_n\text{.}
\end{equation*}
</div>
<p class="continuation">If we then form the matrix</p>
<div class="displaymath">
\begin{equation*}
Q =
\begin{bmatrix}
\uvec_1 \amp \uvec_2 \amp \ldots \amp \uvec_n 
\end{bmatrix}\text{,}
\end{equation*}
</div>
<p class="continuation">this expression may be succintly written</p>
<div class="displaymath">
\begin{align*}
\bhat \amp {}={}
(\bvec\cdot\uvec_1)~\uvec_1 + 
(\bvec\cdot\uvec_2)~\uvec_2 +
\ldots + 
(\bvec\cdot\uvec_n)~\uvec_n\\
\amp {}={}
\begin{bmatrix}
\uvec_1\amp\uvec_2\amp\ldots\amp\uvec_n
\end{bmatrix}
\begin{bmatrix}
\uvec_1\cdot\bvec \\
\uvec_2\cdot\bvec \\
\vdots \\
\uvec_n\cdot\bvec \\
\end{bmatrix}\\
\amp {}={} QQ^T\bvec
\end{align*}
</div>
<p id="p-5873">This leads to the following proposition.</p>
<article class="proposition theorem-like" id="prop-proj-orthonormal"><h6 class="heading">
<span class="type">Proposition</span><span class="space"> </span><span class="codenumber">6.3.16</span><span class="period">.</span>
</h6>
<p id="p-5874">If \(\uvec_1,\uvec_2,\ldots,\uvec_n\) is an orthonormal basis for a subspace \(W\) of \(\real^m\text{,}\) then the matrix transformation that projects vectors in \(\real^m\) orthogonally onto \(W\) is represented by the matrix \(QQ^T\) where</p>
<div class="displaymath">
\begin{equation*}
Q =
\begin{bmatrix}
\uvec_1 \amp \uvec_2 \amp \ldots \amp \uvec_n \\
\end{bmatrix}\text{.}
\end{equation*}
</div></article><article class="example example-like" id="example-projection-matrix"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">6.3.17</span><span class="period">.</span>
</h6>
<p id="p-5875">In the previous activity, we looked at the plane \(W\) defined by the two orthogonal vectors</p>
<div class="displaymath">
\begin{equation*}
\wvec_1=\threevec22{-1},\hspace{24pt}
\wvec_2=\threevec102\text{.}
\end{equation*}
</div>
<p class="continuation">We can form an orthonormal basis by scalar multiplying these vectors to have unit length:</p>
<div class="displaymath">
\begin{equation*}
\uvec_1=\frac13\threevec22{-1} =
\threevec{2/3}{2/3}{-1/3},\hspace{24pt}
\uvec_2=\frac1{\sqrt{5}}\threevec102 =
\threevec{1/\sqrt{5}}0{2/\sqrt{5}}\text{.}
\end{equation*}
</div>
<p class="continuation">Using these vectors, we form the matrix</p>
<div class="displaymath">
\begin{equation*}
Q =
\begin{bmatrix}
2/3 \amp 1/\sqrt{5} \\
2/3 \amp 0 \\
-1/3 \amp 2/\sqrt{5} \\
\end{bmatrix}\text{.}
\end{equation*}
</div>
<p class="continuation">The projection onto the plane \(W\) is then given by the matrix</p>
<div class="displaymath">
\begin{equation*}
QQ^T =
\begin{bmatrix}
2/3 \amp 1/\sqrt{5} \\
2/3 \amp 0 \\
-1/3 \amp 2/\sqrt{5} \\
\end{bmatrix}
\begin{bmatrix}
2/3 \amp 2/3 \amp -1/3 \\
1/\sqrt{5} \amp 0 \amp 2/\sqrt{5} \\
\end{bmatrix}
=
\begin{bmatrix}
{29}/{45} \amp {4}/{9} \amp {8}/{45} \\
{4}/{9} \amp {4}/{9} \amp -{2}/{9} \\
{8}/{45} \amp -{2}/{9} \amp {41}/{45}
\end{bmatrix}\text{.}
\end{equation*}
</div>
<p id="p-5876">Let's check that this works by considering the vector \(\bvec=\threevec100\) and finding \(\bhat\text{,}\) its orthogonal projection onto the plane \(W\text{.}\)  In terms of the original basis \(\wvec_1\) and \(\wvec_2\text{,}\) the projection formula from <a class="xref" data-knowl="./knowl/prop-proj-formula.html" title="Proposition 6.3.15: Projection formula">Proposition 6.3.15</a> tells us that</p>
<div class="displaymath">
\begin{equation*}
\bhat=\frac{\bvec\cdot\wvec_1}
{\wvec_1\cdot\wvec_1}~\wvec_1 + 
\frac{\bvec\cdot\wvec_2}
{\wvec_2\cdot\wvec_2}~\wvec_2 
=
\threevec{{29}/{45}}{4/9}{8/{45}} \\
\end{equation*}
</div>
<p id="p-5877">Alternatively, we use the matrix \(QQ^T\text{,}\) as in <a class="xref" data-knowl="./knowl/prop-proj-orthonormal.html" title="Proposition 6.3.16">Proposition 6.3.16</a>, to find that</p>
<div class="displaymath">
\begin{equation*}
\bhat = QQ^T\bvec = 
\begin{bmatrix}
{29}/{45} \amp {4}/{9} \amp {8}/{45} \\
{4}/{9} \amp {4}/{9} \amp -{2}/{9} \\
{8}/{45} \amp -{2}/{9} \amp {41}/{45}
\end{bmatrix}\threevec100
=
\threevec{{29}/{45}}{4/9}{8/{45}}\text{.}
\end{equation*}
</div></article><article class="activity project-like" id="activity-77"><h6 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">6.3.4</span><span class="period">.</span>
</h6>
<ol id="p-5878" class="lower-alpha">
<li id="li-4017">
<p id="p-5879">Suppose that \(L\) is the line in \(\real^3\) defined by the vector \(\wvec=\threevec{1}{2}{-2}\text{.}\) <div class="sagecell-sage" id="sage-159"><script type="text/x-sage">
</script></div></p>
<ol class="lower-roman">
<li id="li-4018"><p id="p-5880">Find an orthonormal basis \(\uvec\) for \(L\text{.}\)</p></li>
<li id="li-4019"><p id="p-5881">Construct the matrix \(Q = \begin{bmatrix}\uvec\end{bmatrix}\) and use it to construct the matrix \(P\) that projects vectors orthogonally onto \(L\text{.}\)</p></li>
<li id="li-4020"><p id="p-5882">Use your matrix to find \(\bhat\text{,}\) the orthogonal projection of \(\bvec=\threevec111\) onto \(L\text{.}\)</p></li>
<li id="li-4021"><p id="p-5883">Find \(\rank(P)\) and explain its geometric significance.</p></li>
</ol>
</li>
<li id="li-4022">
<p id="p-5884">The vectors</p>
<div class="displaymath">
\begin{equation*}
\wvec_1 = \fourvec1111,\hspace{24pt}
\wvec_2 = \fourvec011{-2}
\end{equation*}
</div>
<p class="continuation">form an orthogonal basis of \(W\text{,}\) a two-dimensional subspace of \(\real^4\text{.}\) <div class="sagecell-sage" id="sage-160"><script type="text/x-sage">
</script></div></p>
<ol class="lower-roman">
<li id="li-4023"><p id="p-5885">Use the projection formula from <a class="xref" data-knowl="./knowl/prop-proj-formula.html" title="Proposition 6.3.15: Projection formula">Proposition 6.3.15</a> to find \(\bhat\text{,}\) the orthogonal projection of \(\bvec=\fourvec92{-2}3\) onto \(W\text{.}\)</p></li>
<li id="li-4024"><p id="p-5886">Find an orthonormal basis \(\uvec_1\) and \(\uvec_2\) for \(W\) and use it to construct the matrix \(P\) that projects vectors orthogonally onto \(W\text{.}\)  Check that \(P\bvec = \bhat\text{,}\) the orthogonal projection you found in the previous part of this activity.</p></li>
<li id="li-4025"><p id="p-5887">Find \(\rank(P)\) and explain its geometric significance.</p></li>
<li id="li-4026"><p id="p-5888">Find a basis for \(W^\perp\text{.}\)</p></li>
<li id="li-4027">
<p id="p-5889">Find a vector \(\bvec^\perp\) in \(W^\perp\) such that</p>
<div class="displaymath">
\begin{equation*}
\bvec = \bhat + \bvec^\perp.
\end{equation*}
</div>
</li>
<li id="li-4028"><p id="p-5890">Find the product \(Q^TQ\) and explain your result.</p></li>
</ol>
</li>
</ol></article><p id="p-5917">This activity demonstrates one issue of note.  We found \(\bhat\text{,}\) the orthogonal projection of \(\bvec\) onto \(W\text{,}\) by requiring that \(\bvec-\bhat\) be orthogonal to \(W\text{.}\)  In other words, \(\bvec-\bhat\) is a vector in the orthogonal complement \(W^\perp\text{,}\) which we may denote \(\bvec^\perp\text{.}\)  This explains the following proposition, which is illustrated in <a class="xref" data-knowl="./knowl/fig-orthog-decomp.html" title="Figure 6.3.19">Figure 6.3.19</a></p>
<article class="proposition theorem-like" id="prop-orthog-decomp"><h6 class="heading">
<span class="type">Proposition</span><span class="space"> </span><span class="codenumber">6.3.18</span><span class="period">.</span>
</h6>If \(W\) is a subspace of \(\real^n\) with orthogonal complement \(W^\perp\text{,}\) then any \(n\)-dimensional vector \(\bvec\) can be uniquely written as<div class="displaymath">
\begin{equation*}
\bvec = \bhat + \bvec^\perp
\end{equation*}
</div>where \(\bhat\) is in \(W\) and \(\bvec^\perp\) is in \(W^\perp\text{.}\)  The vector \(\bhat\) is the orthogonal projection of \(\bvec\) onto \(W\) and \(\bvec^\perp\) is the orthogonal projection of \(\bvec\) onto \(W^\perp\text{.}\)</article><figure class="figure figure-like" id="fig-orthog-decomp"><div class="sidebyside"><div class="sbsrow" style="margin-left:27.5%;margin-right:27.5%;"><div class="sbspanel top" style="width:100%;"><img src="images/orthog-decomp.svg" role="img" class="contained" alt=""></div></div></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">6.3.19<span class="period">.</span></span><span class="space"> </span>A vector \(\bvec\) along with \(\bhat\text{,}\) its orthogonal projection onto the line \(L\text{,}\) and \(\bvec^\perp\text{,}\) its orthogonal projection onto the orthogonal complement \(L^\perp\text{.}\)</figcaption></figure><p id="p-5918">Let's summarize what we've found.  If \(Q\) is a matrix whose columns \(\uvec_1, \uvec_2,\ldots,\uvec_n\) form an orthonormal set in \(\real^m\text{,}\) then</p>
<ul class="disc">
<li id="li-4053"><p id="p-5919">\(Q^TQ = I_n\text{,}\) the \(n\times n\) identity matrix, because this product computes the dot products between the columns of \(Q\text{.}\)</p></li>
<li id="li-4054"><p id="p-5920">\(QQ^T\) is the matrix the projects vectors orthogonally onto \(W\text{,}\) the subspace of \(\real^m\) spanned by \(\uvec_1,\ldots,\uvec_n\text{.}\)</p></li>
</ul>
<p class="continuation">As we've said before, matrix multiplication depends on the order in which we multiply the matrices, and we see this clearly here.</p>
<p id="p-5921">Because \(Q^TQ=I\text{,}\) there is a temptation to say that \(Q\) is invertible.  This is usually not the case, however. Remember that an invertible matrix must be a square matrix, and the matrix \(Q\) will only be square if \(n=m\text{.}\)  In this case, there are \(m\) vectors in the orthonormal set so the subspace \(W\) spanned by the vectors \(\uvec_1,\uvec_2,\ldots,\uvec_m\) is \(\real^m\text{.}\)  If \(\bvec\) is a vector in \(\real^m\text{,}\) then \(\bhat=QQ^T\bvec\) is the orthogonal projection of \(\bvec\) onto \(\real^m\text{.}\)  In other words, \(QQ^T\bvec\) is the closest vector in \(\real^m\) to \(\bvec\text{,}\) and this closest vector must be \(\bvec\) itself.  Therefore, \(QQ^T\bvec = \bvec\text{,}\) which means that \(QQ^T=I\text{.}\)  In this case, \(Q\) is an invertible matrix.</p>
<article class="example example-like" id="example-41"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">6.3.20</span><span class="period">.</span>
</h6>
<p id="p-5922">Consider the orthonormal set of vectors</p>
<div class="displaymath">
\begin{equation*}
\uvec_1=\threevec{1/\sqrt{3}}{-1/\sqrt{3}}{1/\sqrt{3}},
\hspace{24pt}
\uvec_2=\threevec{1/\sqrt{2}}{1/\sqrt{2}}0
\end{equation*}
</div>
<p class="continuation">and the matrix they define</p>
<div class="displaymath">
\begin{equation*}
Q = \begin{bmatrix}
1/\sqrt{3} \amp 1/\sqrt{2} \\
-1/\sqrt{3} \amp 1/\sqrt{2} \\
1/\sqrt{3} \amp 0 \\
\end{bmatrix}\text{.}
\end{equation*}
</div>
<p class="continuation">In this case, \(\uvec_1\) and \(\uvec_2\) span a plane, a 2-dimensional subspace of \(\real^3\text{.}\)  We know that \(Q^TQ = I_2\) and \(QQ^T\) projects vectors orthogonally onto the plane.  However, \(Q\) is not a square matrix so it cannot be invertible.</p></article><article class="example example-like" id="example-42"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">6.3.21</span><span class="period">.</span>
</h6>
<p id="p-5923">Now consider the orthonormal set of vectors</p>
<div class="displaymath">
\begin{equation*}
\uvec_1=\threevec{1/\sqrt{3}}{-1/\sqrt{3}}{1/\sqrt{3}},
\hspace{24pt}
\uvec_2=\threevec{1/\sqrt{2}}{1/\sqrt{2}}0,
\hspace{24pt}
\uvec_3=\threevec{1/\sqrt{6}}{-1/\sqrt{6}}{-2/\sqrt{6}}
\end{equation*}
</div>
<p class="continuation">and the matrix they define</p>
<div class="displaymath">
\begin{equation*}
Q = \begin{bmatrix}
1/\sqrt{3} \amp 1/\sqrt{2} \amp 1/\sqrt{6} \\
-1/\sqrt{3} \amp 1/\sqrt{2} \amp -1/\sqrt{6} \\
1/\sqrt{3} \amp 0 \amp -2/\sqrt{6} \\
\end{bmatrix}\text{.}
\end{equation*}
</div>
<p class="continuation">Here, \(\uvec_1\text{,}\) \(\uvec_2\text{,}\) and \(\uvec_3\) form a basis for \(\real^3\) so that both \(Q^TQ=I_3\) and \(QQ^T=I_3\text{.}\)  Therefore, \(Q\) is a square matrix and is invertible.</p>
<p id="p-5924">Moreover, since \(Q^TQ = I\text{,}\) we see that \(Q^{-1} =
Q^T\) so finding the inverse of \(Q\) is as simple as writing its transpose.  Matrices with this property are very special and will play an important role in our upcoming work.  We will therefore give them a special name.</p></article><article class="definition definition-like" id="definition-29"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">6.3.22</span><span class="period">.</span>
</h6>
<p id="p-5925">A square \(m\times m\) matrix \(Q\) whose columns form an orthonormal basis for \(\real^m\) is called <em class="emphasis">orthogonal</em>.</p></article><p id="p-5926">This terminology can be a little confusing.  We call a basis orthogonal if the basis vectors are orthogonal to one another. However, a matrix is orthogonal if the columns are orthogonal to one another and have unit length.  It pays to keep this in mind when reading statements about orthogonal bases and orthogonal matrices.  In the meantime, we record the following proposition.</p>
<article class="proposition theorem-like" id="prop-orthog-matrix"><h6 class="heading">
<span class="type">Proposition</span><span class="space"> </span><span class="codenumber">6.3.23</span><span class="period">.</span>
</h6>
<p id="p-5927">An orthogonal matrix \(Q\) is invertible and its inverse \(Q^{-1} = Q^T\text{.}\)</p></article></section><section class="subsection" id="subsection-90"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">6.3.3</span> <span class="title">Summary</span>
</h3>
<p id="p-5928">This section introduced orthogonal sets and the projection formula that allows us to project vectors orthogonally onto a subspace.</p>
<ul class="disc">
<li id="li-4055">
<p id="p-5929">Given an orthogonal set \(\wvec_1,\wvec_2,\ldots,\wvec_n\) that spans an \(n\)-dimensional subspace \(W\) of \(\real^m\text{,}\) the orthogonal projection of \(\bvec\) onto \(W\) is the vector in \(W\) closest to \(\bvec\) and may be written as</p>
<div class="displaymath">
\begin{equation*}
\bhat =
\frac{\bvec\cdot\wvec_1}{\wvec_1\cdot\wvec_1}~\wvec_1 + 
\frac{\bvec\cdot\wvec_2}{\wvec_2\cdot\wvec_2}~\wvec_2 +
\ldots + 
\frac{\bvec\cdot\wvec_n}{\wvec_n\cdot\wvec_n}~\wvec_n\text{.}
\end{equation*}
</div>
</li>
<li id="li-4056"><p id="p-5930">If \(\uvec_1,\uvec_2,\ldots,\uvec_n\) is an orthonormal basis of \(W\) and \(Q\) is the matrix whose columns are \(\uvec_i\text{,}\) then the matrix \(P=QQ^T\) projects vectors orthogonally onto \(W\text{.}\)</p></li>
<li id="li-4057"><p id="p-5931">If the columns of \(Q\) form an orthonormal basis for an \(n\)-dimensional subspace of \(\real^m\text{,}\) then \(Q^TQ=I_n\text{.}\)</p></li>
<li id="li-4058"><p id="p-5932">An orthogonal matrix \(Q\) is a square matrix whose columns form an orthonormal basis.  In this case, \(QQ^T=Q^TQ = I\) so that \(Q^{-1} = Q^T\text{.}\)</p></li>
</ul></section><section class="exercises" id="exercises-24"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">6.3.4</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-212"><h6 class="heading"><span class="codenumber">1<span class="period">.</span></span></h6>
<p id="p-5933">Suppose that</p>
<div class="displaymath">
\begin{equation*}
\wvec_1=\threevec111,\hspace{24pt}
\wvec_2=\threevec1{-2}1.
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-4059"><p id="p-5934">Verify that \(\wvec_1\) and \(\wvec_2\) form an orthogonal basis for a plane \(W\) in \(\real^3\text{.}\)</p></li>
<li id="li-4060"><p id="p-5935">Use <a class="xref" data-knowl="./knowl/prop-proj-formula.html" title="Proposition 6.3.15: Projection formula">Proposition 6.3.15</a> to find \(\bhat\text{,}\) the orthogonal projection of \(\bvec=\threevec21{-1}\) onto \(W\text{.}\)</p></li>
<li id="li-4061"><p id="p-5936">Find an orthonormal basis \(\uvec_1\text{,}\) \(\uvec_2\) for \(W\text{.}\)</p></li>
<li id="li-4062"><p id="p-5937">Find the matrix \(P\) representing the matrix transformation that projects vectors in \(\real^3\) orthogonally onto \(W\text{.}\)  Verify that \(\bhat =
P\bvec\text{.}\)</p></li>
<li id="li-4063"><p id="p-5938">Determine \(\rank(P)\) and explain its geometric significance.</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-213"><h6 class="heading"><span class="codenumber">2<span class="period">.</span></span></h6>
<p id="p-5951">Consider the vectors</p>
<div class="displaymath">
\begin{equation*}
\wvec_1=\threevec111,\hspace{24pt}
\wvec_2=\threevec{-1}01,\hspace{24pt}
\wvec_3=\threevec1{-2}1.
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-4074"><p id="p-5952">Explain why these vectors form an orthogonal basis for \(\real^3\text{.}\)</p></li>
<li id="li-4075"><p id="p-5953">Suppose that \(A=\begin{bmatrix} \wvec_1 \amp \wvec_2 \amp \wvec_3
\end{bmatrix}\) and evaluate the product \(A^TA\text{.}\) Why is this product a diagonal matrix and what is the significance of the diagonal entries?</p></li>
<li id="li-4076"><p id="p-5954">Express the vector \(\bvec=\threevec{-3}{-6}3\) as a linear combination of \(\wvec_1\text{,}\) \(\wvec_2\text{,}\) and \(\wvec_3\text{.}\)</p></li>
<li id="li-4077"><p id="p-5955">Multiply the vectors \(\wvec_1\text{,}\) \(\wvec_2\text{,}\) \(\wvec_3\) by appropriate scalars to find an orthonormal basis \(\uvec_1\text{,}\) \(\uvec_2\text{,}\) \(\uvec_3\) of \(\real^3\text{.}\)</p></li>
<li id="li-4078"><p id="p-5956">If \(Q=\begin{bmatrix} \uvec_1 \amp \uvec_2 \amp
\uvec_3
\end{bmatrix}\text{,}\) find the matrix product \(QQ^T\) and explain the result.</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-214"><h6 class="heading"><span class="codenumber">3<span class="period">.</span></span></h6>
<p id="p-5969">Suppose that</p>
<div class="displaymath">
\begin{equation*}
\wvec_1=\fourvec110{-1}, \hspace{24pt}
\wvec_2=\fourvec1011
\end{equation*}
</div>
<p class="continuation">form an orthogonal basis for a subspace \(W\) of \(\real^4\text{.}\)</p>
<ol class="lower-alpha">
<li id="li-4089"><p id="p-5970">Find \(\bhat\text{,}\) the orthogonal projection of \(\bvec=\fourvec{2}{-1}{-6}{7}\) onto \(W\text{.}\)</p></li>
<li id="li-4090"><p id="p-5971">Find the vector \(\bvec^\perp\) in \(W^\perp\) such that \(\bvec = \bhat + \bvec^\perp\text{.}\)</p></li>
<li id="li-4091"><p id="p-5972">Find a basis for \(W^\perp\text{.}\) and express \(\bvec^\perp\) as a linear combination of the basis vectors.</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-215"><h6 class="heading"><span class="codenumber">4<span class="period">.</span></span></h6>
<p id="p-5981">Consider the vectors</p>
<div class="displaymath">
\begin{equation*}
\wvec_1=\fourvec1100,\hspace{24pt}
\wvec_2=\fourvec0011,\hspace{24pt}
\bvec=\fourvec2{-4}13.
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-4098"><p id="p-5982">If \(L\) is the line defined by the vector \(\wvec_1\text{,}\) find the vector in \(L\) closest to \(\bvec\text{.}\)  Call this vector \(\bhat_1\text{.}\)</p></li>
<li id="li-4099"><p id="p-5983">If \(W\) is the subspace spanned by \(\wvec_1\) and \(\wvec_2\text{,}\) find the vector in \(W\) closest to \(\bvec\text{.}\)  Call this vector \(\bhat_2\text{.}\)</p></li>
<li id="li-4100"><p id="p-5984">Determine whether \(\bhat_1\) or \(\bhat_2\) is closer to \(\bvec\) and explain why.</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-216"><h6 class="heading"><span class="codenumber">5<span class="period">.</span></span></h6>
<p id="p-5993">Suppose that \(\wvec=\threevec2{-1}2\) defines a line \(L\) in \(\real^3\text{.}\)</p>
<ol class="lower-alpha">
<li id="li-4107"><p id="p-5994">Find the orthogonal projections of the vectors \(\threevec100\text{,}\) \(\threevec010\text{,}\) \(\threevec001\) onto \(L\text{.}\)</p></li>
<li id="li-4108"><p id="p-5995">Find the matrix \(P = \frac{1}{\len{\wvec}^2} \wvec
\wvec^T\text{.}\)</p></li>
<li id="li-4109"><p id="p-5996">Use <a class="xref" data-knowl="./knowl/prop-linear-trans-columns.html" title="Proposition 2.5.4">Proposition 2.5.4</a> to explain why the columns of \(P\) are related to the orthogonal projections you found in the first part of this exericse.</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-217"><h6 class="heading"><span class="codenumber">6<span class="period">.</span></span></h6>
<p id="p-6005">Suppose that</p>
<div class="displaymath">
\begin{equation*}
\vvec_1=\threevec103,\hspace{24pt}
\vvec_2=\threevec222
\end{equation*}
</div>
<p class="continuation">form the basis for a plane \(W\) in \(\real^3\text{.}\)</p>
<ol class="lower-alpha">
<li id="li-4116"><p id="p-6006">Find a basis for the line that is the orthogonal complement \(W^\perp\text{.}\)</p></li>
<li id="li-4117"><p id="p-6007">Given the vector \(\bvec=\threevec6{-6}2\text{,}\) find \(\yvec\text{,}\) the orthogonal projection of \(\bvec\) onto the line \(W^\perp\text{.}\)</p></li>
<li id="li-4118"><p id="p-6008">Explain why the vector \(\zvec = \bvec-\yvec\) must be in \(W\) and write \(\zvec\) as a linear combination of \(\vvec_1\) and \(\vvec_2\text{.}\)</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-218"><h6 class="heading"><span class="codenumber">7<span class="period">.</span></span></h6>
<p id="p-6017">Determine whether the following statements are true or false and explain your thinking.</p>
<ol class="lower-alpha">
<li id="li-4125"><p id="p-6018">If the columns of \(Q\) form an orthonormal basis for a subspace \(W\) and \(\wvec\) is a vector in \(W\text{,}\) then \(QQ^T\wvec = \wvec\text{.}\)</p></li>
<li id="li-4126"><p id="p-6019">An orthogonal set of vectors in \(\real^8\) can have no more than 8 vectors.</p></li>
<li id="li-4127"><p id="p-6020">If \(Q\) is a \(7\times5\) matrix whose columns are orthonormal, then \(QQ^T = I_7\text{.}\)</p></li>
<li id="li-4128"><p id="p-6021">If \(Q\) is a \(7\times5\) matrix whose columns are orthonormal, then \(Q^TQ = I_5\text{.}\)</p></li>
<li id="li-4129"><p id="p-6022">Suppose that the orthogonal projection of \(\bvec\) onto a subspace \(W\) satisfies \(\bhat =
\zerovec\text{.}\)  Then \(\bvec\) is in \(W^\perp\text{.}\)</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-219"><h6 class="heading"><span class="codenumber">8<span class="period">.</span></span></h6>
<p id="p-6034">Suppose that \(Q\) is an orthogonal matrix.</p>
<ol class="lower-alpha">
<li id="li-4140">
<p id="p-6035">Remembering that \(\vvec\cdot\wvec=\vvec^T\wvec\text{,}\) explain why</p>
<div class="displaymath">
\begin{equation*}
Q\xvec\cdot(Q\yvec) = \xvec\cdot\yvec.
\end{equation*}
</div>
</li>
<li id="li-4141">
<p id="p-6036">Explain why \(\len{Q\xvec} = \len{\xvec}\text{.}\)</p>
<p id="p-6037">This means that the length of a vector is unchanged after multiplying by an orthogonal matrix.</p>
</li>
<li id="li-4142"><p id="p-6038">If \(\lambda\) is a real eigenvalue of \(Q\text{,}\) explain why \(\lambda=\pm1\text{.}\)</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-220"><h6 class="heading"><span class="codenumber">9<span class="period">.</span></span></h6>
<p id="p-6047">Explain why the following statements are true.</p>
<ol class="lower-alpha">
<li id="li-4149"><p id="p-6048">If \(Q\) is an orthogonal matrix, then \(\det Q =
\pm 1\text{.}\)</p></li>
<li id="li-4150"><p id="p-6049">If \(Q\) is a \(8\times 4\) matrix whose columns are orthonormal, then \(QQ^T\) is an \(8\times8\) matrix whose rank is 4.</p></li>
<li id="li-4151"><p id="p-6050">If \(\bhat\) is the orthogonal projection of \(\bvec\) onto a subspace \(W\text{,}\) then \(\bvec-\bhat\) is the orthogonal projection of \(\bvec\) onto \(W^\perp\text{.}\)</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-221"><h6 class="heading"><span class="codenumber">10<span class="period">.</span></span></h6>
<p id="p-6059">This exercise is about \(2\times2\) orthogonal matrices.</p>
<ol class="lower-alpha">
<li id="li-4158"><p id="p-6060">In <a href="sec-transforms-geom.html" class="internal" title="Section 2.6: The geometry of matrix transformations">Section 2.6</a>, we saw that the matrix \(\begin{bmatrix}
\cos\theta \amp -\sin\theta \\
\sin\theta \amp \cos\theta
\end{bmatrix}\) represents a rotation by an angle \(\theta\text{.}\) Explain why this matrix is an orthogonal matrix.</p></li>
<li id="li-4159"><p id="p-6061">We also saw that the matrix \(\begin{bmatrix}
\cos\theta \amp \sin\theta \\
\sin\theta \amp -\cos\theta
\end{bmatrix}\) represents a reflection in a line. Explain why this matrix is an orthogonal matrix.</p></li>
<li id="li-4160"><p id="p-6062">Suppose that \(\uvec_1=\twovec{\cos\theta}{\sin\theta}\) is a 2-dimensional unit vector.  Use a sketch to indicate all the possible vectors \(\uvec_2\) such that \(\uvec_1\) and \(\uvec_2\) form an orthonormal basis of \(\real^2\text{.}\)</p></li>
<li id="li-4161"><p id="p-6063">Explain why every \(2\times2\) orthogonal matrix is either a rotation or a reflection.</p></li>
</ol></article></section></section></div></main>
</div>
</body>
</html>
