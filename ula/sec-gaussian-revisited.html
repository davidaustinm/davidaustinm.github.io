<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2022-08-08T13:56:12-04:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US">
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Gaussian elimination revisited</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="Understanding Linear Algebra">
<meta property="book:author" content=" David Austin ">
<script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script><script>window.MathJax = {
  tex: {
    inlineMath: [['\\(','\\)']],
    tags: "none",
    tagSide: "right",
    tagIndent: ".8em",
    packages: {'[+]': ['base', 'extpfeil', 'ams', 'amscd', 'newcommand', 'knowl']}
  },
  options: {
    ignoreHtmlClass: "tex2jax_ignore|ignore-math",
    processHtmlClass: "process-math",
    renderActions: {
        findScript: [10, function (doc) {
            document.querySelectorAll('script[type^="math/tex"]').forEach(function(node) {
                var display = !!node.type.match(/; *mode=display/);
                var math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                var text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = {node: text, delim: '', n: 0};
                math.end = {node: text, delim: '', n: 0};
                doc.math.push(math);
            });
        }, '']
    },
  },
  chtml: {
    scale: 0.88,
    mtextInheritFont: true
  },
  loader: {
    load: ['input/asciimath', '[tex]/extpfeil', '[tex]/amscd', '[tex]/newcommand', '[pretext]/mathjaxknowl3.js'],
    paths: {pretext: "https://pretextbook.org/js/lib"},
  },
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><script>// Make *any* pre with class 'sagecell-sage' an executable Sage cell
// Their results will be linked, only within language type
sagecell.makeSagecell({inputLocation: 'pre.sagecell-sage',
                       linked: true,
                       languages: ['sage'],
                       evalButtonText: 'Evaluate (Sage)'});
</script><script async="" src="https://cse.google.com/cse.js?cx=015103900096539427448:ngwuia10qci"></script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.13/pretext.js"></script><script>miniversion=0.674</script><script src="https://pretextbook.org/js/0.13/pretext_add_on.js?x=1"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/banner_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/toc_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/knowls_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/style_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/colors_blue_grey.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/setcolors.css" rel="stylesheet" type="text/css">
<!--** eBookCongig is necessary to configure interactive       **-->
<!--** Runestone components to run locally in reader's browser **-->
<!--** No external communication:                              **-->
<!--**     log level is 0, Runestone Services are disabled     **-->
<script type="text/javascript">
eBookConfig = {};
eBookConfig.useRunestoneServices = false;
eBookConfig.host = 'http://127.0.0.1:8000';
eBookConfig.course = 'PTX Course: Title Here';
eBookConfig.basecourse = 'PTX Base Course';
eBookConfig.isLoggedIn = false;
eBookConfig.email = '';
eBookConfig.isInstructor = false;
eBookConfig.logLevel = 0;
eBookConfig.username = '';
eBookConfig.readings = null;
eBookConfig.activities = null;
eBookConfig.downloadsEnabled = false;
eBookConfig.allow_pairs = false;
eBookConfig.enableScratchAC = false;
eBookConfig.build_info = "";
eBookConfig.python3 = null;
eBookConfig.acDefaultLanguage = 'python';
eBookConfig.runestone_version = '5.0.1';
eBookConfig.jobehost = '';
eBookConfig.proxyuri_runs = '';
eBookConfig.proxyuri_files = '';
eBookConfig.enable_chatcodes =  false;
</script>
<!--*** Runestone Services ***-->
<script type="text/javascript" src="https://runestone.academy/cdn/runestone/6.2.1/runtime.b0f8547c48f16a9f.bundle.js"></script><script type="text/javascript" src="https://runestone.academy/cdn/runestone/6.2.1/637.d54be67956c5c660.bundle.js"></script><script type="text/javascript" src="https://runestone.academy/cdn/runestone/6.2.1/runestone.0e9550fe42760516.bundle.js"></script><link rel="stylesheet" type="text/css" href="https://runestone.academy/cdn/runestone/6.2.1/637.fafafbd97df8a0d1.css">
<link rel="stylesheet" type="text/css" href="https://runestone.academy/cdn/runestone/6.2.1/runestone.e4d5592da655219f.css">
</head>
<body class="pretext-book ignore-math has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\newcommand{\avec}{{\mathbf a}}
\newcommand{\bvec}{{\mathbf b}}
\newcommand{\cvec}{{\mathbf c}}
\newcommand{\dvec}{{\mathbf d}}
\newcommand{\dtil}{\widetilde{\mathbf d}}
\newcommand{\evec}{{\mathbf e}}
\newcommand{\fvec}{{\mathbf f}}
\newcommand{\nvec}{{\mathbf n}}
\newcommand{\pvec}{{\mathbf p}}
\newcommand{\qvec}{{\mathbf q}}
\newcommand{\svec}{{\mathbf s}}
\newcommand{\tvec}{{\mathbf t}}
\newcommand{\uvec}{{\mathbf u}}
\newcommand{\vvec}{{\mathbf v}}
\newcommand{\wvec}{{\mathbf w}}
\newcommand{\xvec}{{\mathbf x}}
\newcommand{\yvec}{{\mathbf y}}
\newcommand{\zvec}{{\mathbf z}}
\newcommand{\rvec}{{\mathbf r}}
\newcommand{\mvec}{{\mathbf m}}
\newcommand{\zerovec}{{\mathbf 0}}
\newcommand{\onevec}{{\mathbf 1}}
\newcommand{\real}{{\mathbb R}}
\newcommand{\twovec}[2]{\left[\begin{array}{r}#1 \\ #2
\end{array}\right]}
\newcommand{\ctwovec}[2]{\left[\begin{array}{c}#1 \\ #2
\end{array}\right]}
\newcommand{\threevec}[3]{\left[\begin{array}{r}#1 \\ #2 \\ #3
\end{array}\right]}
\newcommand{\cthreevec}[3]{\left[\begin{array}{c}#1 \\ #2 \\ #3
\end{array}\right]}
\newcommand{\fourvec}[4]{\left[\begin{array}{r}#1 \\ #2 \\ #3 \\ #4
\end{array}\right]}
\newcommand{\cfourvec}[4]{\left[\begin{array}{c}#1 \\ #2 \\ #3 \\ #4
\end{array}\right]}
\newcommand{\fivevec}[5]{\left[\begin{array}{r}#1 \\ #2 \\ #3 \\
#4 \\ #5 \\ \end{array}\right]}
\newcommand{\cfivevec}[5]{\left[\begin{array}{c}#1 \\ #2 \\ #3 \\
#4 \\ #5 \\ \end{array}\right]}
\newcommand{\mattwo}[4]{\left[\begin{array}{rr}#1 \amp #2 \\ #3 \amp #4 \\ \end{array}\right]}
\newcommand{\laspan}[1]{\text{Span}\{#1\}}
\newcommand{\bcal}{{\cal B}}
\newcommand{\ccal}{{\cal C}}
\newcommand{\scal}{{\cal S}}
\newcommand{\wcal}{{\cal W}}
\newcommand{\ecal}{{\cal E}}
\newcommand{\coords}[2]{\left\{#1\right\}_{#2}}
\newcommand{\gray}[1]{\color{gray}{#1}}
\newcommand{\lgray}[1]{\color{lightgray}{#1}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\row}{\text{Row}}
\newcommand{\col}{\text{Col}}
\renewcommand{\row}{\text{Row}}
\newcommand{\nul}{\text{Nul}}
\newcommand{\var}{\text{Var}}
\newcommand{\corr}{\text{corr}}
\newcommand{\len}[1]{\left|#1\right|}
\newcommand{\bbar}{\overline{\bvec}}
\newcommand{\bhat}{\widehat{\bvec}}
\newcommand{\bperp}{\bvec^\perp}
\newcommand{\xhat}{\widehat{\xvec}}
\newcommand{\vhat}{\widehat{\vvec}}
\newcommand{\uhat}{\widehat{\uvec}}
\newcommand{\what}{\widehat{\wvec}}
\newcommand{\Sighat}{\widehat{\Sigma}}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\definecolor{fillinmathshade}{gray}{0.9}
\newcommand{\fillinmath}[1]{\mathchoice{\colorbox{fillinmathshade}{$\displaystyle     \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\textstyle        \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptstyle      \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptscriptstyle\phantom{\,#1\,}$}}}
\)</span></div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="ula.html"><span class="title">Understanding Linear Algebra</span></a></h1>
<p class="byline">David Austin</p>
</div>
<div class="searchwrapper" role="search"><div class="gcse-search"></div></div>
</div></div>
<nav id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3">
<a class="index-button toolbar-item button" href="index-1.html" title="Index">Index</a><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="chap5.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="chap5.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="sec-power-method.html" title="Next">Next</a></span>
</div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="chap5.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="chap5.html" title="Up">Up</a><a class="next-button button toolbar-item" href="sec-power-method.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link frontmatter">
<a href="frontmatter.html" data-scroll="frontmatter" class="internal"><span class="title">Front Matter</span></a><ul>
<li><a href="dedication-1.html" data-scroll="dedication-1" class="internal">Dedication</a></li>
<li><a href="colophon-1.html" data-scroll="colophon-1" class="internal">Colophon</a></li>
<li><a href="preface-1.html" data-scroll="preface-1" class="internal">Our goals</a></li>
</ul>
</li>
<li class="link">
<a href="chap1.html" data-scroll="chap1" class="internal"><span class="codenumber">1</span> <span class="title">Systems of equations</span></a><ul>
<li><a href="sec-expect.html" data-scroll="sec-expect" class="internal">What can we expect</a></li>
<li><a href="sec-finding-solutions.html" data-scroll="sec-finding-solutions" class="internal">Finding solutions to linear systems</a></li>
<li><a href="sec-sage-introduction.html" data-scroll="sec-sage-introduction" class="internal">Computation with Sage</a></li>
<li><a href="sec-pivots.html" data-scroll="sec-pivots" class="internal">Pivots and their influence on solution spaces</a></li>
</ul>
</li>
<li class="link">
<a href="chap2.html" data-scroll="chap2" class="internal"><span class="codenumber">2</span> <span class="title">Vectors, matrices, and linear combinations</span></a><ul>
<li><a href="sec-vectors-lin-combs.html" data-scroll="sec-vectors-lin-combs" class="internal">Vectors and linear combinations</a></li>
<li><a href="sec-matrices-lin-combs.html" data-scroll="sec-matrices-lin-combs" class="internal">Matrix multiplication and linear combinations</a></li>
<li><a href="sec-span.html" data-scroll="sec-span" class="internal">The span of a set of vectors</a></li>
<li><a href="sec-linear-dep.html" data-scroll="sec-linear-dep" class="internal">Linear independence</a></li>
<li><a href="sec-linear-trans.html" data-scroll="sec-linear-trans" class="internal">Matrix transformations</a></li>
<li><a href="sec-transforms-geom.html" data-scroll="sec-transforms-geom" class="internal">The geometry of matrix transformations</a></li>
</ul>
</li>
<li class="link">
<a href="chap3.html" data-scroll="chap3" class="internal"><span class="codenumber">3</span> <span class="title">Invertibility, bases, and coordinate systems</span></a><ul>
<li><a href="sec-matrix-inverse.html" data-scroll="sec-matrix-inverse" class="internal">Invertibility</a></li>
<li><a href="sec-bases.html" data-scroll="sec-bases" class="internal">Bases and coordinate systems</a></li>
<li><a href="sec-jpeg.html" data-scroll="sec-jpeg" class="internal">Image compression</a></li>
<li><a href="sec-determinants.html" data-scroll="sec-determinants" class="internal">Determinants</a></li>
<li><a href="sec-subspaces.html" data-scroll="sec-subspaces" class="internal">Subspaces</a></li>
</ul>
</li>
<li class="link">
<a href="chap4.html" data-scroll="chap4" class="internal"><span class="codenumber">4</span> <span class="title">Eigenvalues and eigenvectors</span></a><ul>
<li><a href="sec-eigen-intro.html" data-scroll="sec-eigen-intro" class="internal">An introduction to eigenvalues and eigenvectors</a></li>
<li><a href="sec-eigen-find.html" data-scroll="sec-eigen-find" class="internal">Finding eigenvalues and eigenvectors</a></li>
<li><a href="sec-eigen-diag.html" data-scroll="sec-eigen-diag" class="internal">Diagonalization, similarity, and powers of a matrix</a></li>
<li><a href="sec-dynamical.html" data-scroll="sec-dynamical" class="internal">Dynamical systems</a></li>
<li><a href="sec-stochastic.html" data-scroll="sec-stochastic" class="internal">Markov chains and Google's PageRank algorithm</a></li>
</ul>
</li>
<li class="link">
<a href="chap5.html" data-scroll="chap5" class="internal"><span class="codenumber">5</span> <span class="title">Linear algebra and computing</span></a><ul>
<li><a href="sec-gaussian-revisited.html" data-scroll="sec-gaussian-revisited" class="active">Gaussian elimination revisited</a></li>
<li><a href="sec-power-method.html" data-scroll="sec-power-method" class="internal">Finding eigenvectors numerically</a></li>
</ul>
</li>
<li class="link">
<a href="chap6.html" data-scroll="chap6" class="internal"><span class="codenumber">6</span> <span class="title">Orthogonality and Least Squares</span></a><ul>
<li><a href="sec-dot-product.html" data-scroll="sec-dot-product" class="internal">The dot product</a></li>
<li><a href="sec-transpose.html" data-scroll="sec-transpose" class="internal">Orthogonal complements and the matrix transpose</a></li>
<li><a href="sec-orthogonal-bases.html" data-scroll="sec-orthogonal-bases" class="internal">Orthogonal bases and projections</a></li>
<li><a href="sec-gram-schmidt.html" data-scroll="sec-gram-schmidt" class="internal">Finding orthogonal bases</a></li>
<li><a href="sec-least-squares.html" data-scroll="sec-least-squares" class="internal">Orthogonal least squares</a></li>
</ul>
</li>
<li class="link">
<a href="chap7.html" data-scroll="chap7" class="internal"><span class="codenumber">7</span> <span class="title">The Spectral Theorem and singular value decompositions</span></a><ul>
<li><a href="sec-symmetric-matrices.html" data-scroll="sec-symmetric-matrices" class="internal">Symmetric matrices and variance</a></li>
<li><a href="sec-quadratic-forms.html" data-scroll="sec-quadratic-forms" class="internal">Quadratic forms</a></li>
<li><a href="sec-pca.html" data-scroll="sec-pca" class="internal">Principal Component Analysis</a></li>
<li><a href="sec-svd-intro.html" data-scroll="sec-svd-intro" class="internal">Singular Value Decompositions</a></li>
<li><a href="sec-svd-uses.html" data-scroll="sec-svd-uses" class="internal">Using Singular Value Decompositions</a></li>
</ul>
</li>
<li class="link backmatter"><a href="backmatter.html" data-scroll="backmatter" class="internal"><span class="title">Back Matter</span></a></li>
<li class="link"><a href="app-sage-reference.html" data-scroll="app-sage-reference" class="internal"><span class="codenumber">A</span> <span class="title">Sage Reference</span></a></li>
<li class="link"><a href="index-1.html" data-scroll="index-1" class="internal"><span class="title">Index</span></a></li>
<li class="link"><a href="colophon-2.html" data-scroll="colophon-2" class="internal"><span class="title">Colophon</span></a></li>
</ul></nav><div class="extras"><nav><a class="pretext-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content"><section class="section" id="sec-gaussian-revisited"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">5.1</span> <span class="title">Gaussian elimination revisited</span>
</h2>
<section class="introduction" id="introduction-25"><p id="p-5077">In this section, we revisit Gaussian elimination and explore some problems with implementing it in the straightforward way that we described back in <a href="sec-finding-solutions.html" class="internal" title="Section 1.2: Finding solutions to linear systems">Section 1.2</a>.  In particular, we will see how the fact that computers only approximate arithmetic operations can lead us to find solutions that are far from the actual solutions.  Second, we will explore how much work is required to implement Gaussian elimination and devise a more efficient means of implementing it when we want to solve equations <span class="process-math">\(A\xvec = \bvec\)</span> for several different vectors <span class="process-math">\(\bvec\text{.}\)</span></p>
<article class="exploration project-like" id="exploration-19"><h3 class="heading">
<span class="type">Preview Activity</span><span class="space"> </span><span class="codenumber">5.1.1</span><span class="period">.</span>
</h3>
<p id="p-5078">To begin, let's recall how we implemented Gaussian elimination by considering the matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = \left[\begin{array}{rrrr}
1 \amp 2 \amp -1 \amp 2 \\
1 \amp 0 \amp -2 \amp 1 \\
3 \amp 2 \amp 1 \amp 0 \\
\end{array}\right]
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-3490"><p id="p-5079">What is the first row operation we perform? If the resulting matrix is <span class="process-math">\(A_1\text{,}\)</span> find a matrix <span class="process-math">\(E_1\)</span> such that <span class="process-math">\(E_1A =
A_1\text{.}\)</span></p></li>
<li id="li-3491"><p id="p-5080">What is the matrix inverse <span class="process-math">\(E_1^{-1}\text{?}\)</span>  You can find this using your favorite technique for finding a matrix inverse.  However, it may be easier to think about the effect that the row operation has and how it can be undone.</p></li>
<li id="li-3492">
<p id="p-5081">Perform the next two steps in the Gaussian elimination algorithm to obtain <span class="process-math">\(A_3\text{.}\)</span>  Represent these steps using multiplication by matrices <span class="process-math">\(E_2\)</span> and <span class="process-math">\(E_3\)</span> so that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
E_3E_2E_1A = A_3\text{.}
\end{equation*}
</div>
</li>
<li id="li-3493"><p id="p-5082">Suppose we need to scale the second row by <span class="process-math">\(-2\text{.}\)</span> What is the <span class="process-math">\(3\times3\)</span> matrix that perfoms this row operation by left multiplication?</p></li>
<li id="li-3494"><p id="p-5083">Suppose that we need to interchange the first and second rows.  What is the <span class="process-math">\(3\times3\)</span> matrix that performs this row operation by left multiplication?</p></li>
</ol></article></section><section class="subsection" id="subsec-partial-pivot"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">5.1.1</span> <span class="title">Partial pivoting</span>
</h3>
<p id="p-5090">The first issue that we will address is the fact that computers do not perform arithemtic operations exactly.  For instance, if we ask Python to evaluate <code class="code-inline tex2jax_ignore">0.1 + 0.2</code>, it reports <code class="code-inline tex2jax_ignore">0.30000000000000004</code> though we know that the true value is 0.3.  There are a couple of reasons for this.</p>
<p id="p-5091">First, computers perform arithmetic using base 2 numbers, which means that numbers we enter in decimal form, such as <span class="process-math">\(0.1\text{,}\)</span> must be converted to base 2.  Even though 0.1 has a simple decimal form, its representation in base 2 is the repeating decimal</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
0.000110011001100110011001100110011001100110011\ldots\text{.}\text{,}
\end{equation*}
</div>
<p class="continuation">To accurately represent this number inside a computer would require infinitely many digits.  Since a computer can only hold a finite number of digits, we are necessarily using an approximation just by representing this number in a computer.</p>
<p id="p-5092">In addition, arithmetic operations, such as addition, are prone to error.  To keep things simple, suppose we have a computer that represents numbers using only three decimal digits.  For instance, the number 1.023 would be represented as <code class="code-inline tex2jax_ignore">1.02</code> while 0.023421 would be <code class="code-inline tex2jax_ignore">0.0234</code>.  If we add these numbers, we have 1.023 + 0.023421 = 1.046421;  the computer reports this sum as <code class="code-inline tex2jax_ignore"> 1.02 + 0.0234 = 1.04</code>, whose last digit is not correctly rounded.  Generally speaking, we will see this problem, which is called <em class="emphasis">round off error</em>, whenever we add numbers of signficantly different magnitudes. </p>
<p id="p-5093">Remember that Gaussian elimination, when applied to an <span class="process-math">\(n\times n\)</span> matrix, requires approximately <span class="process-math">\(\frac 23
n^3\)</span> operations.  If we have a <span class="process-math">\(1000\times1000\)</span> matrix, performing Gaussian elimination requires roughly a billion operations, and the errors introduced in each operation could accumulate.  How can we have confidence in the final result?  We can never completely avoid these errors, but we can take steps to mitigate them.  The next activity will introduce one such technique.</p>
<article class="activity project-like" id="activity-62"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">5.1.2</span><span class="period">.</span>
</h4>
<p id="p-5094">Suppose we have a hypothetical computer that represents numbers using only three decimal digits. We will consider the linear system</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\begin{alignedat}{3}
0.0001x \amp  {}+{}  \amp y \amp  {}={}  \amp 1 \\
x \amp  {}+{}  \amp y \amp  {}={}  \amp 2\text{.} \\
\end{alignedat}
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-3500">
<p id="p-5095">Show that this system has the unique solution</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\begin{aligned}
x \amp {}={} \frac{10000}{9999} = 1.00010001\ldots, \\
y \amp {}={} \frac{9998}{9999} =  0.99989998\ldots\text{.}
\end{aligned}
\end{equation*}
</div>
</li>
<li id="li-3501"><p id="p-5096">If we represent this solution inside our computer that only holds 3 decimal digits, what do we find for the solution? This is the best that we can hope to find using our computer.</p></li>
<li id="li-3502"><p id="p-5097">Let's imagine that we use our computer to find the solution using Gaussian elimination; that is, after every arithmetic operation, we keep only three decimal digits.  Our first step is to multiply the first equation by 10000 and subtract it from the second equation.  If we represent numbers using only three decimal digits, what does this give for the value of <span class="process-math">\(y\text{?}\)</span></p></li>
<li id="li-3503"><p id="p-5098">By substituting our value for <span class="process-math">\(y\)</span> into the first equation, what do we find for <span class="process-math">\(x\text{?}\)</span></p></li>
<li id="li-3504"><p id="p-5099">Compare the solution we find on our computer with the actual solution and assess the quality of the approximation.</p></li>
<li id="li-3505">
<p id="p-5100">Let's now modify the linear system by simplying interchanging the equations:</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\begin{alignedat}{3}
x \amp  {}+{}  \amp y \amp  {}={}  \amp 2 \\
0.0001x \amp  {}+{}  \amp y \amp  {}={}  \amp 1\text{.} \\
\end{alignedat}
\end{equation*}
</div>
<p class="continuation">Of course, this doesn't change the actual solution.  Let's imagine we use our computer to find the solution using Gaussian elimination.  Perform the first step where we multiply the first equation by 0.0001 and subtract from the second equation.  What does this give for <span class="process-math">\(y\)</span> if we represent numbers using only three decimal digits?</p>
</li>
<li id="li-3506"><p id="p-5101">Substitute the value you found for <span class="process-math">\(y\)</span> into the first equation and solve for <span class="process-math">\(x\text{.}\)</span>  Then compare the approximate solution found with our hypothetical computer to the exact solution.</p></li>
<li id="li-3507"><p id="p-5102">Which approach produces the most accurate approximation?</p></li>
</ol></article><p id="p-5112">This activity demonstrates how the practical aspects of computing differ from the theoretical.  We know that the order in which we write the equations has no effect on the solution space; row interchange is one of our three allowed row operations in the Gaussian elimination algorithm.  However, when we are only able to perform arithmetic operations approximately, applying row interchanges can dramatically improve the accuracy of our approximations.</p>
<p id="p-5113">If we could compute the solution exactly, we find</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
x = 1.00010001\ldots, \qquad
y =  0.99989998\ldots\text{.}
\end{equation*}
</div>
<p class="continuation">Since our hypothetical computer represents numbers using only three decimal digits, our computer finds</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
x \approx 1.00, \qquad y \approx 1.00.
\end{equation*}
</div>
<p class="continuation">This is the best we can hope to do with our computer since it is impossible to represent the solution exactly.</p>
<p id="p-5114">When the equations are written in their original order and we multiply the first equation by 10000 and subtract from the second, we find</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\begin{aligned}
(1-10000)y \amp {}={}2-10000 \\
-9999 y \amp {}={} -9998 \\
-10000y\amp {}\approx{} -10000 \\
y \amp {}\approx{} 1.00\text{.} \\
\end{aligned}
\end{equation*}
</div>
<p id="p-5115">In fact, we find the same value for <span class="process-math">\(y\)</span> when we interchange the equations.  Here we multiply the first equation by 0.0001 and subtract from the second equation.  We then find</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\begin{aligned}
(1-0.0001)y \amp {}={}2-0.0001 \\
-0.9999 y \amp {}={} -0.9998 \\
-y\amp {}\approx{} -1.00 \\
y \amp {}\approx{} 1.00\text{.} \\
\end{aligned}
\end{equation*}
</div>
<p id="p-5116">The difference occurs when we substitute <span class="process-math">\(y\approx 1\)</span> into the first equation.  When the equations are written in their original order, we have</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\begin{aligned}
0.0001x + 1.00 \amp {}\approx{} 1.00 \\
0.0001x \amp {}\approx{} 0.00 \\
x \amp {}\approx{} 0.00\text{.} \\
\end{aligned}
\end{equation*}
</div>
<p class="continuation">When the equations are written in their original order, we find the solution <span class="process-math">\(x\approx 0.00, y \approx 1.00\text{.}\)</span></p>
<p id="p-5117">When we write the equation in the opposite order, however, substituting <span class="process-math">\(y\approx 1\)</span> into the first equation gives</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\begin{aligned}
x + 1.00 \amp {}\approx{} 2.00 \\
x \amp {}\approx{} 1.00\text{.} \\
\end{aligned}
\end{equation*}
</div>
<p class="continuation">In this case, we find the approximate solution <span class="process-math">\(x\approx 1.00,
y\approx1.00\text{,}\)</span> which is the most accurate solution that our hypothetical computer can find.  Simply interchanging the order of the equation produces a much more accurate solution.</p>
<div class="sidebyside"><div class="sbsrow" style="margin-left:0%;margin-right:0%;">
<div class="sbspanel top" style="width:60%;"><p id="p-5118">We can understand why this works graphically.  Each equation represents a line in the plane, and the solution is the intersection point.  Notice that the slopes of these lines differ considerably.</p></div>
<div class="sbspanel top" style="width:40%;"><img src="external/images/partial-pivot-ex.svg" role="img" class="contained"></div>
</div></div>
<p id="p-5119">When the equations are written in their original order, we substitute <span class="process-math">\(y\approx1\)</span> into the equation <span class="process-math">\(0.00001x + y = 1\text{,}\)</span> which is a nearly horizontal line.  Along this line, a small change in <span class="process-math">\(y\)</span> leads to a large change in <span class="process-math">\(x\text{.}\)</span>  The slight difference in our approximation <span class="process-math">\(y\approx
1\)</span> from the exact value <span class="process-math">\(y=0.9998999\ldots\)</span> leads to a large difference in the approximation <span class="process-math">\(x\approx0\)</span> from the exact value <span class="process-math">\(x=1.00010001\ldots\text{.}\)</span></p>
<p id="p-5120">If we exchange the order in which the equations are written, we substitute our approximation <span class="process-math">\(y\approx 1\)</span> into the equation <span class="process-math">\(x+y=2\text{.}\)</span>  Notice that the slope of the associated line is <span class="process-math">\(-1\text{.}\)</span>  On this line, a small change in <span class="process-math">\(y\)</span> leads to a relatively small change in <span class="process-math">\(x\)</span> as well.  Therefore, the difference in our approximation <span class="process-math">\(y\approx1\)</span> from the exact value leads to only a small difference in the approximation <span class="process-math">\(x\approx1\)</span> from the exact value.</p>
<p id="p-5121">This example motivates the technique that computers usually use to perform Gaussian elimation.  We only need to perform a row interchange when a zero occurs in a pivot position, such as</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\left[\begin{array}{rrrr}
1 \amp -1 \amp 2 \amp 2 \\
0 \amp 0 \amp -3 \amp 1 \\
0 \amp 2 \amp 2 \amp -3 \\
\end{array}\right]\text{.}
\end{equation*}
</div>
<p class="continuation">However, we will perform a row interchange to put the entry having the largest possible absolute value into the pivot position. For instance, when performing Gaussian elimination on the following matrix, we begin by interchanging the first and third rows so that the upper left entry has the largest possible absolute value.</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\left[\begin{array}{rrrr}
2 \amp 1 \amp 2 \amp 3 \\
1 \amp -3 \amp -2 \amp 1 \\
-3 \amp 2 \amp 3 \amp -2 \\
\end{array}\right]
\sim
\left[\begin{array}{rrrr}
-3 \amp 2 \amp 3 \amp -2 \\
1 \amp -3 \amp -2 \amp 1 \\
2 \amp 1 \amp 2 \amp 3 \\
\end{array}\right]\text{.}
\end{equation*}
</div>
<p class="continuation"> This technique is called <em class="emphasis">partial pivoting</em>, and it means that, in practice, we will perform many more row interchange operations than we typically do when computing exactly by hand.</p></section><section class="subsection" id="subsection-77"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">5.1.2</span> <span class="title"><span class="process-math">\(LU\)</span> factorizations</span>
</h3>
<p id="p-5122">In <a href="sec-sage-introduction.html#subsec-compute-effort" class="internal" title="Subsection 1.3.3: Computational effort">Subsection 1.3.3</a>, we saw that the number of arithmetic operations needed to perform Gaussian elimination on an <span class="process-math">\(n\times n\)</span> matrix is about <span class="process-math">\(\frac23 n^3\text{.}\)</span>  This means that a <span class="process-math">\(1000\times1000\)</span> matrix, requires about two thirds of a billion operations.</p>
<p id="p-5123">Suppose that we have two equations, <span class="process-math">\(A\xvec = \bvec_1\)</span> and <span class="process-math">\(A\xvec = \bvec_2\text{,}\)</span> that we would like to solve.  Usually, we would form augmented matrices <span class="process-math">\(\left[\begin{array}{c|c} A \amp \bvec_1 \\ \end{array}\right]\)</span> and <span class="process-math">\(\left[\begin{array}{c|c} A \amp \bvec_2 \\ \end{array}\right]\)</span> and apply Gaussian elimination.  Of course, the steps we perform in these two computations are nearly identical.  Is there a way to store some of the computation we perform in reducing <span class="process-math">\(\left[\begin{array}{c|c} A \amp \bvec_1 \\ \end{array}\right]\)</span> and reuse it in solving subsequent equations?  The next activity will point us in the right direction.</p>
<article class="activity project-like" id="activity-63"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">5.1.3</span><span class="period">.</span>
</h4>
<p id="p-5124">We will consider the matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = \left[\begin{array}{rrr}
1 \amp 2 \amp 1 \\
-2 \amp -3 \amp -2 \\
3 \amp 7 \amp 4 \\
\end{array}\right]
\end{equation*}
</div>
<p class="continuation">and begin performing Gaussian elimination without using partial pivoting.</p>
<ol class="lower-alpha">
<li id="li-3516">
<p id="p-5125">Perform two row replacement operations to find the row equivalent matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A' = \left[\begin{array}{rrr}
1 \amp 2 \amp 1 \\
0 \amp 1 \amp 0 \\
0 \amp 1 \amp 1 \\
\end{array}\right]\text{.}
\end{equation*}
</div>
<p class="continuation">Find elementary matrices <span class="process-math">\(E_1\)</span> and <span class="process-math">\(E_2\)</span> that perform these two operations so that <span class="process-math">\(E_2E_1 A = A'\text{.}\)</span></p>
</li>
<li id="li-3517">
<p id="p-5126">Perform a third row replacement to find the upper triangular matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
U = \left[\begin{array}{rrr}
1 \amp 2 \amp 1 \\
0 \amp 1 \amp 0 \\
0 \amp 0 \amp 1 \\
\end{array}\right]\text{.}
\end{equation*}
</div>
<p class="continuation">Find the elementary matrix <span class="process-math">\(E_3\)</span> such that <span class="process-math">\(E_3E_2E_1A
= U\text{.}\)</span></p>
</li>
<li id="li-3518"><p id="p-5127">We can write <span class="process-math">\(A=E_1^{-1}E_2^{-1}E_3^{-1} U\text{.}\)</span> Find the inverse matrices <span class="process-math">\(E_1^{-1}\text{,}\)</span> <span class="process-math">\(E_2^{-1}\text{,}\)</span> and <span class="process-math">\(E_3^{-1}\)</span> and the product <span class="process-math">\(L = E_1^{-1}E_2^{-1}E_3^{-1}\text{.}\)</span>  Then verify that <span class="process-math">\(A=LU\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-135"><script type="text/x-sage">
</script></pre></p></li>
<li id="li-3519">
<p id="p-5128">Suppose that we want to solve the equation <span class="process-math">\(A\xvec = \bvec = \threevec4{-7}{12}\text{.}\)</span>  We will write</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A\xvec = LU\xvec = L(U\xvec) = \bvec
\end{equation*}
</div>
<p class="continuation">and introduce an unknown vector <span class="process-math">\(\cvec\)</span> such that <span class="process-math">\(U\xvec = \cvec\text{.}\)</span>  Find <span class="process-math">\(\cvec\)</span> by noting that <span class="process-math">\(L\cvec = \bvec\)</span> and solving this equation.</p>
</li>
<li id="li-3520"><p id="p-5129">Now that we have found <span class="process-math">\(\cvec\text{,}\)</span> find <span class="process-math">\(\xvec\)</span> by solving <span class="process-math">\(U\xvec = \cvec\text{.}\)</span></p></li>
<li id="li-3521"><p id="p-5130">Using the factorization <span class="process-math">\(A=LU\)</span> and this two-step process, solve the equation <span class="process-math">\(A\xvec = \threevec{2}{-2}{7}\text{.}\)</span></p></li>
</ol></article><p id="p-5138">This activity introduces a method for factoring a matrix <span class="process-math">\(A\)</span> as a product of two triangular matrices, <span class="process-math">\(A=LU\text{,}\)</span> where <span class="process-math">\(L\)</span> is lower triangular and <span class="process-math">\(U\)</span> is upper triangular.  The key to finding this factorization is to represent the row operations that we apply in the Gaussian elimination algorithm through multiplication by elementary matrices.</p>
<article class="example example-like" id="example-LU"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">5.1.1</span><span class="period">.</span>
</h4>
<p id="p-5139">Suppose we have the equation</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\begin{bmatrix}
2 \amp -3 \amp 1 \\
-4 \amp 5 \amp 0 \\
2 \amp -2 \amp 2
\end{bmatrix}
\xvec = \cthreevec8{-13}8,
\end{equation*}
</div>
<p class="continuation">which we write in the form <span class="process-math">\(A\xvec = \bvec\text{.}\)</span>  We begin by applying the Gaussian elimination algorithm to find an <span class="process-math">\(LU\)</span> factorization of <span class="process-math">\(A\text{.}\)</span></p>
<p id="p-5140">The first step is to multiply the first row of <span class="process-math">\(A\)</span> by <span class="process-math">\(2\)</span> and add it to the second row.  The elementary matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
E_1 = \begin{bmatrix}
1 \amp 0 \amp 0 \\
2 \amp 1 \amp 0 \\
0 \amp 0 \amp 0 \\
\end{bmatrix}
\end{equation*}
</div>
<p class="continuation">performs this operation so that <span class="process-math">\(E_1A = \begin{bmatrix}
2 \amp -3 \amp 1 \\
0 \amp -1 \amp 2 \\
2 \amp -2 \amp 2
\end{bmatrix}
\text{.}\)</span></p>
<p id="p-5141">We next apply matrices</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
E_2 = \begin{bmatrix}
1 \amp 0 \amp 0 \\
0 \amp 1 \amp 0 \\
-1 \amp 0 \amp 1
\end{bmatrix},~~~
E_3 = \begin{bmatrix}
1 \amp 0 \amp 0 \\
0 \amp 1 \amp 0 \\
0 \amp 1 \amp 1
\end{bmatrix}
\end{equation*}
</div>
<p class="continuation">to obtain the upper triangular matrix <span class="process-math">\(U = E_3E_2E_1 A =
\begin{bmatrix} 
2 \amp -3 \amp 1 \\
0 \amp -1 \amp 2 \\
0 \amp 0 \amp 3
\end{bmatrix}
\text{.}\)</span></p>
<p id="p-5142">We can write <span class="process-math">\(U = (E_3E_2E_1)A\text{,}\)</span> which tells us that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = (E_3E_2E_1)^{-1}U  = \begin{bmatrix}
1 \amp 0 \amp 0 \\
-2 \amp 1 \amp 0 \\
1 \amp -1 \amp 1
\end{bmatrix} U = LU.
\end{equation*}
</div>
<p class="continuation">That is, we have</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = LU =
\begin{bmatrix}
1 \amp 0 \amp 0 \\
-2 \amp 1 \amp 0 \\
1 \amp -1 \amp 1
\end{bmatrix}
\begin{bmatrix}
2 \amp -3 \amp 1 \\
0 \amp -1 \amp 2 \\
0 \amp 0 \amp 3
\end{bmatrix}.
\end{equation*}
</div>
<p class="continuation">Notice that the matrix <span class="process-math">\(L\)</span> is lower triangular, a result of the fact that the elementary matrices <span class="process-math">\(E_1\text{,}\)</span> <span class="process-math">\(E_2\text{,}\)</span> and <span class="process-math">\(E_3\)</span> are lower triangular.</p>
<p id="p-5143">Now that we have factored <span class="process-math">\(A=LU\)</span> into two triangular matrices, we can solve the equation <span class="process-math">\(A\xvec = \bvec\)</span> by solving two triangular systems.  We write</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A\xvec = L(U\xvec) = \bvec
\end{equation*}
</div>
<p class="continuation">and define the unknown vector <span class="process-math">\(\cvec = U\xvec\text{,}\)</span> which is determined by the equation <span class="process-math">\(L\cvec = \bvec\text{.}\)</span> Because <span class="process-math">\(L\)</span> is lower triangular, we find the solution using forward substitution, <span class="process-math">\(\cvec = \threevec833\text{.}\)</span>  Finally, we find <span class="process-math">\(\xvec\text{,}\)</span> the solution to our original system <span class="process-math">\(A\xvec = \bvec\text{,}\)</span> by applying back substitution to solve <span class="process-math">\(U\xvec = \cvec\text{.}\)</span>  This gives <span class="process-math">\(\xvec =
\threevec2{-1}1\text{.}\)</span></p>
<p id="p-5144">If we want to solve <span class="process-math">\(A\xvec = \bvec\)</span> for a different right-hand side <span class="process-math">\(\bvec\text{,}\)</span> we can simply repeat this two-step process.</p></article><p id="p-5145">An <span class="process-math">\(LU\)</span> factorization allow us to trade in one equation <span class="process-math">\(A\xvec = \bvec\)</span> for two simpler equations</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\begin{aligned}
L\cvec \amp = \bvec \\
U\xvec \amp = \cvec. \\
\end{aligned}
\end{equation*}
</div>
<p class="continuation">For instance, the equation <span class="process-math">\(L\cvec = \bvec\)</span> in our example has the form</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\left[\begin{array}{rrr}
1 \amp 0 \amp 0 \\
-2 \amp 1 \amp 0 \\
1 \amp -1 \amp 1
\end{array}\right]\cvec = \threevec8{-13}8\text{.}
\end{equation*}
</div>
<p class="continuation">Because <span class="process-math">\(L\)</span> is a lower-triangular matrix, we can read off the first component of <span class="process-math">\(\cvec\)</span> directly from the equations:  <span class="process-math">\(c_1 = 8\text{.}\)</span>  We then have <span class="process-math">\(-2c_1+c_2 = -13\text{,}\)</span> which gives <span class="process-math">\(c_2 = 3\text{,}\)</span> and <span class="process-math">\(c_1 - c_2 + c_3 = 8\text{,}\)</span> which gives <span class="process-math">\(c_3=3\text{.}\)</span> Solving a triangular system is simplified because we only need to perform a sequence of substitutions.</p>
<p id="p-5146">In fact, solving an equation with an <span class="process-math">\(n\times n\)</span> triangular matrix requires approximately <span class="process-math">\(\frac 12 n^2\)</span> operations. Once we have the factorization <span class="process-math">\(A=LU\text{,}\)</span> we solve the equation <span class="process-math">\(A\xvec=\bvec\)</span> by solving two equations involving triangular matrices, which requires about <span class="process-math">\(n^2\)</span> operations. For example, if <span class="process-math">\(A\)</span> is a <span class="process-math">\(1000\times1000\)</span> matrix, we solve the equation <span class="process-math">\(A\xvec = \bvec\)</span> using about one million steps.  The compares with roughly a billion operations needed to perform Gaussian elimination, which represents a significant savings.  Of course, we have to first find the <span class="process-math">\(LU\)</span> factorization of <span class="process-math">\(A\)</span> and this requires roughly the same amount of work as performing Gaussian elimination. However, once we have the <span class="process-math">\(LU\)</span> factorization, we can use it to solve <span class="process-math">\(A\xvec=\bvec\)</span> for different right hand sides <span class="process-math">\(\bvec\text{.}\)</span></p>
<p id="p-5147">Our discussion so far has ignored one issue, however. Remember that we sometimes have to perform row interchange operations in addition to row replacement.  A typical row interchange is represented by multiplication by a matrix such as</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
P = \left[\begin{array}{rrr}
0 \amp 0 \amp 1 \\
0 \amp 1 \amp 0 \\
1 \amp 0 \amp 0 \\
\end{array}\right]\text{,}
\end{equation*}
</div>
<p class="continuation">which has the effect of interchanging the first and third rows. Notice that this matrix is not triangular so performing a row interchange will disrupt the structure of the <span class="process-math">\(LU\)</span> factorization we seek.  Without giving the details, we simply note that linear algebra software packages provide a matrix <span class="process-math">\(P\)</span> that describes how the rows are permuted in the Gaussian elimination process.  In particular, we will write <span class="process-math">\(PA = LU\text{,}\)</span> where <span class="process-math">\(P\)</span> is a permutation matrix, <span class="process-math">\(L\)</span> is lower triangular, and <span class="process-math">\(U\)</span> is upper triangular.</p>
<p id="p-5148">Therefore, to solve the equation <span class="process-math">\(A\xvec = \bvec\text{,}\)</span> we first multiply both sides by <span class="process-math">\(P\)</span> to obtain</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
PA\xvec = LU\xvec = P\bvec\text{.}
\end{equation*}
</div>
<p class="continuation">That is, we multiply <span class="process-math">\(\bvec\)</span> by <span class="process-math">\(P\)</span> and then find <span class="process-math">\(\xvec\)</span> using the factorization:  <span class="process-math">\(L\cvec = P\bvec\)</span> and <span class="process-math">\(U\xvec = \cvec\text{.}\)</span></p>
<article class="activity project-like" id="activity-64"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">5.1.4</span><span class="period">.</span>
</h4>
<p id="p-5149">Sage will create <span class="process-math">\(LU\)</span> factorizations;  once we have a matrix <code class="code-inline tex2jax_ignore">A</code>, we write <code class="code-inline tex2jax_ignore">P, L, U = A.LU()</code> to obtain the matrices <span class="process-math">\(P\text{,}\)</span> <span class="process-math">\(L\text{,}\)</span> and <span class="process-math">\(U\)</span> such that <span class="process-math">\(PA =
LU\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-136"><script type="text/x-sage">
</script></pre></p>
<ol class="lower-alpha">
<li id="li-3528">
<p id="p-5150">In <a href="" class="xref" data-knowl="./knowl/example-LU.html" title="Example 5.1.1">Example 5.1.1</a>, we found the <span class="process-math">\(LU\)</span> factorization</p>
<div class="displaymath process-math" data-contains-math-knowls="./knowl/example-LU.html">
\begin{equation*}
A = 
\begin{bmatrix}
2 \amp -3 \amp 1 \\
-4 \amp 5 \amp 0 \\
2 \amp -2 \amp 2
\end{bmatrix}
= 
\begin{bmatrix}
1 \amp 0 \amp 0 \\
-2 \amp 1 \amp 0 \\
1 \amp -1 \amp 1
\end{bmatrix}
\begin{bmatrix}
2 \amp -3 \amp 1 \\
0 \amp -1 \amp 2 \\
0 \amp 0 \amp 3
\end{bmatrix}=LU.
\end{equation*}
</div>
<p class="continuation">Using Sage, define the matrix <span class="process-math">\(A\)</span> and then ask Sage for the <span class="process-math">\(LU\)</span> factorization.  What are the matrices <span class="process-math">\(P\text{,}\)</span> <span class="process-math">\(L\text{,}\)</span> and <span class="process-math">\(U\text{?}\)</span></p>
<p id="p-5151">Notice that Sage finds a different <span class="process-math">\(LU\)</span> factorization than we found in the previous activity because Sage uses partial pivoting, as described in the previous section, when it performs Gaussian elimination.</p>
</li>
<li id="li-3529"><p id="p-5152">Define the vector <span class="process-math">\(\bvec = \threevec8{-13}{8}\)</span> in Sage and compute <span class="process-math">\(P\bvec\text{.}\)</span></p></li>
<li id="li-3530"><p id="p-5153">Use the matrices <code class="code-inline tex2jax_ignore">L</code> and <code class="code-inline tex2jax_ignore">U</code> to solve <span class="process-math">\(L\cvec = P\bvec\)</span> and <span class="process-math">\(U\xvec = \cvec\text{.}\)</span>  You should find the same solution <span class="process-math">\(\xvec\)</span> that you found in the previous activity.</p></li>
<li id="li-3531"><p id="p-5154">Use the factorization to solve the equation <span class="process-math">\(A\xvec
= \threevec9{-16}{10}\text{.}\)</span></p></li>
<li id="li-3532"><p id="p-5155">How does the factorization show us that <span class="process-math">\(A\)</span> is invertible and that, therefore, every equation <span class="process-math">\(A\xvec=\bvec\)</span> has a unique solution?</p></li>
<li id="li-3533">
<p id="p-5156">Suppose that we have the matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
B = \left[\begin{array}{rrr}
3 \amp -1 \amp 2 \\
2 \amp -1 \amp 1 \\
2 \amp 1 \amp 3 \\
\end{array}\right]\text{.}
\end{equation*}
</div>
<p class="continuation">Use Sage to find the <span class="process-math">\(LU\)</span> factorization.  Explain how the factorization shows that <span class="process-math">\(B\)</span> is not invertible.</p>
</li>
<li id="li-3534">
<p id="p-5157">Consider the matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
C = \left[\begin{array}{rrrr}
-2 \amp 1 \amp 2 \amp -1 \\
1 \amp -1 \amp 0 \amp 2 \\
3 \amp 2 \amp -1 \amp 0 \\
\end{array}\right]
\end{equation*}
</div>
<p class="continuation">and find its <span class="process-math">\(LU\)</span> factorization.  Explain why <span class="process-math">\(C\)</span> and <span class="process-math">\(U\)</span> have the same null space and use this observation to find a basis for <span class="process-math">\(\nul(A)\text{.}\)</span></p>
</li>
</ol></article></section><section class="subsection" id="subsection-78"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">5.1.3</span> <span class="title">Summary</span>
</h3>
<p id="p-5167">We returned to Gaussian elimination, which we have used as a primary tool for finding solutions to linear systems, and explored its practicality, both in terms of numerical accuracy and computational effort.</p>
<ul class="disc">
<li id="li-3542"><p id="p-5168">We saw that the accuracy of computations implemented on a computer could be improved using <em class="emphasis">partial pivoting</em>, a technique that performs row interchanges so that the entry in a pivot position has the largest possible magnitude.</p></li>
<li id="li-3543"><p id="p-5169">Beginning with a matrix <span class="process-math">\(A\text{,}\)</span> we used the Gaussian elimination algorithm to write <span class="process-math">\(PA = LU\text{,}\)</span> where <span class="process-math">\(P\)</span> is a permutation matrix, <span class="process-math">\(L\)</span> is lower triangular, and <span class="process-math">\(U\)</span> is upper triangular.</p></li>
<li id="li-3544"><p id="p-5170">Finding this factorization involves roughly as much work as performing Gaussian elimination.  However, once we have the factorization, we are able to quickly solve equations of the form <span class="process-math">\(A\xvec = \bvec\)</span> by first solving <span class="process-math">\(L\cvec =
P\bvec\)</span> and then <span class="process-math">\(U\xvec = \cvec\text{.}\)</span></p></li>
</ul></section><section class="exercises" id="exercises-20"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">5.1.4</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-179"><h4 class="heading"><span class="codenumber">1<span class="period">.</span></span></h4>
<p id="p-5171">In this section, we saw that errors made in computer arithmetic can produce approximate solutions that are far from the exact solutions.  Here is another example in which this can happen.  Consider the matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = \left[\begin{array}{cc}
1 \amp 1 \\
1 \amp 1.0001 \\
\end{array}\right]\text{.}
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-3545"><p id="p-5172">Find the exact solution to the equation <span class="process-math">\(A\xvec = \twovec{2}{2}\text{.}\)</span></p></li>
<li id="li-3546"><p id="p-5173">Suppose that this linear system arises in the midst of a larger computation except that, due to some error in the computation of the right hand side of the equation, our computer thinks we want to solve <span class="process-math">\(A\xvec = \ctwovec{2}{2.0001}\text{.}\)</span>  Find the solution to this equation and compare it to the solution of the equation in the previous part of this exericse.</p></li>
</ol>
<p id="p-5174">Notice how a small change in the right hand side of the equation leads to a large change in the solution.  In this case, we say that the matrix <span class="process-math">\(A\)</span> is <em class="emphasis">ill-conditioned</em> because the solutions are extremely sensitive to small changes in the right hand side of the equation.  Though we will not do so here, it is possible to create a measure of the matrix that tells us when a matrix is ill-conditioned.  Regrettably, there is not much we can do to remedy this problem.</p></article><article class="exercise exercise-like" id="exercise-180"><h4 class="heading"><span class="codenumber">2<span class="period">.</span></span></h4>
<p id="p-5181">In this section, we found the <span class="process-math">\(LU\)</span> factorization of the matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = \left[\begin{array}{rrr}
1 \amp 2 \amp 1 \\
-2 \amp -3 \amp -2 \\
3 \amp 7 \amp 4 \\
\end{array}\right]
\end{equation*}
</div>
<p class="continuation">in one of the activities, without using partial pivoting.  Apply a sequence of row operations, now using partial pivoting, to find an upper triangular matrix <span class="process-math">\(U\)</span> that is row equivalent to <span class="process-math">\(A\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-137"><script type="text/x-sage">
</script></pre></p></article><article class="exercise exercise-like" id="exercise-181"><h4 class="heading"><span class="codenumber">3<span class="period">.</span></span></h4>
<p id="p-5184">In the following exercises, use the given <span class="process-math">\(LU\)</span> factorizations to solve the equations <span class="process-math">\(A\xvec = \bvec\text{.}\)</span></p>
<ol class="lower-alpha">
<li id="li-3551">
<p id="p-5185">Solve the equation</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A\xvec =
\left[\begin{array}{rr}
1 \amp 0 \\
-2 \amp 1 \\
\end{array}\right]
\left[\begin{array}{rr}
3 \amp 1 \\
0 \amp -2 \\
\end{array}\right]\xvec
=
\twovec{-3}{0}\text{.}
\end{equation*}
</div>
</li>
<li id="li-3552">
<p id="p-5186">Solve the equation</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A\xvec =
\left[\begin{array}{rrr}
1 \amp 0 \amp 0 \\
-2 \amp 1 \amp 0 \\
-1 \amp 2 \amp 1 \\
\end{array}\right]
\left[\begin{array}{rrr}
2 \amp 1 \amp 0 \\
0 \amp -1 \amp 3 \\
0 \amp 0 \amp 1 \\
\end{array}\right]\xvec
=
\threevec{5}{-5}{7}\text{.}
\end{equation*}
</div>
</li>
</ol></article><article class="exercise exercise-like" id="exercise-182"><h4 class="heading"><span class="codenumber">4<span class="period">.</span></span></h4>
<p id="p-5193">Use Sage to solve the following equation by finding an <span class="process-math">\(LU\)</span> factorization:</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\left[\begin{array}{rrr}
3 \amp 4 \amp -1 \\
2 \amp 4 \amp 1 \\
-3 \amp 1 \amp 4 \\
\end{array}\right]
\xvec = \threevec{-3}{-3}{-4}\text{.}
\end{equation*}
</div>
<p class="continuation"><pre class="ptx-sagecell sagecell-sage" id="sage-138"><script type="text/x-sage">
</script></pre></p></article><article class="exercise exercise-like" id="exercise-eigenvector-approx"><h4 class="heading"><span class="codenumber">5<span class="period">.</span></span></h4>
<p id="p-5196">Here is another problem with approximate computer arithmetic that we will encounter in the next section.  Consider the matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = \left[\begin{array}{rrr}
0.2 \amp 0.2 \amp 0.4 \\
0.2 \amp 0.3 \amp 0.1 \\
0.6 \amp 0.5 \amp 0.5 \\
\end{array}\right]\text{.}
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-3557"><p id="p-5197">Notice that this is a positive stochastic matrix. What do we know about the eigenvalues of this matrix?</p></li>
<li id="li-3558"><p id="p-5198">Use Sage to define the matrix <span class="process-math">\(A\)</span> using decimals such as <code class="code-inline tex2jax_ignore">0.2</code> and the <span class="process-math">\(3\times3\)</span> identity matrix <span class="process-math">\(I\text{.}\)</span>  Ask Sage to compute <span class="process-math">\(B = A-I\)</span> and find the reduced row echelon form of <span class="process-math">\(B\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-139"><script type="text/x-sage">
</script></pre></p></li>
<li id="li-3559"><p id="p-5199">Why is the computation that Sage performed incorrect?</p></li>
<li id="li-3560"><p id="p-5200">Explain why using a computer to find the eigenvectors of a matrix <span class="process-math">\(A\)</span> by finding a basis for <span class="process-math">\(\nul(A-\lambda
I)\)</span> is problematic.</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-184"><h4 class="heading"><span class="codenumber">6<span class="period">.</span></span></h4>
<p id="p-5211">In practice, one rarely finds the inverse of a matrix <span class="process-math">\(A\text{.}\)</span>  It requires considerable effort to compute, and we can solve any equation of the form <span class="process-math">\(A\xvec = \bvec\)</span> using an <span class="process-math">\(LU\)</span> factorization, which means that the inverse isn't necessary.  In any case, the best way to compute an inverse is using an <span class="process-math">\(LU\)</span> factorization, as this exericse demonstrates.</p>
<ol class="lower-alpha">
<li id="li-3569">
<p id="p-5212">Suppose that <span class="process-math">\(PA = LU\text{.}\)</span>  Explain why <span class="process-math">\(A^{-1} =
U^{-1}L^{-1}P\text{.}\)</span></p>
<p id="p-5213">Since <span class="process-math">\(L\)</span> and <span class="process-math">\(U\)</span> are triangular, finding their inverses is relatively efficient.  That makes this an effective means of finding <span class="process-math">\(A^{-1}\text{.}\)</span></p>
</li>
<li id="li-3570">
<p id="p-5214">Consider the matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = \left[\begin{array}{rrr}
3 \amp 4 \amp -1 \\
2 \amp 4 \amp 1 \\
-3 \amp 1 \amp 4 \\
\end{array}\right]\text{.}
\end{equation*}
</div>
<p class="continuation">Find the <span class="process-math">\(LU\)</span> factorization of <span class="process-math">\(A\)</span> and use it to find <span class="process-math">\(A^{-1}\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-140"><script type="text/x-sage">
</script></pre></p>
</li>
</ol></article><article class="exercise exercise-like" id="exercise-185"><h4 class="heading"><span class="codenumber">7<span class="period">.</span></span></h4>
<p id="p-5219">Consider the matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = \left[\begin{array}{rrrr}
a \amp a \amp a \amp a \\
a \amp b \amp b \amp b \\
a \amp b \amp c \amp c \\
a \amp b \amp c \amp d \\
\end{array}\right]\text{.}
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-3573"><p id="p-5220">Find the <span class="process-math">\(LU\)</span> factorization of <span class="process-math">\(A\text{.}\)</span></p></li>
<li id="li-3574"><p id="p-5221">What conditions on <span class="process-math">\(a\text{,}\)</span> <span class="process-math">\(b\text{,}\)</span> <span class="process-math">\(c\text{,}\)</span> and <span class="process-math">\(d\)</span> guarantee that <span class="process-math">\(A\)</span> is invertible?</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-186"><h4 class="heading"><span class="codenumber">8<span class="period">.</span></span></h4>
<p id="p-5228">In the <span class="process-math">\(LU\)</span> factorization of a matrix, the diagonal entries of <span class="process-math">\(L\)</span> are all <span class="process-math">\(1\)</span> while the diagonal entries of <span class="process-math">\(U\)</span> are not necessarily <span class="process-math">\(1\text{.}\)</span>  This exercise will explore that observation by considering the matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = \left[\begin{array}{rrr}
3 \amp 1 \amp 1 \\
-6 \amp -4 \amp -1 \\
0 \amp -4 \amp 1 \\
\end{array}\right]\text{.}
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-3579"><p id="p-5229">Perform Gaussian elimination without partial pivoting to find <span class="process-math">\(U\text{,}\)</span> an upper triangular matrix that is row equivalent to <span class="process-math">\(A\text{.}\)</span></p></li>
<li id="li-3580"><p id="p-5230">The diagonal entries of <span class="process-math">\(U\)</span> are called <em class="emphasis">pivots</em>.  Explain why <span class="process-math">\(\det A\)</span> equals the product of the pivots.</p></li>
<li id="li-3581"><p id="p-5231">What is <span class="process-math">\(\det A\)</span> for our matrix <span class="process-math">\(A\text{?}\)</span></p></li>
<li id="li-3582"><p id="p-5232">More generally, if we have <span class="process-math">\(PA=LU\text{,}\)</span> explain why <span class="process-math">\(\det A\)</span> equals plus or minus the product of the pivots.</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-187"><h4 class="heading"><span class="codenumber">9<span class="period">.</span></span></h4>
<p id="p-5243">Please provide a justification to your responses to these questions.</p>
<ol class="lower-alpha">
<li id="li-3591"><p id="p-5244">In this section, our hypothetical computer could only store numbers using 3 decimal places.  Most computers can store numbers using 15 or more decimal places.  Why do we still need to be concerned about the accuracy of our computations when solving systems of linear equations?</p></li>
<li id="li-3592"><p id="p-5245">Finding the <span class="process-math">\(LU\)</span> factorization of a matrix <span class="process-math">\(A\)</span> is roughly the same amount of work as finding its reduced row echelon form.  Why is the <span class="process-math">\(LU\)</span> factorization useful then?</p></li>
<li id="li-3593"><p id="p-5246">How can we detect whether a matrix is invertible from its <span class="process-math">\(LU\)</span> factorization?</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-188"><h4 class="heading"><span class="codenumber">10<span class="period">.</span></span></h4>
<p id="p-5255">Consider the matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = \left[\begin{array}{rrrr}
-1 \amp 1 \amp 0 \amp 0 \\
1 \amp -2 \amp 1 \amp 0 \\
0 \amp 1 \amp -2 \amp 1 \\
0 \amp 0 \amp -1 \amp 1 \\
\end{array}\right]\text{.}
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-3600"><p id="p-5256">Find the <span class="process-math">\(LU\)</span> factorization of <span class="process-math">\(A\text{.}\)</span></p></li>
<li id="li-3601"><p id="p-5257">Use the factorization to find a basis for <span class="process-math">\(\nul(A)\text{.}\)</span></p></li>
<li id="li-3602"><p id="p-5258">We have seen that <span class="process-math">\(\nul(A) = \nul(U)\text{.}\)</span>  Is it true that <span class="process-math">\(\col(A) = \col(L)\text{?}\)</span></p></li>
</ol></article></section></section></div></main>
</div>
</body>
</html>
