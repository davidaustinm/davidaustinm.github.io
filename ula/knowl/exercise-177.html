<!DOCTYPE html>
<html lang="en-US">
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2022-08-08T13:57:16-04:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
<script>// Make *any* pre with class 'sagecell-sage' an executable Sage cell
// Their results will be linked, only within language type
sagecell.makeSagecell({inputLocation: 'pre.sagecell-sage',
                       linked: true,
                       languages: ['sage'],
                       evalButtonText: 'Evaluate (Sage)'});
</script>
</head>
<body class="ignore-math">
<article class="exercise exercise-like"><h4 class="heading">
<span class="type">Exercise</span><span class="space"> </span><span class="codenumber">9<span class="period">.</span></span>
</h4>
<p>We saw a couple of model Internets in which a Markov chain defined by the Google matrix <span class="process-math">\(G\)</span> did not converge to an appropriate PageRank vector.  For this reason, Google defines the matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
H_n = \left[\begin{array}{rrrr}
\frac1n \amp \frac1n \amp \ldots \amp \frac1n \\
\frac1n \amp \frac1n \amp \ldots \amp \frac1n \\
\vdots \amp \vdots \amp \ddots \amp \vdots \\
\frac1n \amp \frac1n \amp \ldots \amp \frac1n \\
\end{array}\right]\text{,}
\end{equation*}
</div>
<p class="continuation">where <span class="process-math">\(n\)</span> is the number of web pages, and constructs a Markov chain from the modified Google matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
G' = \alpha G + (1-\alpha)H_n\text{.}
\end{equation*}
</div>
<p class="continuation">Since <span class="process-math">\(G'\)</span> is positive, the Markov chain is guaranteed to converge to a unique steady-state vector.</p>
<p>We said that Google chooses <span class="process-math">\(\alpha = 0.85\)</span> so we might wonder why this is a good choice.  We will explore the role of <span class="process-math">\(\alpha\)</span> in this exercise.  Let's consider the model Internet described in <a href="" class="xref" data-knowl="./knowl/fig-google-cyclic.html" title="Figure 4.5.9">FigureÂ 4.5.9</a> and construct the Google matrix <span class="process-math">\(G\text{.}\)</span>  In the Sage cell below, you can enter the matrix <span class="process-math">\(G\)</span> and choose a value for <span class="process-math">\(\alpha\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-132"><script type="text/x-sage">def modified_markov_chain(A, x0, N):
    r = A.nrows()
    A = alpha*A + (1-alpha)*matrix(r,r,[1.0/r]*(r*r))	      
    for i in range(N):
        x0 = A*x0
        print (x0.numerical_approx(digits=3))
## Define the matrix original Google matrix G and choose alpha.
## The function above finds the modified Google matrix
## and resulting Markov chain
alpha = 0
G =
x0 = vector([1,0,0,0,0])
modified_markov_chain(G, x0, 20)
</script></pre></p>
<ol class="lower-alpha">
<li><p>Let's begin with <span class="process-math">\(\alpha=0\text{.}\)</span>  With this choice, what is the matrix <span class="process-math">\(G'=\alpha G + (1-\alpha)H_n\text{?}\)</span> Construct a Markov chain using the Sage cell above.  How many steps are required for the Markov chain to converge to the accuracy with which the vectors <span class="process-math">\(\xvec_k\)</span> are displayed?</p></li>
<li><p>Now choose <span class="process-math">\(\alpha=0.25\text{.}\)</span>  How many steps are required for the Markov chain to converge to the accuracy at which the vectors <span class="process-math">\(\xvec_k\)</span> are displayed?</p></li>
<li><p>Repeat this experiment with <span class="process-math">\(\alpha = 0.5\)</span> and <span class="process-math">\(\alpha=0.75\text{.}\)</span></p></li>
<li><p>What happens if <span class="process-math">\(\alpha = 1\text{?}\)</span></p></li>
</ol>
<p>This experiment gives some insight into the choice of <span class="process-math">\(\alpha\text{.}\)</span>  The smaller <span class="process-math">\(\alpha\)</span> is, the faster the Markov chain converges.  This is important; since the matrix <span class="process-math">\(G'\)</span> that Google works with is so large, we would like to minimize the number of terms in the Markov chain that we need to compute.  On the other hand, as we lower <span class="process-math">\(\alpha\text{,}\)</span> the matrix <span class="process-math">\(G' = \alpha G + (1-\alpha)H_n\)</span> begins to resemble <span class="process-math">\(H_n\)</span> more and <span class="process-math">\(G\)</span> less.  The value <span class="process-math">\(\alpha=0.85\)</span> is chosen so that the matrix <span class="process-math">\(G'\)</span> sufficiently resembles <span class="process-math">\(G\)</span> while having the Markov chain converge in a reasonable amount of steps.</p></article><span class="incontext"><a href="sec-stochastic.html#exercise-177" class="internal">in-context</a></span>
</body>
</html>
