<!DOCTYPE html>
<html lang="en-US">
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2022-08-08T13:57:16-04:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body class="ignore-math">
<h5 class="heading"><span class="type">Paragraph</span></h5>
<p>This experiment gives some insight into the choice of <span class="process-math">\(\alpha\text{.}\)</span>  The smaller <span class="process-math">\(\alpha\)</span> is, the faster the Markov chain converges.  This is important; since the matrix <span class="process-math">\(G'\)</span> that Google works with is so large, we would like to minimize the number of terms in the Markov chain that we need to compute.  On the other hand, as we lower <span class="process-math">\(\alpha\text{,}\)</span> the matrix <span class="process-math">\(G' = \alpha G + (1-\alpha)H_n\)</span> begins to resemble <span class="process-math">\(H_n\)</span> more and <span class="process-math">\(G\)</span> less.  The value <span class="process-math">\(\alpha=0.85\)</span> is chosen so that the matrix <span class="process-math">\(G'\)</span> sufficiently resembles <span class="process-math">\(G\)</span> while having the Markov chain converge in a reasonable amount of steps.</p>
<span class="incontext"><a href="sec-stochastic.html#p-5031" class="internal">in-context</a></span>
</body>
</html>
