<!DOCTYPE html>
<html lang="en-US">
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2022-08-08T13:58:42-04:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body class="ignore-math">
<h4 class="heading"><span class="type">Paragraph</span></h4>
<p>In our first encounter with principal component analysis, we began with a demeaned data matrix <span class="process-math">\(A\text{,}\)</span> formed the covariance matrix <span class="process-math">\(C\text{,}\)</span> and used the eigenvalues and eigenvectors of <span class="process-math">\(C\)</span> to project the demeaned data onto a smaller dimensional subspace.  In this section, we have seen that a singular value decomposition of <span class="process-math">\(A\)</span> provides a more direct route: the left singular vectors of <span class="process-math">\(A\)</span> form the principal components and the approximating matrix <span class="process-math">\(A_k\)</span> represents the data points projected onto the subspace spanned by the first <span class="process-math">\(k\)</span> principal components.  The coordinates of a projected demeaned data point are given by the columns of <span class="process-math">\(\Gamma_k =
\Sigma_kV_k^T\text{.}\)</span></p>
<span class="incontext"><a href="sec-svd-uses.html#p-8254" class="internal">in-context</a></span>
</body>
</html>
