<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2022-08-08T13:56:32-04:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US">
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Using Singular Value Decompositions</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="Understanding Linear Algebra">
<meta property="book:author" content=" David Austin ">
<script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script><script>window.MathJax = {
  tex: {
    inlineMath: [['\\(','\\)']],
    tags: "none",
    tagSide: "right",
    tagIndent: ".8em",
    packages: {'[+]': ['base', 'extpfeil', 'ams', 'amscd', 'newcommand', 'knowl']}
  },
  options: {
    ignoreHtmlClass: "tex2jax_ignore|ignore-math",
    processHtmlClass: "process-math",
    renderActions: {
        findScript: [10, function (doc) {
            document.querySelectorAll('script[type^="math/tex"]').forEach(function(node) {
                var display = !!node.type.match(/; *mode=display/);
                var math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                var text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = {node: text, delim: '', n: 0};
                math.end = {node: text, delim: '', n: 0};
                doc.math.push(math);
            });
        }, '']
    },
  },
  chtml: {
    scale: 0.88,
    mtextInheritFont: true
  },
  loader: {
    load: ['input/asciimath', '[tex]/extpfeil', '[tex]/amscd', '[tex]/newcommand', '[pretext]/mathjaxknowl3.js'],
    paths: {pretext: "https://pretextbook.org/js/lib"},
  },
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><script>// Make *any* pre with class 'sagecell-sage' an executable Sage cell
// Their results will be linked, only within language type
sagecell.makeSagecell({inputLocation: 'pre.sagecell-sage',
                       linked: true,
                       languages: ['sage'],
                       evalButtonText: 'Evaluate (Sage)'});
</script><script async="" src="https://cse.google.com/cse.js?cx=015103900096539427448:ngwuia10qci"></script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.13/pretext.js"></script><script>miniversion=0.674</script><script src="https://pretextbook.org/js/0.13/pretext_add_on.js?x=1"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/banner_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/toc_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/knowls_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/style_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/colors_blue_grey.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/setcolors.css" rel="stylesheet" type="text/css">
<!--** eBookCongig is necessary to configure interactive       **-->
<!--** Runestone components to run locally in reader's browser **-->
<!--** No external communication:                              **-->
<!--**     log level is 0, Runestone Services are disabled     **-->
<script type="text/javascript">
eBookConfig = {};
eBookConfig.useRunestoneServices = false;
eBookConfig.host = 'http://127.0.0.1:8000';
eBookConfig.course = 'PTX Course: Title Here';
eBookConfig.basecourse = 'PTX Base Course';
eBookConfig.isLoggedIn = false;
eBookConfig.email = '';
eBookConfig.isInstructor = false;
eBookConfig.logLevel = 0;
eBookConfig.username = '';
eBookConfig.readings = null;
eBookConfig.activities = null;
eBookConfig.downloadsEnabled = false;
eBookConfig.allow_pairs = false;
eBookConfig.enableScratchAC = false;
eBookConfig.build_info = "";
eBookConfig.python3 = null;
eBookConfig.acDefaultLanguage = 'python';
eBookConfig.runestone_version = '5.0.1';
eBookConfig.jobehost = '';
eBookConfig.proxyuri_runs = '';
eBookConfig.proxyuri_files = '';
eBookConfig.enable_chatcodes =  false;
</script>
<!--*** Runestone Services ***-->
<script type="text/javascript" src="https://runestone.academy/cdn/runestone/6.2.1/runtime.b0f8547c48f16a9f.bundle.js"></script><script type="text/javascript" src="https://runestone.academy/cdn/runestone/6.2.1/637.d54be67956c5c660.bundle.js"></script><script type="text/javascript" src="https://runestone.academy/cdn/runestone/6.2.1/runestone.0e9550fe42760516.bundle.js"></script><link rel="stylesheet" type="text/css" href="https://runestone.academy/cdn/runestone/6.2.1/637.fafafbd97df8a0d1.css">
<link rel="stylesheet" type="text/css" href="https://runestone.academy/cdn/runestone/6.2.1/runestone.e4d5592da655219f.css">
</head>
<body class="pretext-book ignore-math has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(\newcommand{\avec}{{\mathbf a}}
\newcommand{\bvec}{{\mathbf b}}
\newcommand{\cvec}{{\mathbf c}}
\newcommand{\dvec}{{\mathbf d}}
\newcommand{\dtil}{\widetilde{\mathbf d}}
\newcommand{\evec}{{\mathbf e}}
\newcommand{\fvec}{{\mathbf f}}
\newcommand{\nvec}{{\mathbf n}}
\newcommand{\pvec}{{\mathbf p}}
\newcommand{\qvec}{{\mathbf q}}
\newcommand{\svec}{{\mathbf s}}
\newcommand{\tvec}{{\mathbf t}}
\newcommand{\uvec}{{\mathbf u}}
\newcommand{\vvec}{{\mathbf v}}
\newcommand{\wvec}{{\mathbf w}}
\newcommand{\xvec}{{\mathbf x}}
\newcommand{\yvec}{{\mathbf y}}
\newcommand{\zvec}{{\mathbf z}}
\newcommand{\rvec}{{\mathbf r}}
\newcommand{\mvec}{{\mathbf m}}
\newcommand{\zerovec}{{\mathbf 0}}
\newcommand{\onevec}{{\mathbf 1}}
\newcommand{\real}{{\mathbb R}}
\newcommand{\twovec}[2]{\left[\begin{array}{r}#1 \\ #2
\end{array}\right]}
\newcommand{\ctwovec}[2]{\left[\begin{array}{c}#1 \\ #2
\end{array}\right]}
\newcommand{\threevec}[3]{\left[\begin{array}{r}#1 \\ #2 \\ #3
\end{array}\right]}
\newcommand{\cthreevec}[3]{\left[\begin{array}{c}#1 \\ #2 \\ #3
\end{array}\right]}
\newcommand{\fourvec}[4]{\left[\begin{array}{r}#1 \\ #2 \\ #3 \\ #4
\end{array}\right]}
\newcommand{\cfourvec}[4]{\left[\begin{array}{c}#1 \\ #2 \\ #3 \\ #4
\end{array}\right]}
\newcommand{\fivevec}[5]{\left[\begin{array}{r}#1 \\ #2 \\ #3 \\
#4 \\ #5 \\ \end{array}\right]}
\newcommand{\cfivevec}[5]{\left[\begin{array}{c}#1 \\ #2 \\ #3 \\
#4 \\ #5 \\ \end{array}\right]}
\newcommand{\mattwo}[4]{\left[\begin{array}{rr}#1 \amp #2 \\ #3 \amp #4 \\ \end{array}\right]}
\newcommand{\laspan}[1]{\text{Span}\{#1\}}
\newcommand{\bcal}{{\cal B}}
\newcommand{\ccal}{{\cal C}}
\newcommand{\scal}{{\cal S}}
\newcommand{\wcal}{{\cal W}}
\newcommand{\ecal}{{\cal E}}
\newcommand{\coords}[2]{\left\{#1\right\}_{#2}}
\newcommand{\gray}[1]{\color{gray}{#1}}
\newcommand{\lgray}[1]{\color{lightgray}{#1}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\row}{\text{Row}}
\newcommand{\col}{\text{Col}}
\renewcommand{\row}{\text{Row}}
\newcommand{\nul}{\text{Nul}}
\newcommand{\var}{\text{Var}}
\newcommand{\corr}{\text{corr}}
\newcommand{\len}[1]{\left|#1\right|}
\newcommand{\bbar}{\overline{\bvec}}
\newcommand{\bhat}{\widehat{\bvec}}
\newcommand{\bperp}{\bvec^\perp}
\newcommand{\xhat}{\widehat{\xvec}}
\newcommand{\vhat}{\widehat{\vvec}}
\newcommand{\uhat}{\widehat{\uvec}}
\newcommand{\what}{\widehat{\wvec}}
\newcommand{\Sighat}{\widehat{\Sigma}}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\definecolor{fillinmathshade}{gray}{0.9}
\newcommand{\fillinmath}[1]{\mathchoice{\colorbox{fillinmathshade}{$\displaystyle     \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\textstyle        \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptstyle      \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptscriptstyle\phantom{\,#1\,}$}}}
\)</span></div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="ula.html"><span class="title">Understanding Linear Algebra</span></a></h1>
<p class="byline">David Austin</p>
</div>
<div class="searchwrapper" role="search"><div class="gcse-search"></div></div>
</div></div>
<nav id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3">
<a class="index-button toolbar-item button" href="index-1.html" title="Index">Index</a><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="sec-svd-intro.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="chap7.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="backmatter.html" title="Next">Next</a></span>
</div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="sec-svd-intro.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="chap7.html" title="Up">Up</a><a class="next-button button toolbar-item" href="backmatter.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link frontmatter">
<a href="frontmatter.html" data-scroll="frontmatter" class="internal"><span class="title">Front Matter</span></a><ul>
<li><a href="dedication-1.html" data-scroll="dedication-1" class="internal">Dedication</a></li>
<li><a href="colophon-1.html" data-scroll="colophon-1" class="internal">Colophon</a></li>
<li><a href="preface-1.html" data-scroll="preface-1" class="internal">Our goals</a></li>
</ul>
</li>
<li class="link">
<a href="chap1.html" data-scroll="chap1" class="internal"><span class="codenumber">1</span> <span class="title">Systems of equations</span></a><ul>
<li><a href="sec-expect.html" data-scroll="sec-expect" class="internal">What can we expect</a></li>
<li><a href="sec-finding-solutions.html" data-scroll="sec-finding-solutions" class="internal">Finding solutions to linear systems</a></li>
<li><a href="sec-sage-introduction.html" data-scroll="sec-sage-introduction" class="internal">Computation with Sage</a></li>
<li><a href="sec-pivots.html" data-scroll="sec-pivots" class="internal">Pivots and their influence on solution spaces</a></li>
</ul>
</li>
<li class="link">
<a href="chap2.html" data-scroll="chap2" class="internal"><span class="codenumber">2</span> <span class="title">Vectors, matrices, and linear combinations</span></a><ul>
<li><a href="sec-vectors-lin-combs.html" data-scroll="sec-vectors-lin-combs" class="internal">Vectors and linear combinations</a></li>
<li><a href="sec-matrices-lin-combs.html" data-scroll="sec-matrices-lin-combs" class="internal">Matrix multiplication and linear combinations</a></li>
<li><a href="sec-span.html" data-scroll="sec-span" class="internal">The span of a set of vectors</a></li>
<li><a href="sec-linear-dep.html" data-scroll="sec-linear-dep" class="internal">Linear independence</a></li>
<li><a href="sec-linear-trans.html" data-scroll="sec-linear-trans" class="internal">Matrix transformations</a></li>
<li><a href="sec-transforms-geom.html" data-scroll="sec-transforms-geom" class="internal">The geometry of matrix transformations</a></li>
</ul>
</li>
<li class="link">
<a href="chap3.html" data-scroll="chap3" class="internal"><span class="codenumber">3</span> <span class="title">Invertibility, bases, and coordinate systems</span></a><ul>
<li><a href="sec-matrix-inverse.html" data-scroll="sec-matrix-inverse" class="internal">Invertibility</a></li>
<li><a href="sec-bases.html" data-scroll="sec-bases" class="internal">Bases and coordinate systems</a></li>
<li><a href="sec-jpeg.html" data-scroll="sec-jpeg" class="internal">Image compression</a></li>
<li><a href="sec-determinants.html" data-scroll="sec-determinants" class="internal">Determinants</a></li>
<li><a href="sec-subspaces.html" data-scroll="sec-subspaces" class="internal">Subspaces</a></li>
</ul>
</li>
<li class="link">
<a href="chap4.html" data-scroll="chap4" class="internal"><span class="codenumber">4</span> <span class="title">Eigenvalues and eigenvectors</span></a><ul>
<li><a href="sec-eigen-intro.html" data-scroll="sec-eigen-intro" class="internal">An introduction to eigenvalues and eigenvectors</a></li>
<li><a href="sec-eigen-find.html" data-scroll="sec-eigen-find" class="internal">Finding eigenvalues and eigenvectors</a></li>
<li><a href="sec-eigen-diag.html" data-scroll="sec-eigen-diag" class="internal">Diagonalization, similarity, and powers of a matrix</a></li>
<li><a href="sec-dynamical.html" data-scroll="sec-dynamical" class="internal">Dynamical systems</a></li>
<li><a href="sec-stochastic.html" data-scroll="sec-stochastic" class="internal">Markov chains and Google's PageRank algorithm</a></li>
</ul>
</li>
<li class="link">
<a href="chap5.html" data-scroll="chap5" class="internal"><span class="codenumber">5</span> <span class="title">Linear algebra and computing</span></a><ul>
<li><a href="sec-gaussian-revisited.html" data-scroll="sec-gaussian-revisited" class="internal">Gaussian elimination revisited</a></li>
<li><a href="sec-power-method.html" data-scroll="sec-power-method" class="internal">Finding eigenvectors numerically</a></li>
</ul>
</li>
<li class="link">
<a href="chap6.html" data-scroll="chap6" class="internal"><span class="codenumber">6</span> <span class="title">Orthogonality and Least Squares</span></a><ul>
<li><a href="sec-dot-product.html" data-scroll="sec-dot-product" class="internal">The dot product</a></li>
<li><a href="sec-transpose.html" data-scroll="sec-transpose" class="internal">Orthogonal complements and the matrix transpose</a></li>
<li><a href="sec-orthogonal-bases.html" data-scroll="sec-orthogonal-bases" class="internal">Orthogonal bases and projections</a></li>
<li><a href="sec-gram-schmidt.html" data-scroll="sec-gram-schmidt" class="internal">Finding orthogonal bases</a></li>
<li><a href="sec-least-squares.html" data-scroll="sec-least-squares" class="internal">Orthogonal least squares</a></li>
</ul>
</li>
<li class="link">
<a href="chap7.html" data-scroll="chap7" class="internal"><span class="codenumber">7</span> <span class="title">The Spectral Theorem and singular value decompositions</span></a><ul>
<li><a href="sec-symmetric-matrices.html" data-scroll="sec-symmetric-matrices" class="internal">Symmetric matrices and variance</a></li>
<li><a href="sec-quadratic-forms.html" data-scroll="sec-quadratic-forms" class="internal">Quadratic forms</a></li>
<li><a href="sec-pca.html" data-scroll="sec-pca" class="internal">Principal Component Analysis</a></li>
<li><a href="sec-svd-intro.html" data-scroll="sec-svd-intro" class="internal">Singular Value Decompositions</a></li>
<li><a href="sec-svd-uses.html" data-scroll="sec-svd-uses" class="active">Using Singular Value Decompositions</a></li>
</ul>
</li>
<li class="link backmatter"><a href="backmatter.html" data-scroll="backmatter" class="internal"><span class="title">Back Matter</span></a></li>
<li class="link"><a href="app-sage-reference.html" data-scroll="app-sage-reference" class="internal"><span class="codenumber">A</span> <span class="title">Sage Reference</span></a></li>
<li class="link"><a href="index-1.html" data-scroll="index-1" class="internal"><span class="title">Index</span></a></li>
<li class="link"><a href="colophon-2.html" data-scroll="colophon-2" class="internal"><span class="title">Colophon</span></a></li>
</ul></nav><div class="extras"><nav><a class="pretext-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content">
<section class="section" id="sec-svd-uses"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">7.5</span> <span class="title">Using Singular Value Decompositions</span>
</h2>
<section class="introduction" id="introduction-38"><p id="p-8154">We've now seen what singular value decompositions are, how to construct them, and how they provide important information about a matrix such as orthonormal bases for the four fundamental subspaces.  This puts us in a good position to begin using singular value decompositions to solve a wide variety of problems.</p>
<p id="p-8155">Given the fact that singular value decompositions so immediately convey fundamental data about a matrix, it seems natural that some of our previous work can be reinterpreted in terms of singular value decompositions.  Therefore, we'll take some time in this section to revisit some familiar issues, such as least squares problems and principal component analysis, while also looking at some new applications.</p>
<article class="exploration project-like" id="exploration-30"><h3 class="heading">
<span class="type">Preview Activity</span><span class="space"> </span><span class="codenumber">7.5.1</span><span class="period">.</span>
</h3>
<p id="p-8156">Suppose that <span class="process-math">\(A = U\Sigma V^T\)</span> where</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\Sigma = \begin{bmatrix}
13 \amp 0 \amp 0 \amp 0 \\
0 \amp 8 \amp 0 \amp 0 \\
0 \amp 0 \amp 2 \amp 0 \\
0 \amp0 \amp 0 \amp 0 \\
0 \amp0 \amp 0 \amp 0
\end{bmatrix},
\end{equation*}
</div>
<p class="continuation">vectors <span class="process-math">\(\uvec_j\)</span> form the columns of <span class="process-math">\(U\text{,}\)</span> and vectors <span class="process-math">\(\vvec_j\)</span> form the columns of <span class="process-math">\(V\text{.}\)</span></p>
<ol class="lower-alpha">
<li id="li-5665"><p id="p-8157">What are the shapes of the matrices <span class="process-math">\(A\text{,}\)</span> <span class="process-math">\(U\text{,}\)</span> and <span class="process-math">\(V\text{?}\)</span></p></li>
<li id="li-5666"><p id="p-8158">What is the rank of <span class="process-math">\(A\text{?}\)</span></p></li>
<li id="li-5667"><p id="p-8159">Describe how to find an orthonormal basis for <span class="process-math">\(\col(A)\text{.}\)</span></p></li>
<li id="li-5668"><p id="p-8160">Describe how to find an orthonormal basis for <span class="process-math">\(\nul(A)\text{.}\)</span></p></li>
<li id="li-5669"><p id="p-8161">If the columns of <span class="process-math">\(Q\)</span> form an orthonormal basis for <span class="process-math">\(\col(A)\text{,}\)</span> what is <span class="process-math">\(Q^TQ\text{?}\)</span></p></li>
<li id="li-5670"><p id="p-8162">How would you form a matrix that projects vectors orthogonally onto <span class="process-math">\(\col(A)\text{?}\)</span></p></li>
</ol></article></section><section class="subsection" id="subsection-113"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">7.5.1</span> <span class="title">Least squares problems</span>
</h3>
<p id="p-8170">Least squares problems, which we explored in <a href="sec-least-squares.html" class="internal" title="Section 6.5: Orthogonal least squares">Section 6.5</a>, arise when we are confronted with an inconsistent linear system <span class="process-math">\(A\xvec=\bvec\text{.}\)</span>  Since there is no solution to the system, we instead find the vector <span class="process-math">\(\xvec\)</span> minimizing the distance between <span class="process-math">\(\bvec\)</span> and <span class="process-math">\(A\xvec\text{.}\)</span> That is, we find the vector <span class="process-math">\(\xhat\text{,}\)</span> the least squares approximate solution, by solving <span class="process-math">\(A\xhat=\bhat\)</span> where <span class="process-math">\(\bhat\)</span> is the orthogonal projection of <span class="process-math">\(\bvec\)</span> onto the column space of <span class="process-math">\(A\text{.}\)</span></p>
<p id="p-8171">If we have a singular value decomposition <span class="process-math">\(A=U\Sigma V^T\text{,}\)</span> then the number of nonzero singular values <span class="process-math">\(r\)</span> tells us the rank of <span class="process-math">\(A\text{,}\)</span> and the first <span class="process-math">\(r\)</span> columns of <span class="process-math">\(U\)</span> form an orthonormal basis for <span class="process-math">\(\col(A)\text{.}\)</span>  This basis may be used to project vectors onto <span class="process-math">\(\col(A)\)</span> and hence to solve least squares problems.</p>
<p id="p-8172">Before exploring this connection further, we will introduce Sage as a tool for automating the construction of singular value decompositions.  One new feature is that we need to declare our matrix to consist of floating point entries.  We do this by including <code class="code-inline tex2jax_ignore">RDF</code> inside the matrix definition, as illustrated in the following cell. <pre class="ptx-sagecell sagecell-sage" id="sage-245"><script type="text/x-sage">A = matrix(RDF, 3, 2, [1,0,-1,1,1,1])
U, Sigma, V = A.SVD()	  
print(U)
print('---------')
print(Sigma)
print('---------')
print(V)
</script></pre></p>
<article class="activity project-like" id="activity-105"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.5.2</span><span class="period">.</span>
</h4>
<p id="p-8173">Consider the equation <span class="process-math">\(A\xvec=\bvec\)</span> where</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\begin{bmatrix}
1 \amp 0 \\
1 \amp 1 \\
1 \amp 2
\end{bmatrix}
\xvec = \threevec{-1}36
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-5677"><p id="p-8174">Find a singular value decomposition for <span class="process-math">\(A\)</span> using the Sage cell below.  What are singular values of <span class="process-math">\(A\text{?}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-246"><script type="text/x-sage">
</script></pre></p></li>
<li id="li-5678"><p id="p-8175">What is <span class="process-math">\(r\text{,}\)</span> the rank of <span class="process-math">\(A\text{?}\)</span>  How can we identify an orthonormal basis for <span class="process-math">\(\col(A)\text{?}\)</span></p></li>
<li id="li-5679">
<p id="p-8176">Form the reduced singular value decomposition <span class="process-math">\(U_r\Sigma_rV_r^T\)</span> by constructing the matrix <span class="process-math">\(U_r\text{,}\)</span> consisting of the first <span class="process-math">\(r\)</span> columns of <span class="process-math">\(U\text{,}\)</span> the matrix <span class="process-math">\(V_r\text{,}\)</span> consisting of the first <span class="process-math">\(r\)</span> columns of <span class="process-math">\(V\text{,}\)</span> and <span class="process-math">\(\Sigma_r\text{,}\)</span> a square <span class="process-math">\(r\times r\)</span> diagonal matrix.  Verify that <span class="process-math">\(A=U_r\Sigma_r V_r^T\text{.}\)</span></p>
<p id="p-8177">You may find it convenient to remember that if <code class="code-inline tex2jax_ignore">B</code> is a matrix defined in Sage, then <code class="code-inline tex2jax_ignore">B.matrix_from_columns( list )</code> and <code class="code-inline tex2jax_ignore">B.matrix_from_rows( list )</code> can be used to extract columns or rows from <code class="code-inline tex2jax_ignore">B</code>.  For instance, <code class="code-inline tex2jax_ignore">B.matrix_from_rows([0,1,2])</code> provides a matrix formed from the first three rows of <code class="code-inline tex2jax_ignore">B</code>. <pre class="ptx-sagecell sagecell-sage" id="sage-247"><script type="text/x-sage">
</script></pre></p>
</li>
<li id="li-5680"><p id="p-8178">How does the reduced singular value decomposition provide a matrix whose columns are an orthonormal basis for <span class="process-math">\(\col(A)\text{?}\)</span></p></li>
<li id="li-5681">
<p id="p-8179">Explain why a least squares approximate solution <span class="process-math">\(\xhat\)</span> satisfies</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A\xhat = U_rU_r^T\bvec.
\end{equation*}
</div>
</li>
<li id="li-5682"><p id="p-8180">What is the product <span class="process-math">\(V_r^TV_r\)</span> and why does it have this form?</p></li>
<li id="li-5683">
<p id="p-8181">Explain why</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\xhat = V_r\Sigma_r^{-1}U_r^T\bvec
\end{equation*}
</div>
<p class="continuation">is the least squares approximate solution, and use this expression to find <span class="process-math">\(\xhat\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-248"><script type="text/x-sage">
</script></pre></p>
</li>
</ol></article><p id="p-8198">This activity demonstrates the power of a singular value decomposition to find a least squares approximate solution for an equation <span class="process-math">\(A\xvec = \bvec\text{.}\)</span>  Because it immediately provides an orthonormal basis for <span class="process-math">\(\col(A)\text{,}\)</span> something that we've had to construct using the Gram-Schmidt process in the past, we can easily project <span class="process-math">\(\bvec\)</span> onto <span class="process-math">\(\col(A)\text{,}\)</span> which results in a simple expression for <span class="process-math">\(\xhat\text{.}\)</span></p>
<article class="proposition theorem-like" id="prop-svd-ols"><h4 class="heading">
<span class="type">Proposition</span><span class="space"> </span><span class="codenumber">7.5.1</span><span class="period">.</span>
</h4>
<p id="p-8199">If <span class="process-math">\(A=U_r\Sigma_r V_r^T\)</span> is a reduced singular value decomposition of <span class="process-math">\(A\text{,}\)</span> then a least squares approximate solution to <span class="process-math">\(A\xvec=\bvec\)</span> is given by</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\xhat = V_r\Sigma_r^{-1}U_r^T\bvec.
\end{equation*}
</div></article><p id="p-8200">If the columns of <span class="process-math">\(A\)</span> are linearly independent, then the equation <span class="process-math">\(A\xhat = \bhat\)</span> has only one solution so there is a unique least squares approximate solution <span class="process-math">\(\xhat\text{.}\)</span> Otherwise, the expression in <a href="" class="xref" data-knowl="./knowl/prop-svd-ols.html" title="Proposition 7.5.1">Proposition 7.5.1</a> produces the solution to <span class="process-math">\(A\xhat=\bhat\)</span> having the shortest length.</p>
<p id="p-8201"> The matrix <span class="process-math">\(A^+ = V_r\Sigma_r^{-1}U_r^T\)</span> is known as the <em class="emphasis">Moore-Penrose psuedoinverse</em> of <span class="process-math">\(A\text{.}\)</span>  When <span class="process-math">\(A\)</span> is invertible, <span class="process-math">\(A^{-1} = A^+\text{.}\)</span></p></section><section class="subsection" id="subsection-114"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">7.5.2</span> <span class="title">Rank <span class="process-math">\(k\)</span> approximations</span>
</h3>
<p id="p-8202">If we have a singular value decomposition for a matrix <span class="process-math">\(A\text{,}\)</span> we can form a sequence of matrices <span class="process-math">\(A_k\)</span> that approximate <span class="process-math">\(A\)</span> with increasing accuracy.  This may feel familiar to calculus students who have seen the way in which a function <span class="process-math">\(f(x)\)</span> can be approximated by a linear function, a quadratic function, and so forth with increasing accuracy.</p>
<p id="p-8203">We'll begin with a singular value decomposition of a rank <span class="process-math">\(r\)</span> matrix <span class="process-math">\(A\)</span> so that <span class="process-math">\(A=U\Sigma V^T\text{.}\)</span>  To create the approximating matrix <span class="process-math">\(A_k\text{,}\)</span> we keep the first <span class="process-math">\(k\)</span> singular values and set the others to zero.  For instance, if <span class="process-math">\(\Sigma = \begin{bmatrix}
22 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 14 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 3 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \\
\end{bmatrix}
\text{,}\)</span> we can form matrices</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\Sigma^{(1)} = \begin{bmatrix}
22 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \\
\end{bmatrix}, \hspace{24pt}
\Sigma^{(2)} = \begin{bmatrix}
22 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 14 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \\
\end{bmatrix}
\end{equation*}
</div>
<p class="continuation">and define <span class="process-math">\(A_1 = U\Sigma^{(1)}V^T\)</span> and <span class="process-math">\(A_2 =
U\Sigma^{(2)}V^T\text{.}\)</span>  Because <span class="process-math">\(A_k\)</span> has <span class="process-math">\(k\)</span> nonzero singular values, we know that <span class="process-math">\(\rank(A_k) = k\text{.}\)</span>  In fact, there is a sense in which <span class="process-math">\(A_k\)</span> is the closest matrix to <span class="process-math">\(A\)</span> among all rank <span class="process-math">\(k\)</span> matrices.</p>
<article class="activity project-like" id="activity-106"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.5.3</span><span class="period">.</span>
</h4>
<p id="p-8204">Let's consider a matrix <span class="process-math">\(A=U\Sigma V^T\)</span> where</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-36">
\begin{align*}
\amp U = \begin{bmatrix}
\frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \\
\frac{1}{2} \amp \frac{1}{2} \amp -\frac{1}{2} \amp -\frac{1}{2} \\
\frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2} \amp -\frac{1}{2} \\
\frac{1}{2} \amp -\frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2} \\
\end{bmatrix},\hspace{10pt}
\Sigma = \begin{bmatrix}
500 \amp 0 \amp 0 \amp 0 \\
0 \amp 100 \amp 0 \amp 0 \\
0 \amp 0 \amp 20 \amp 0  \\
0 \amp 0 \amp 0 \amp 4
\end{bmatrix}\\
\amp V = \begin{bmatrix}
\frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \\
\frac{1}{2} \amp -\frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2} \\
-\frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \\
-\frac{1}{2} \amp \frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2}
\end{bmatrix}
\end{align*}
</div>
<p class="continuation">Evaluating the following cell will create the matrices <code class="code-inline tex2jax_ignore">U</code>, <code class="code-inline tex2jax_ignore">V</code>, and <code class="code-inline tex2jax_ignore">Sigma</code>.  Notice how the <code class="code-inline tex2jax_ignore">diagonal_matrix</code> command provides a convenient way to form the diagonal matrix <span class="process-math">\(\Sigma\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-249"><script type="text/x-sage">h = 1/2
U = matrix(4,4,[h,h,h,h,  h,h,-h,-h,  h,-h,h,-h,  h,-h,-h,h])	    
V = matrix(4,4,[h,h,h,h,  h,-h,-h,h,  -h,-h,h,h,  -h,h,-h,h])
Sigma = diagonal_matrix([500, 100, 20, 4])
</script></pre></p>
<ol class="lower-alpha">
<li id="li-5698"><p id="p-8205">Form the matrix <span class="process-math">\(A=U\Sigma V^T\text{.}\)</span>  What is <span class="process-math">\(\rank(A)\text{?}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-250"><script type="text/x-sage">
</script></pre></p></li>
<li id="li-5699"><p id="p-8206">Now form the approximating matrix <span class="process-math">\(A_1=U\Sigma^{(1)}
V^T\text{.}\)</span>  What is <span class="process-math">\(\rank(A_1)\text{?}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-251"><script type="text/x-sage">
</script></pre></p></li>
<li id="li-5700"><p id="p-8207">Find the error in the approximation <span class="process-math">\(A\approx
A_1\)</span> by finding <span class="process-math">\(A-A_1\text{.}\)</span></p></li>
<li id="li-5701"><p id="p-8208">Now find <span class="process-math">\(A_2 = U\Sigma^{(2)} V^T\)</span> and the error <span class="process-math">\(A-A_2\text{.}\)</span>  What is <span class="process-math">\(\rank(A_2)\text{?}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-252"><script type="text/x-sage">
</script></pre></p></li>
<li id="li-5702"><p id="p-8209">Find <span class="process-math">\(A_3 = U\Sigma^{(3)} V^T\)</span> and the error <span class="process-math">\(A-A_3\text{.}\)</span>  What is <span class="process-math">\(\rank(A_3)\text{?}\)</span></p></li>
<li id="li-5703"><p id="p-8210">What would happen if we were to compute <span class="process-math">\(A_4\text{?}\)</span></p></li>
<li id="li-5704"><p id="p-8211">What do you notice about the error <span class="process-math">\(A-A_k\)</span> as <span class="process-math">\(k\)</span> increases?</p></li>
</ol></article><p id="p-8228">In this activity, the approximating matrix <span class="process-math">\(A_k\)</span> has rank <span class="process-math">\(k\)</span> because its singular value decomposition has <span class="process-math">\(k\)</span> nonzero singular values.  We then saw how the difference between <span class="process-math">\(A\)</span> and the approximations <span class="process-math">\(A_k\)</span> decreases as <span class="process-math">\(k\)</span> increases, which means that the sequence <span class="process-math">\(A_k\)</span> forms better approximations as <span class="process-math">\(k\)</span> increases.</p>
<p id="p-8229">Another way to represent <span class="process-math">\(A_k\)</span> is with a reduced singular value decomposition so that <span class="process-math">\(A_k = U_k\Sigma_kV_k^T\)</span> where</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
U_k = \begin{bmatrix}
\uvec_1 \amp \ldots \amp \uvec_k
\end{bmatrix},\hspace{10pt}
\Sigma_k = \begin{bmatrix}
\sigma_1 \amp 0 \amp \ldots \amp 0 \\
0 \amp \sigma_2 \amp \ldots \amp 0 \\
\vdots \amp \vdots \amp \ddots \amp \vdots \\
0 \amp 0 \amp \ldots \amp \sigma_k
\end{bmatrix},\hspace{10pt}
V_k = \begin{bmatrix}
\vvec_1 \amp \ldots \amp \vvec_k
\end{bmatrix}\text{.}
\end{equation*}
</div>
<p class="continuation">Notice that the rank <span class="process-math">\(1\)</span> matrix <span class="process-math">\(A_1\)</span> then has the form <span class="process-math">\(A_1 = \uvec_1\begin{bmatrix}\sigma_1\end{bmatrix}
\vvec_1^T = \sigma_1\uvec_1\vvec_1^T\)</span> and that we can similarly write:</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-37">
\begin{align*}
A \approx A_1 \amp = \sigma_1\uvec_1\vvec_1^T\\
A \approx A_2 \amp = \sigma_1\uvec_1\vvec_1^T +
\sigma_2\uvec_2\vvec_2^T\\
A \approx A_3 \amp = \sigma_1\uvec_1\vvec_1^T +
\sigma_2\uvec_2\vvec_2^T + 
\sigma_3\uvec_3\vvec_3^T\\
\vdots \amp\\
A = A_r \amp = \sigma_1\uvec_1\vvec_1^T +
\sigma_2\uvec_2\vvec_2^T +
\sigma_3\uvec_3\vvec_3^T + 
\ldots +
\sigma_r\uvec_r\vvec_r^T\text{.}
\end{align*}
</div>
<p id="p-8230">Given two vectors <span class="process-math">\(\uvec\)</span> and <span class="process-math">\(\vvec\text{,}\)</span> the matrix <span class="process-math">\(\uvec~\vvec^T\)</span> is called the <em class="emphasis">outer product</em> of <span class="process-math">\(\uvec\)</span> and <span class="process-math">\(\vvec\text{.}\)</span>  (The dot product <span class="process-math">\(\uvec\cdot\vvec=\uvec^T\vvec\)</span> is sometimes called the <em class="emphasis">inner product</em>.)  An outer product will always be a rank <span class="process-math">\(1\)</span> matrix so we see above how <span class="process-math">\(A_k\)</span> is obtained by adding together <span class="process-math">\(k\)</span> rank <span class="process-math">\(1\)</span> matrices, each of which gets us one step closer to the original matrix <span class="process-math">\(A\text{.}\)</span></p></section><section class="subsection" id="subsection-115"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">7.5.3</span> <span class="title">Principal component analysis</span>
</h3>
<p id="p-8231">In <a href="sec-pca.html" class="internal" title="Section 7.3: Principal Component Analysis">Section 7.3</a>, we explored principal component analysis as a technique to reduce the dimension of a dataset. In particular, we constructed the covariance matrix <span class="process-math">\(C\)</span> from a demeaned data matrix and saw that the eigenvalues and eigenvectors of <span class="process-math">\(C\)</span> tell us about the variance of the dataset in different directions.  We referred to the eigenvectors of <span class="process-math">\(C\)</span> as <em class="emphasis">principal components</em> and found that projecting the data onto a subspace defined by the first few principal components frequently gave us a way to visualize the dataset.  As we added more principal components, we retained more information about the original dataset.  This feels similar to the rank <span class="process-math">\(k\)</span> approximations we have just seen so let's explore the connection.</p>
<p id="p-8232">Suppose that we have a dataset with <span class="process-math">\(N\)</span> points, that <span class="process-math">\(A\)</span> represents the demeaned data matrix, that <span class="process-math">\(A = U\Sigma V^T\)</span> is a singular value decomposition, and that the singular values are <span class="process-math">\(A\)</span> are denoted as <span class="process-math">\(\sigma_i\text{.}\)</span> It follows that the covariance matrix</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
C = \frac1N AA^T = \frac1N (U\Sigma V^T) (U\Sigma V^T)^T =
U\left(\frac1N \Sigma \Sigma^T\right) U^T.
\end{equation*}
</div>
<p class="continuation">Notice that <span class="process-math">\(\frac1N \Sigma\Sigma^T\)</span> is a diagonal matrix whose diagonal entries are <span class="process-math">\(\frac1N\sigma_i^2\text{.}\)</span>  Therefore, it follows that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
C = 
U\left(\frac1N \Sigma \Sigma^T\right) U^T
\end{equation*}
</div>
<p class="continuation">is an orthgonal diagonalization of <span class="process-math">\(C\)</span> showing that</p>
<ul class="disc">
<li id="li-5719"><p id="p-8233">the principal components of the dataset, which are the eigenvectors of <span class="process-math">\(C\text{,}\)</span> are given by the columns of <span class="process-math">\(U\text{.}\)</span>  In other words, the left singular vectors of <span class="process-math">\(A\)</span> are the principal components of the dataset.</p></li>
<li id="li-5720">
<p id="p-8234">the variance in the direction of a principal component is the associated eigenvalue of <span class="process-math">\(C\)</span> and therefore</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
V_{\uvec_i} = \frac1N\sigma_i^2.
\end{equation*}
</div>
</li>
</ul>
<article class="activity project-like" id="activity-107"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.5.4</span><span class="period">.</span>
</h4>
<p id="p-8235">Let's revisit the iris data set that we studied in <a href="sec-pca.html" class="internal" title="Section 7.3: Principal Component Analysis">Section 7.3</a>.  Remember that there are four measurements given for each of 150 irises and that each iris belongs to one of three species.</p>
<p id="p-8236">Evaluating the following cell will load the dataset and define the demeaned data matrix <span class="process-math">\(A\)</span> whose shape is <span class="process-math">\(4\times150\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-253"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/pca_iris.py', globals())
df.T
</script></pre></p>
<ol class="lower-alpha">
<li id="li-5721"><p id="p-8237">Find the singular values of <span class="process-math">\(A\)</span> using the command <code class="code-inline tex2jax_ignore">A.singular_values()</code> and use them to determine the variance <span class="process-math">\(V_{\uvec_j}\)</span> in the direction of each of the four principal components.  What is the fraction of variance retained by the first two principal components? <pre class="ptx-sagecell sagecell-sage" id="sage-254"><script type="text/x-sage">
</script></pre></p></li>
<li id="li-5722">
<p id="p-8238">We will now write the matrix <span class="process-math">\(\Gamma = \Sigma
V^T\)</span> so that <span class="process-math">\(A = U\Gamma\text{.}\)</span> Suppose that a demeaned data point, say, the 100th column of <span class="process-math">\(A\text{,}\)</span> is written as a linear combination of principal components:</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\xvec = c_1\uvec_1+c_2\uvec_2+c_3\uvec_3+c_4\uvec_4.
\end{equation*}
</div>
<p class="continuation">Explain why <span class="process-math">\(\fourvec{c_1}{c_2}{c_3}{c_4}\text{,}\)</span> the vector of coordinates of <span class="process-math">\(\xvec\)</span> in the basis of principal components, appears as 100th column of <span class="process-math">\(\Gamma\text{.}\)</span></p>
</li>
<li id="li-5723"><p id="p-8239">Suppose that we now project this demeaned data point <span class="process-math">\(\xvec\)</span> orthogonally onto the subspace spanned by the first two principal components <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\text{.}\)</span>  What are the coordinates of the projected point in this basis and how can we find them in the matrix <span class="process-math">\(\Gamma\text{?}\)</span></p></li>
<li id="li-5724"><p id="p-8240">Alternatively, consider the approximation <span class="process-math">\(A_2=U_2\Sigma_2V_2^T\)</span> of the demeaned data matrix <span class="process-math">\(A\text{.}\)</span> Explain why the 100th column of <span class="process-math">\(A_2\)</span> represents the projection of <span class="process-math">\(\xvec\)</span> onto the two-dimensional subspace spanned by the first two principal components, <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\text{.}\)</span>  Then explain why the coefficients in that projection, <span class="process-math">\(c_1\uvec_1 + c_2\uvec_2\text{,}\)</span> form the two-dimensional vector <span class="process-math">\(\twovec{c_1}{c_2}\)</span> that is the 100th column of <span class="process-math">\(\Gamma_2=\Sigma_2
V_2^T\text{.}\)</span></p></li>
<li id="li-5725"><p id="p-8241">Now we've seen that the columns of <span class="process-math">\(\Gamma_2 =
\Sigma_2 V_2^T\)</span> form the coordinates of the demeaned data points projected on to the two-dimensional subspace spanned by <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\text{.}\)</span> In the cell below, find a singular value decomposition of <span class="process-math">\(A\)</span> and use it to form the matrix <code class="code-inline tex2jax_ignore">Gamma2</code>.  When you evaluate this cell, you will see a plot of the projected demeaned data plots, similar to the one we created in <a href="sec-pca.html" class="internal" title="Section 7.3: Principal Component Analysis">Section 7.3</a>. <pre class="ptx-sagecell sagecell-sage" id="sage-255"><script type="text/x-sage"># Form the SVD of A and use it to form Gamma2

		    
Gamma2 = 

# The following will plot the projected demeaned data points
data = Gamma2.columns()
(list_plot(data[:50], color='blue', aspect_ratio=1) +
 list_plot(data[50:100], color='orange') +
 list_plot(data[100:], color='green'))
</script></pre></p></li>
</ol></article><p id="p-8254">In our first encounter with principal component analysis, we began with a demeaned data matrix <span class="process-math">\(A\text{,}\)</span> formed the covariance matrix <span class="process-math">\(C\text{,}\)</span> and used the eigenvalues and eigenvectors of <span class="process-math">\(C\)</span> to project the demeaned data onto a smaller dimensional subspace.  In this section, we have seen that a singular value decomposition of <span class="process-math">\(A\)</span> provides a more direct route: the left singular vectors of <span class="process-math">\(A\)</span> form the principal components and the approximating matrix <span class="process-math">\(A_k\)</span> represents the data points projected onto the subspace spanned by the first <span class="process-math">\(k\)</span> principal components.  The coordinates of a projected demeaned data point are given by the columns of <span class="process-math">\(\Gamma_k =
\Sigma_kV_k^T\text{.}\)</span></p></section><section class="subsection" id="subsection-116"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">7.5.4</span> <span class="title">Image compressing and denoising</span>
</h3>
<p id="p-8255">In addition to principal component analysis, the approximations <span class="process-math">\(A_k\)</span> of a matrix <span class="process-math">\(A\)</span> obtained from a singular value decomposition can be used in image processing. Remember that we studied the JPEG compression algorithm, whose foundation is the change of basis defined by the Discrete Cosine Transform, in <a href="sec-jpeg.html" class="internal" title="Section 3.3: Image compression">Section 3.3</a>. We will now see how a singular value decomposition provides another tool for both compressing images and removing noise in them.</p>
<article class="activity project-like" id="activity-108"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.5.5</span><span class="period">.</span>
</h4>
<p id="p-8256">Evaluating the following cell loads some data that we'll use in this activity.  To begin, it defines and displays a <span class="process-math">\(25\times15\)</span> matrix <span class="process-math">\(A\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-256"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/svd_compress.py', globals())
print(A)
</script></pre></p>
<ol class="lower-alpha">
<li id="li-5736">
<p id="p-8257">If we interpret 0 as black and 1 as white, this matrix represents an image as shown below. <pre class="ptx-sagecell sagecell-sage" id="sage-257"><script type="text/x-sage">display_matrix(A)
</script></pre> We will explore how the singular value decomposition helps us to compress this image.</p>
<ol class="decimal">
<li id="li-5737"><p id="p-8258">By inspecting the image represented by <span class="process-math">\(A\text{,}\)</span> identify a basis for <span class="process-math">\(\col(A)\)</span> and determine <span class="process-math">\(\rank(A)\text{.}\)</span></p></li>
<li id="li-5738"><p id="p-8259">The following cell plots the singular values of <span class="process-math">\(A\text{.}\)</span>  Explain how this plot verifies that the rank is what you found in the previous part. <pre class="ptx-sagecell sagecell-sage" id="sage-258"><script type="text/x-sage">plot_sv(A)
</script></pre></p></li>
<li id="li-5739"><p id="p-8260">There is a command <code class="code-inline tex2jax_ignore">approximate(A, k)</code> that creates the approximation <span class="process-math">\(A_k\text{.}\)</span>  Use the cell below to define <span class="process-math">\(k\)</span> and look at the images represented by the first few approximations.  What is the smallest value of <span class="process-math">\(k\)</span> for which <span class="process-math">\(A=A_k\text{?}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-259"><script type="text/x-sage">k = 
display_matrix(approximate(A, k))
</script></pre></p></li>
<li id="li-5740">
<p id="p-8261">Now we can see how the singular value decomposition allows us to compress images. Since this is a <span class="process-math">\(25\times15\)</span> matrix, we need <span class="process-math">\(25\cdot15=375\)</span> numbers to represent the image.  However, we can also reconstruct the image using a small number of singular values and vectors:</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = A_k = \sigma_1\uvec_1\vvec_1^T +
\sigma_2\uvec_2\vvec_2^T + \ldots +
\sigma_k\uvec_k\vvec_k^T.
\end{equation*}
</div>
<p class="continuation">What are the dimensions of the singular vectors <span class="process-math">\(\uvec_i\)</span> and <span class="process-math">\(\vvec_i\text{?}\)</span>  Between the singular vectors and singular values, how many numbers do we need to reconstruct <span class="process-math">\(A_k\)</span> for the smallest <span class="process-math">\(k\)</span> for which <span class="process-math">\(A=A_k\text{?}\)</span>  This is the compressed size of the image.</p>
</li>
<li id="li-5741"><p id="p-8262">The <em class="emphasis">compression ratio</em> is the ratio of the uncompressed size to the compressed size.  What compression ratio does this represent?</p></li>
</ol>
</li>
<li id="li-5742">
<p id="p-8263">Next we'll explore an example based on a photograph.</p>
<ol class="decimal">
<li id="li-5743">
<p id="p-8264">Consider the following image consisting of an array of <span class="process-math">\(316\times310\)</span> pixels stored in the matrix <span class="process-math">\(A\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-260"><script type="text/x-sage">A = matrix(RDF, image)
display_image(A)
</script></pre></p>
<p id="p-8265">Plot the singular values of <span class="process-math">\(A\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-261"><script type="text/x-sage">plot_sv(A)
</script></pre></p>
</li>
<li id="li-5744">
<p id="p-8266">Use the cell below to study the approximations <span class="process-math">\(A_k\)</span> for <span class="process-math">\(k=1, 10, 20, 50, 100\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-262"><script type="text/x-sage">k = 1
display_image(approximate(A, k))
</script></pre> Notice how the approximating image <span class="process-math">\(A_k\)</span> more closely approximates the original image <span class="process-math">\(A\)</span> as <span class="process-math">\(k\)</span> increases.</p>
<p id="p-8267">What is the compression ratio when <span class="process-math">\(k=50\text{?}\)</span> What is the compression ratio when <span class="process-math">\(k=100\text{?}\)</span> Notice how a higher compression ratio leads to a lower quality reconstruction of the image.</p>
</li>
</ol>
</li>
<li id="li-5745">
<p id="p-8268">A second, related application of the singular value decomposition to image processing is called <em class="emphasis">denoising</em>.  For example, consider the image represented by the matrix <span class="process-math">\(A\)</span> below. <pre class="ptx-sagecell sagecell-sage" id="sage-263"><script type="text/x-sage">A = matrix(RDF, noise.values)		    
display_matrix(A)
</script></pre> This image is similar to the image of the letter "O" we first studied in this activity, but there are splotchy regions in the background that result, perhaps, from scanning the image.  We think of the splotchy regions as noise, and our goal is to improve the quality of the image by reducing the noise.</p>
<ol class="decimal">
<li id="li-5746"><p id="p-8269">Plot the singular values below.  How are the singular values of this matrix similar to those represented by the clean image that we considered earlier and how are they different? <pre class="ptx-sagecell sagecell-sage" id="sage-264"><script type="text/x-sage">plot_sv(A)
</script></pre></p></li>
<li id="li-5747"><p id="p-8270">There is a natural point where the singular values dramatically decrease so it makes sense to think of the noise as being formed by the small singular values.  To denoise the image, we will therefore replace <span class="process-math">\(A\)</span> by its approximation <span class="process-math">\(A_k\text{,}\)</span> where <span class="process-math">\(k\)</span> is the point at which the singular values drop off. This has the effect of setting the small singular values to zero and hence eliminating the noise.  Choose an appropriate value of <span class="process-math">\(k\)</span> below and notice that the new image appears to be somewhat cleaned up as a result of removing the noise. <pre class="ptx-sagecell sagecell-sage" id="sage-265"><script type="text/x-sage">k = 
display_matrix(approximate(A, k))
</script></pre></p></li>
</ol>
</li>
</ol></article><p id="p-8297">Several examples illustrating how the singular value decomposition compresses images are available at this page from <a class="external" href="http://timbaumann.info/svd-image-compression-demo/" target="_blank">Tim Baumann.</a><a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-7" id="fn-7"><sup> 1 </sup></a></p></section><section class="subsection" id="subsection-117"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">7.5.5</span> <span class="title">Analyzing Supreme Court cases</span>
</h3>
<p id="p-8298">As we've seen, a singular value decomposition concentrates the most important features of a matrix into the first singular values and singular vectors.  We will now use this observation to extract meaning from a large dataset giving the voting records of Supreme Court justices.  A similar analysis appears in the paper <a class="external" href="https://www.pnas.org/content/100/13/7432" target="_blank">A pattern analysis of the second Rehnquist U.S. Supreme Court</a><a href="" data-knowl="" class="id-ref fn-knowl original" data-refid="hk-fn-8" id="fn-8"><sup> 2 </sup></a> by Lawrence Sirovich.</p>
<p id="p-8299">The makeup of the Supreme Court was unusually stable during a period from 1994-2005 when it was led by Chief Justice William Rehnquist.  This is sometimes called the <em class="emphasis">second Rehnquist court</em>.  The justices during this period were:</p>
<ul class="disc">
<li id="li-5772"><p id="p-8300">William Rehnquist</p></li>
<li id="li-5773"><p id="p-8301">Antonin Scalia</p></li>
<li id="li-5774"><p id="p-8302">Clarence Thomas</p></li>
<li id="li-5775"><p id="p-8303">Anthony Kennedy</p></li>
<li id="li-5776"><p id="p-8304">Sandra Day O'Connor</p></li>
<li id="li-5777"><p id="p-8305">John Paul Stevens</p></li>
<li id="li-5778"><p id="p-8306">David Souter</p></li>
<li id="li-5779"><p id="p-8307">Ruth Bader Ginsburg</p></li>
<li id="li-5780"><p id="p-8308">Stephen Breyer</p></li>
</ul>
<p id="p-8309">During this time, there were 911 cases in which all nine judges voted.  We would like to understand patterns in their voting.</p>
<article class="activity project-like" id="activity-109"><h4 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.5.6</span><span class="period">.</span>
</h4>
<p id="p-8310">Evaluating the following cell loads and displays a dataset describing the votes of each justice in these 911 cases. More specifically, an entry of +1 means that the justice represented by the row voted with the majority in the case represented by the column.  An entry of -1 means that justice was in the minority.  This information is also stored in the <span class="process-math">\(9\times911\)</span> matrix <span class="process-math">\(A\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-266"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/svd_supreme.py', globals())
A = matrix(RDF, cases.values)
cases
</script></pre> The justices are listed, very roughly, in order from more conservative to more progressive.</p>
<p id="p-8311">In this activity, it will be helpful to visualize the entries in various matrices and vectors.  The next cell displays the first 50 columns of the matrix <span class="process-math">\(A\)</span> with white representing an entry of +1, red representing -1, and black representing 0. <pre class="ptx-sagecell sagecell-sage" id="sage-267"><script type="text/x-sage">display_matrix(A.matrix_from_columns(range(50)))
</script></pre></p>
<ol class="lower-alpha">
<li id="li-5781"><p id="p-8312">Plot the singular values of <span class="process-math">\(A\)</span> below.  Describe the significance of this plot, including the relative contributions from the singular values <span class="process-math">\(\sigma_k\)</span> as <span class="process-math">\(k\)</span> increases. <pre class="ptx-sagecell sagecell-sage" id="sage-268"><script type="text/x-sage">plot_sv(A)
</script></pre></p></li>
<li id="li-5782"><p id="p-8313">Form the singular value decomposition <span class="process-math">\(A=U\Sigma
V^T\)</span> and the matrix of coefficients <span class="process-math">\(\Gamma\)</span> so that <span class="process-math">\(A=U\Gamma\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-269"><script type="text/x-sage">
</script></pre></p></li>
<li id="li-5783"><p id="p-8314">We will now study a particular case, the second case which appears as the column of <span class="process-math">\(A\)</span> indexed by <code class="code-inline tex2jax_ignore">1</code>. There is a command <code class="code-inline tex2jax_ignore">display_column(A, k)</code> that provides a visual display of the <span class="process-math">\(k^{th}\)</span> column of a matrix <span class="process-math">\(A\text{.}\)</span>  Describe the justices' votes in the second case. <pre class="ptx-sagecell sagecell-sage" id="sage-270"><script type="text/x-sage">
</script></pre></p></li>
<li id="li-5784"><p id="p-8315">Also, display the first left singular vector <span class="process-math">\(\uvec_1\text{,}\)</span> the column of <span class="process-math">\(U\)</span> indexed by <span class="process-math">\(0\text{,}\)</span> and the column of <span class="process-math">\(\Gamma\)</span> holding the coefficients that express the second case as a linear combination of left singular vectors. <pre class="ptx-sagecell sagecell-sage" id="sage-271"><script type="text/x-sage">
</script></pre> What does this tell us about how the second case is constructed as a linear combination of left singular vectors?  What is the significance of the first left singular vector <span class="process-math">\(\uvec_1\text{?}\)</span></p></li>
<li id="li-5785"><p id="p-8316">Let's now study the <span class="process-math">\(48^{th}\)</span> case, which is represented by the column of <span class="process-math">\(A\)</span> indexed by <code class="code-inline tex2jax_ignore">47</code>.  Describe the voting pattern in this case. <pre class="ptx-sagecell sagecell-sage" id="sage-272"><script type="text/x-sage">
</script></pre></p></li>
<li id="li-5786"><p id="p-8317">Display the second left singular vector <span class="process-math">\(\uvec_2\)</span> and the vector of coefficients that express the <span class="process-math">\(48^{th}\)</span> case as a linear combination of left singular vectors. <pre class="ptx-sagecell sagecell-sage" id="sage-273"><script type="text/x-sage">
</script></pre> Describe how this case is constructed as a linear combination of singular vectors.  What is the significance of the second left singular vector <span class="process-math">\(\uvec_2\text{?}\)</span></p></li>
<li id="li-5787"><p id="p-8318">The data in <a href="" class="xref" data-knowl="./knowl/table-supreme-cases.html" title="Table 7.5.2: Number of cases by vote count">Table 7.5.2</a> describes the number of cases decided by each possible vote count. <figure class="table table-like" id="table-supreme-cases"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">7.5.2<span class="period">.</span></span><span class="space"> </span>Number of cases by vote count</figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="c m b1 r0 l0 t0 lines">Vote count</td>
<td class="c m b1 r0 l0 t0 lines"># of cases</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">9-0</td>
<td class="c m b0 r0 l0 t0 lines">405</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">8-1</td>
<td class="c m b0 r0 l0 t0 lines">89</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">7-2</td>
<td class="c m b0 r0 l0 t0 lines">111</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">6-3</td>
<td class="c m b0 r0 l0 t0 lines">118</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">5-4</td>
<td class="c m b0 r0 l0 t0 lines">188</td>
</tr>
</table></div></figure> How do the singular vectors <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\)</span> reflect this data?  Would you characterize the court as leaning toward the conservatives or progressives?  Use these singular vectors to explain your response.</p></li>
<li id="li-5788"><p id="p-8319">Cases decided by a 5-4 vote are often the most impactful as they represent a sharp divide among the justices and, often, society at large.  For that reason, we will now focus on the 5-4 decisions. Evaluating the next cell forms the <span class="process-math">\(9\times188\)</span> matrix <span class="process-math">\(B\)</span> consisting of 5-4 decisions. <pre class="ptx-sagecell sagecell-sage" id="sage-274"><script type="text/x-sage">B = matrix(RDF, fivefour.values)
display_matrix(B.matrix_from_columns(range(50)))
</script></pre> Form the singular value decomposition of <span class="process-math">\(B=U\Sigma
V^T\)</span> along with the matrix <span class="process-math">\(\Gamma\)</span> of coefficients so that <span class="process-math">\(B=U\Gamma\)</span> and display the first left singular vector <span class="process-math">\(\uvec_1\text{.}\)</span>  Study how the <span class="process-math">\(7^{th}\)</span> case, indexed by <code class="code-inline tex2jax_ignore">6</code>, is constructed as a linear combination of left singular vectors. <pre class="ptx-sagecell sagecell-sage" id="sage-275"><script type="text/x-sage">
</script></pre> What does this singular vector tell us about the make up of the court and whether it leans towards the conservatives or progressives?</p></li>
<li id="li-5789"><p id="p-8320">Display the second left singular vector <span class="process-math">\(\uvec_2\)</span> and study how the <span class="process-math">\(6^{th}\)</span> case, indexed by <code class="code-inline tex2jax_ignore">5</code>, is constructed as a linear combination of left singular vectors. <pre class="ptx-sagecell sagecell-sage" id="sage-276"><script type="text/x-sage">
</script></pre> What does <span class="process-math">\(\uvec_2\)</span> tell us about the relative importance of the justices' voting records?</p></li>
<li id="li-5790"><p id="p-8321">By a <em class="emphasis">swing vote</em>, we mean a justice who is less inclined to vote with a particular bloc of justices but instead swings from one bloc to another with the potential to sway close decisions. What do the singular vectors <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\)</span> tell us about the presence of voting blocs on the court and the presence of a swing vote? Which justice represents the swing vote?</p></li>
</ol></article></section><section class="subsection" id="subsection-118"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">7.5.6</span> <span class="title">Summary</span>
</h3>
<p id="p-8344">This section has demonstrated some uses of the singular value decomposition.  Because the singular values appear in decreasing order, the decomposition has the effect of concentrating the most important features of the matrix into the first singular values and singular vectors.</p>
<ul class="disc">
<li id="li-5811"><p id="p-8345">Because the first left singular vectors form an orthonormal basis for <span class="process-math">\(\col(A)\text{,}\)</span> a singular value decomposition provides a convenient way to project vectors onto <span class="process-math">\(\col(A)\)</span> and therefore to solve least squares problems.</p></li>
<li id="li-5812">
<p id="p-8346">A singular value decomposition of a rank <span class="process-math">\(r\)</span> matrix <span class="process-math">\(A\)</span> leads to a series of approximations <span class="process-math">\(A_k\)</span> of <span class="process-math">\(A\)</span> where</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-38">
\begin{align*}
A \approx A_1 \amp = \sigma_1\uvec_1\vvec_1^T\\
A \approx A_2 \amp = \sigma_1\uvec_1\vvec_1^T +
\sigma_2\uvec_2\vvec_2^T\\
A \approx A_3 \amp = \sigma_1\uvec_1\vvec_1^T +
\sigma_2\uvec_2\vvec_2^T + 
\sigma_3\uvec_3\vvec_3^T\\
\vdots \amp\\
A = A_r \amp = \sigma_1\uvec_1\vvec_1^T +
\sigma_2\uvec_2\vvec_2^T +
\sigma_3\uvec_3\vvec_3^T +
\ldots +
\sigma_r\uvec_r\vvec_r^T
\end{align*}
</div>
<p class="continuation">In each case, <span class="process-math">\(A_k\)</span> is the rank <span class="process-math">\(k\)</span> matrix that is closest to <span class="process-math">\(A\text{.}\)</span></p>
</li>
<li id="li-5813"><p id="p-8347">If <span class="process-math">\(A\)</span> is a demeaned data matrix, the left singular vectors give the principal components of <span class="process-math">\(A\)</span> and the variance in the direction of a principal component can be simply expressed in terms of the corresponding singular value.</p></li>
<li id="li-5814"><p id="p-8348">The singular value decomposition has many applications. In this section, we looked at how the decomposition is used in image processing through the techniques of compression and denoising.</p></li>
<li id="li-5815"><p id="p-8349">Because the first few left singular vectors contain the most important features of a matrix, we can use a singular value decomposition to extract meaning from a large dataset as we did when analyzing the voting patterns of the second Rehnquist court.</p></li>
</ul></section><section class="exercises" id="exercises-31"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">7.5.7</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-286"><h4 class="heading"><span class="codenumber">1<span class="period">.</span></span></h4>
<p id="p-8350">Suppose that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = \begin{bmatrix}
2.1 \amp -1.9 \amp 0.1 \amp 3.7 \\
-1.5 \amp 2.7 \amp 0.9 \amp -0.6 \\
-0.4 \amp 2.8 \amp -1.5 \amp 4.2 \\
-0.4 \amp 2.4 \amp 1.9 \amp -1.8
\end{bmatrix}.
\end{equation*}
</div>
<p class="continuation"><pre class="ptx-sagecell sagecell-sage" id="sage-277"><script type="text/x-sage">
</script></pre></p>
<ol class="lower-alpha">
<li id="li-5816"><p id="p-8351">Find the singular values of <span class="process-math">\(A\text{.}\)</span>  What is <span class="process-math">\(\rank(A)\text{?}\)</span></p></li>
<li id="li-5817"><p id="p-8352">Find the sequence of matrices <span class="process-math">\(A_1\text{,}\)</span> <span class="process-math">\(A_2\text{,}\)</span> <span class="process-math">\(A_3\text{,}\)</span> and <span class="process-math">\(A_4\)</span> where <span class="process-math">\(A_k\)</span> is the rank <span class="process-math">\(k\)</span> approximation of <span class="process-math">\(A\text{.}\)</span></p></li>
</ol></article><article class="exercise exercise-like" id="exercise-287"><h4 class="heading"><span class="codenumber">2<span class="period">.</span></span></h4>
<p id="p-8361">Suppose we would like to find the best quadratic function</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\beta_0 + \beta_1x + \beta_2x^2=y
\end{equation*}
</div>
<p class="continuation">fitting the points</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
(0,1), (1,0), (2,1.5), (3,4), (4,8).
\end{equation*}
</div>
<p class="continuation"><pre class="ptx-sagecell sagecell-sage" id="sage-278"><script type="text/x-sage">
</script></pre></p>
<ol class="lower-alpha">
<li id="li-5822"><p id="p-8362">Set up a linear system <span class="process-math">\(A\xvec = \bvec\)</span> describing the coefficients <span class="process-math">\(\xvec =
\threevec{\beta_0}{\beta_1}{\beta_2}\text{.}\)</span></p></li>
<li id="li-5823"><p id="p-8363">Find the singular value decomposition of <span class="process-math">\(A\text{.}\)</span></p></li>
<li id="li-5824"><p id="p-8364">Use the singular value decomposition to find the least squares approximate solution <span class="process-math">\(\xhat\text{.}\)</span></p></li>
</ol></article><article class="exercise exercise-like" id="exercise-288"><h4 class="heading"><span class="codenumber">3<span class="period">.</span></span></h4>
<p id="p-8373">Remember that the outer product of two vector <span class="process-math">\(\uvec\)</span> and <span class="process-math">\(\vvec\)</span> is the matrix <span class="process-math">\(\uvec~\vvec^T\text{.}\)</span></p>
<ol class="lower-alpha">
<li id="li-5831">
<p id="p-8374">Suppose that <span class="process-math">\(\uvec = \twovec{2}{-3}\)</span> and <span class="process-math">\(\vvec=\threevec201\text{.}\)</span>  Evaluate the outer product <span class="process-math">\(\uvec~\vvec^T\text{.}\)</span>  To get a clearer sense of how this works, perform this operation without using technology.</p>
<p id="p-8375">How is each of the columns of <span class="process-math">\(\uvec~\vvec^T\)</span> related to <span class="process-math">\(\uvec\text{?}\)</span></p>
</li>
<li id="li-5832"><p id="p-8376">Suppose <span class="process-math">\(\uvec\)</span> and <span class="process-math">\(\vvec\)</span> are general vectors.  What is <span class="process-math">\(\rank(\uvec~\vvec^T)\)</span> and what is a basis for its column space <span class="process-math">\(\col(\uvec~\vvec^T)\text{?}\)</span></p></li>
<li id="li-5833"><p id="p-8377">Suppose that <span class="process-math">\(\uvec\)</span> is a unit vector.  What is the effect of multiplying a vector by the matrix <span class="process-math">\(\uvec~\uvec^T\text{?}\)</span></p></li>
</ol></article><article class="exercise exercise-like" id="exercise-289"><h4 class="heading"><span class="codenumber">4<span class="period">.</span></span></h4>
<p id="p-8386">Evaluating the following cell loads in a dataset recording some features of 1057 houses.  Notice how the lot size varies over a relatively small range compared to the other features. For this reason, in addition to demeaning the data, we'll scale each feature by dividing by its standard deviation so that the range of values is similar for each feature.  The matrix <span class="process-math">\(A\)</span> holds the result. <pre class="ptx-sagecell sagecell-sage" id="sage-279"><script type="text/x-sage">import pandas as pd
df = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/housing.csv', index_col=0)
df = df.fillna(df.mean())
std = (df-df.mean())/df.std()
A = matrix(std.values).T
df.T
</script></pre></p>
<ol class="lower-alpha">
<li id="li-5840"><p id="p-8387">Find the singular values of <span class="process-math">\(A\)</span> and use them to determine the variance in the direction of the principal components. <pre class="ptx-sagecell sagecell-sage" id="sage-280"><script type="text/x-sage">
</script></pre></p></li>
<li id="li-5841"><p id="p-8388">For what fraction of the variance do the first two principal components account?</p></li>
<li id="li-5842"><p id="p-8389">Find a singular value decomposition of <span class="process-math">\(A\)</span> and construct the <span class="process-math">\(2\times1057\)</span> matrix <span class="process-math">\(B\)</span> whose entries are the coordinates of the demeaned data points projected on to the two-dimensional subspace spanned by the first two principal components.  You can plot the projected data points using <code class="code-inline tex2jax_ignore">list_plot(B.columns())</code>. <pre class="ptx-sagecell sagecell-sage" id="sage-281"><script type="text/x-sage">
</script></pre></p></li>
<li id="li-5843"><p id="p-8390">Study the entries in the first two principal components <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\text{.}\)</span>  Would a more expensive house lie on the left, right, top, or bottom of the plot you constructed?</p></li>
<li id="li-5844"><p id="p-8391">In what ways does a house that lies on the far left of the plot you constructed differ from an average house? In what ways does a house that lies near the top of the plot you constructed differ from an average house?</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-290"><h4 class="heading"><span class="codenumber">5<span class="period">.</span></span></h4>
<p id="p-8405">Let's revisit the voting records of justices on the second Rehnquist court.  Evaluating the following cell will load the voting records of the justices in the 188 cases decided by a 5-4 vote and store them in the matrix <span class="process-math">\(A\text{.}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-282"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/svd_supreme.py', globals())
A = matrix(RDF, fivefour.values)
v = vector(188*[1])
fivefour
</script></pre></p>
<ol class="lower-alpha">
<li id="li-5855"><p id="p-8406">The cell above also defined the 188-dimensional vector <span class="process-math">\(\vvec\)</span> whose entries are all 1.  What does the product <span class="process-math">\(A\vvec\)</span> represent?  Use the following cell to evaluate this product. <pre class="ptx-sagecell sagecell-sage" id="sage-283"><script type="text/x-sage">
</script></pre></p></li>
<li id="li-5856"><p id="p-8407">How does the product <span class="process-math">\(A\vvec\)</span> tell us which justice voted in the majority most frequently?  What does this say about the presence of a swing vote on the court?</p></li>
<li id="li-5857"><p id="p-8408">How does this product tell us whether we should characterize this court as leaning conservative or progressive?</p></li>
<li id="li-5858"><p id="p-8409">How does this product tell us about the presence of a second swing vote on the court?</p></li>
<li id="li-5859"><p id="p-8410">Study the left singular vector <span class="process-math">\(\uvec_3\)</span> and describe how it reinforces the fact that there was a second swing vote.  Who was this second swing vote?</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-291"><h4 class="heading"><span class="codenumber">6<span class="period">.</span></span></h4>
<p id="p-8423">The following cell loads a dataset that describes the percentages with which justices on the second Rehnquist court agreed with one another.  For instance, the entry in the first row and second column is 72.78, which means that Justices Rehnquist and Scalia agreed with each other in 72.78% of the cases. <pre class="ptx-sagecell sagecell-sage" id="sage-284"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/svd_supreme.py', globals())
A = 1/100*matrix(RDF, agreement.values)
agreement
</script></pre></p>
<ol class="lower-alpha">
<li id="li-5870"><p id="p-8424">Examine the matrix <span class="process-math">\(A\text{.}\)</span>  What special structure does this matrix have and why should we expect it to have this structure?</p></li>
<li id="li-5871"><p id="p-8425">Plot the singular values of <span class="process-math">\(A\)</span> below.  For what value of <span class="process-math">\(k\)</span> would the approximation <span class="process-math">\(A_k\)</span> be a reasonable approximation of <span class="process-math">\(A\text{?}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-285"><script type="text/x-sage">plot_sv(A)
</script></pre></p></li>
<li id="li-5872"><p id="p-8426">Find a singular value decomposition <span class="process-math">\(A=U\Sigma V^T\)</span> and examine the matrices <span class="process-math">\(U\)</span> and <span class="process-math">\(V\)</span> using, for instance, <code class="code-inline tex2jax_ignore">n(U, 3)</code>.  What do you notice about the relationship between <span class="process-math">\(U\)</span> and <span class="process-math">\(V\)</span> and why should we expect this relationship to hold? <pre class="ptx-sagecell sagecell-sage" id="sage-286"><script type="text/x-sage">
</script></pre></p></li>
<li id="li-5873"><p id="p-8427">The command <code class="code-inline tex2jax_ignore">approximate(A, k)</code> will form the approximating matrix <span class="process-math">\(A_k\text{.}\)</span>  Study the matrix <span class="process-math">\(A_1\)</span> using the <code class="code-inline tex2jax_ignore">display_matrix</code> command. Which justice or justices seem to be most agreeable, that is, most likely to agree with other justices? Which justice is least agreeable? <pre class="ptx-sagecell sagecell-sage" id="sage-287"><script type="text/x-sage">
</script></pre></p></li>
<li id="li-5874"><p id="p-8428">Examine the difference <span class="process-math">\(A_2-A_1\)</span> and describe how this tells us about the presence of voting blocs and swing votes on the court. <pre class="ptx-sagecell sagecell-sage" id="sage-288"><script type="text/x-sage">
</script></pre></p></li>
</ol></article><article class="exercise exercise-like" id="exercise-292"><h4 class="heading"><span class="codenumber">7<span class="period">.</span></span></h4>
<p id="p-8441">Suppose that <span class="process-math">\(A=U_r\Sigma_rV_r^T\)</span> is a reduced singular value decomposition of the <span class="process-math">\(m\times n\)</span> matrix <span class="process-math">\(A\text{.}\)</span> The matrix <span class="process-math">\(A^+ = V_r\Sigma_r^{-1}U_r^T\)</span> is called the <em class="emphasis">Moore-Penrose inverse</em> of <span class="process-math">\(A\text{.}\)</span></p>
<ol class="lower-alpha">
<li id="li-5885"><p id="p-8442">Explain why <span class="process-math">\(A^+\)</span> is an <span class="process-math">\(n\times m\)</span> matrix.</p></li>
<li id="li-5886"><p id="p-8443">If <span class="process-math">\(A\)</span> is an invertible, square matrix, explain why <span class="process-math">\(A^+=A^{-1}\text{.}\)</span></p></li>
<li id="li-5887"><p id="p-8444">Explain why <span class="process-math">\(AA^+\bvec=\bhat\text{,}\)</span> the orthogonal projection of <span class="process-math">\(\bvec\)</span> onto <span class="process-math">\(\col(A)\text{.}\)</span></p></li>
<li id="li-5888"><p id="p-8445">Explain why <span class="process-math">\(A^+A\xvec=\xhat\text{,}\)</span> the orthogonal projection of <span class="process-math">\(\xvec\)</span> onto <span class="process-math">\(\col(A^T)\text{.}\)</span></p></li>
</ol></article><article class="exercise exercise-like" id="exercise-293"><h4 class="heading"><span class="codenumber">8<span class="period">.</span></span></h4>
<p id="p-8456">In <a href="sec-gaussian-revisited.html#subsec-partial-pivot" class="internal" title="Subsection 5.1.1: Partial pivoting">Subsection 5.1.1</a>, we saw how some linear algebraic computations are sensitive to round off error made by a computer.  A singular value decomposition can help us understand when this situation can occur.</p>
<p id="p-8457">For instance, consider the matrices</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
A = \begin{bmatrix}
1.0001 \amp 1 \\
1 \amp 1 \\
\end{bmatrix},\hspace{24pt}
B = \begin{bmatrix}
1 \amp 1 \\
1 \amp 1 \\
\end{bmatrix}.
\end{equation*}
</div>
<p class="continuation">The entries in these matrices are quite close to one another, but <span class="process-math">\(A\)</span> is invertible while <span class="process-math">\(B\)</span> is not.  It seems like <span class="process-math">\(A\)</span> is <em class="emphasis">almost</em> singular. In fact, we can measure how close a matrix is to being singular by forming the <em class="emphasis">condition number</em>, <span class="process-math">\(\sigma_1/\sigma_n\text{,}\)</span> the ratio of the largest to smallest singular value.  If <span class="process-math">\(A\)</span> were singular, the condition number would be undefined because the singular value <span class="process-math">\(\sigma_n=0\text{.}\)</span>  Therefore, we will think of matrices with large condition numbers as being close to singular.</p>
<ol class="lower-alpha">
<li id="li-5897"><p id="p-8458">Define the matrix <span class="process-math">\(A\)</span> and find a singular value decomposition.  What is the condition number of <span class="process-math">\(A\text{?}\)</span> <pre class="ptx-sagecell sagecell-sage" id="sage-289"><script type="text/x-sage">
</script></pre></p></li>
<li id="li-5898">
<p id="p-8459">Define the left singular vectors <span class="process-math">\(\uvec_1\)</span> and <span class="process-math">\(\uvec_2\text{.}\)</span>  Compare the results <span class="process-math">\(A^{-1}\bvec\)</span> when</p>
<ol class="decimal">
<li id="li-5899"><p id="p-8460"><span class="process-math">\(\bvec=\uvec_1+\uvec_2\text{.}\)</span></p></li>
<li id="li-5900"><p id="p-8461"><span class="process-math">\(\bvec=2\uvec_1+\uvec_2\text{.}\)</span></p></li>
</ol>
<p class="continuation">Notice how a small change in the vector <span class="process-math">\(\bvec\)</span> leads to a small change in <span class="process-math">\(A^{-1}\bvec\text{.}\)</span></p>
</li>
<li id="li-5901">
<p id="p-8462">Now compare the results <span class="process-math">\(A^{-1}\bvec\)</span> when</p>
<ol class="decimal">
<li id="li-5902"><p id="p-8463"><span class="process-math">\(\bvec=\uvec_1+\uvec_2\text{.}\)</span></p></li>
<li id="li-5903"><p id="p-8464"><span class="process-math">\(\bvec=\uvec_1+2\uvec_2\text{.}\)</span></p></li>
</ol>
<p class="continuation">Notice now how a small change in <span class="process-math">\(\bvec\)</span> leads to a large change in <span class="process-math">\(A^{-1}\bvec\text{.}\)</span></p>
</li>
<li id="li-5904">
<p id="p-8465">Previously, we saw that, if we write <span class="process-math">\(\xvec\)</span> in terms of left singular vectors <span class="process-math">\(\xvec=c_1\vvec_1+c_2\vvec_2\text{,}\)</span> then we have</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\bvec=A\xvec = c_1\sigma_1\uvec_1 +
c_2\sigma_2\uvec_2.
\end{equation*}
</div>
<p class="continuation">If we write <span class="process-math">\(\bvec=d_1\uvec_1+d_2\uvec_2\text{,}\)</span> explain why <span class="process-math">\(A^{-1}\bvec\)</span> is sensitive to small changes in <span class="process-math">\(d_2\text{.}\)</span></p>
</li>
</ol>
<p class="continuation">Generally speaking, a square matrix <span class="process-math">\(A\)</span> with a large condition number will demonstrate this type of behavior so that the computation of <span class="process-math">\(A^{-1}\)</span> is likely to be affected by round off error.  We call such a matrix <em class="emphasis">ill-conditioned</em>.</p></article></section></section><div class="hidden-content tex2jax_ignore" id="hk-fn-7"><div class="fn"><code class="code-inline tex2jax_ignore">timbaumann.info/projects.html</code></div></div>
<div class="hidden-content tex2jax_ignore" id="hk-fn-8"><div class="fn"><code class="code-inline tex2jax_ignore">gvsu.edu/s/21F</code></div></div>
</div></main>
</div>
</body>
</html>
