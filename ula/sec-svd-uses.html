<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2021-08-15T11:46:43-04:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Using Singular Value Decompositions</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<script src="https://sagecell.sagemath.org/embedded_sagecell.js"></script><script>window.MathJax = {
  tex: {
    inlineMath: [['\\(','\\)']],
    tags: "none",
    useLabelIds: true,
    tagSide: "right",
    tagIndent: ".8em",
    packages: {'[+]': ['base', 'extpfeil', 'ams', 'amscd', 'newcommand', 'knowl']}
  },
  options: {
    ignoreHtmlClass: "tex2jax_ignore",
    processHtmlClass: "has_am",
    renderActions: {
        findScript: [10, function (doc) {
            document.querySelectorAll('script[type^="math/tex"]').forEach(function(node) {
                var display = !!node.type.match(/; *mode=display/);
                var math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                var text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = {node: text, delim: '', n: 0};
                math.end = {node: text, delim: '', n: 0};
                doc.math.push(math);
            });
        }, '']
    },
  },
  chtml: {
    scale: 0.88,
    mtextInheritFont: true
  },
  loader: {
    load: ['input/asciimath', '[tex]/extpfeil', '[tex]/amscd', '[tex]/newcommand', '[pretext]/mathjaxknowl3.js'],
    paths: {pretext: "https://pretextbook.org/js/lib"},
  },
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><script>$(function () {
    // Make *any* div with class 'sagecell-sage' an executable Sage cell
    // Their results will be linked, only within language type
    sagecell.makeSagecell({inputLocation: 'div.sagecell-sage',
                           linked: true,
                           languages: ['sage'],
                           evalButtonText: 'Evaluate (Sage)'});
});
</script><script async="" src="https://cse.google.com/cse.js?cx=015103900096539427448:ngwuia10qci"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.min.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/0.13/pretext.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/0.13/pretext_add_on.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script xmlns:svg="http://www.w3.org/2000/svg">sagecellEvalName='Evaluate (Sage)';
</script><link xmlns:svg="http://www.w3.org/2000/svg" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/pretext.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/pretext_add_on.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/banner_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/toc_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/knowls_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/style_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/colors_blue_grey.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/setcolors.css" rel="stylesheet" type="text/css">
<!-- 2019-10-12: Temporary - CSS file for experiments with styling --><link xmlns:svg="http://www.w3.org/2000/svg" href="developer.css" rel="stylesheet" type="text/css">
</head>
<body class="mathbook-book has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div xmlns:svg="http://www.w3.org/2000/svg" id="latex-macros" class="hidden-content" style="display:none">\(\newcommand{\avec}{{\mathbf a}}
\newcommand{\bvec}{{\mathbf b}}
\newcommand{\cvec}{{\mathbf c}}
\newcommand{\dvec}{{\mathbf d}}
\newcommand{\dtil}{\widetilde{\mathbf d}}
\newcommand{\evec}{{\mathbf e}}
\newcommand{\fvec}{{\mathbf f}}
\newcommand{\nvec}{{\mathbf n}}
\newcommand{\pvec}{{\mathbf p}}
\newcommand{\qvec}{{\mathbf q}}
\newcommand{\svec}{{\mathbf s}}
\newcommand{\tvec}{{\mathbf t}}
\newcommand{\uvec}{{\mathbf u}}
\newcommand{\vvec}{{\mathbf v}}
\newcommand{\wvec}{{\mathbf w}}
\newcommand{\xvec}{{\mathbf x}}
\newcommand{\yvec}{{\mathbf y}}
\newcommand{\zvec}{{\mathbf z}}
\newcommand{\rvec}{{\mathbf r}}
\newcommand{\mvec}{{\mathbf m}}
\newcommand{\zerovec}{{\mathbf 0}}
\newcommand{\onevec}{{\mathbf 1}}
\newcommand{\real}{{\mathbb R}}
\newcommand{\twovec}[2]{\left[\begin{array}{r}#1 \\ #2
\end{array}\right]}
\newcommand{\ctwovec}[2]{\left[\begin{array}{c}#1 \\ #2
\end{array}\right]}
\newcommand{\threevec}[3]{\left[\begin{array}{r}#1 \\ #2 \\ #3
\end{array}\right]}
\newcommand{\cthreevec}[3]{\left[\begin{array}{c}#1 \\ #2 \\ #3
\end{array}\right]}
\newcommand{\fourvec}[4]{\left[\begin{array}{r}#1 \\ #2 \\ #3 \\ #4
\end{array}\right]}
\newcommand{\cfourvec}[4]{\left[\begin{array}{c}#1 \\ #2 \\ #3 \\ #4
\end{array}\right]}
\newcommand{\fivevec}[5]{\left[\begin{array}{r}#1 \\ #2 \\ #3 \\
#4 \\ #5 \\ \end{array}\right]}
\newcommand{\cfivevec}[5]{\left[\begin{array}{c}#1 \\ #2 \\ #3 \\
#4 \\ #5 \\ \end{array}\right]}
\newcommand{\mattwo}[4]{\left[\begin{array}{rr}#1 \amp #2 \\ #3 \amp #4 \\ \end{array}\right]}
\newcommand{\laspan}[1]{\text{Span}\{#1\}}
\newcommand{\bcal}{{\cal B}}
\newcommand{\ccal}{{\cal C}}
\newcommand{\scal}{{\cal S}}
\newcommand{\wcal}{{\cal W}}
\newcommand{\ecal}{{\cal E}}
\newcommand{\coords}[2]{\left\{#1\right\}_{#2}}
\newcommand{\gray}[1]{\color{gray}{#1}}
\newcommand{\lgray}[1]{\color{lightgray}{#1}}
\newcommand{\rank}{\text{rank}}
\newcommand{\row}{\text{Row}}
\newcommand{\col}{\text{Col}}
\renewcommand{\row}{\text{Row}}
\newcommand{\nul}{\text{Nul}}
\newcommand{\var}{\text{Var}}
\newcommand{\corr}{\text{corr}}
\newcommand{\len}[1]{\left|#1\right|}
\newcommand{\bbar}{\overline{\bvec}}
\newcommand{\bhat}{\widehat{\bvec}}
\newcommand{\bperp}{\bvec^\perp}
\newcommand{\xhat}{\widehat{\xvec}}
\newcommand{\vhat}{\widehat{\vvec}}
\newcommand{\uhat}{\widehat{\uvec}}
\newcommand{\what}{\widehat{\wvec}}
\newcommand{\Sighat}{\widehat{\Sigma}}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="ula.html"><span class="title">Understanding Linear Algebra</span></a></h1>
<p class="byline">David Austin</p>
</div>
<div class="searchwrapper" role="search"><div class="gcse-search"></div></div>
</div></div>
<nav xmlns:svg="http://www.w3.org/2000/svg" id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3">
<a class="index-button toolbar-item button" href="index-1.html" title="Index">Index</a><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="sec-svd-intro.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="chap7.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="backmatter.html" title="Next">Next</a></span>
</div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="sec-svd-intro.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="chap7.html" title="Up">Up</a><a class="next-button button toolbar-item" href="backmatter.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div xmlns:svg="http://www.w3.org/2000/svg" id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link frontmatter">
<a href="frontmatter.html" data-scroll="frontmatter"><span class="title">Front Matter</span></a><ul>
<li><a href="dedication-1.html" data-scroll="dedication-1">Dedication</a></li>
<li><a href="colophon-1.html" data-scroll="colophon-1">Colophon</a></li>
<li><a href="preface-1.html" data-scroll="preface-1">Our goals</a></li>
</ul>
</li>
<li class="link">
<a href="chap1.html" data-scroll="chap1"><span class="codenumber">1</span> <span class="title">Systems of equations</span></a><ul>
<li><a href="sec-expect.html" data-scroll="sec-expect">What can we expect</a></li>
<li><a href="sec-finding-solutions.html" data-scroll="sec-finding-solutions">Finding solutions to systems of linear equations</a></li>
<li><a href="sec-sage-introduction.html" data-scroll="sec-sage-introduction">Computation with Sage</a></li>
<li><a href="sec-pivots.html" data-scroll="sec-pivots">Pivots and their influence on solution spaces</a></li>
</ul>
</li>
<li class="link">
<a href="chap2.html" data-scroll="chap2"><span class="codenumber">2</span> <span class="title">Vectors, matrices, and linear combinations</span></a><ul>
<li><a href="sec-vectors-lin-combs.html" data-scroll="sec-vectors-lin-combs">Vectors and linear combinations</a></li>
<li><a href="sec-matrices-lin-combs.html" data-scroll="sec-matrices-lin-combs">Matrix multiplication and linear combinations</a></li>
<li><a href="sec-span.html" data-scroll="sec-span">The span of a set of vectors</a></li>
<li><a href="sec-linear-dep.html" data-scroll="sec-linear-dep">Linear independence</a></li>
<li><a href="sec-linear-trans.html" data-scroll="sec-linear-trans">Matrix transformations</a></li>
<li><a href="sec-transforms-geom.html" data-scroll="sec-transforms-geom">The geometry of matrix transformations</a></li>
</ul>
</li>
<li class="link">
<a href="chap3.html" data-scroll="chap3"><span class="codenumber">3</span> <span class="title">Invertibility, bases, and coordinate systems</span></a><ul>
<li><a href="sec-matrix-inverse.html" data-scroll="sec-matrix-inverse">Invertibility</a></li>
<li><a href="sec-bases.html" data-scroll="sec-bases">Bases and coordinate systems</a></li>
<li><a href="sec-jpeg.html" data-scroll="sec-jpeg">Image compression</a></li>
<li><a href="sec-determinants.html" data-scroll="sec-determinants">Determinants</a></li>
<li><a href="sec-subspaces.html" data-scroll="sec-subspaces">Subspaces of \(\real^p\)</a></li>
</ul>
</li>
<li class="link">
<a href="chap4.html" data-scroll="chap4"><span class="codenumber">4</span> <span class="title">Eigenvalues and eigenvectors</span></a><ul>
<li><a href="sec-eigen-intro.html" data-scroll="sec-eigen-intro">An introduction to eigenvalues and eigenvectors</a></li>
<li><a href="sec-eigen-find.html" data-scroll="sec-eigen-find">Finding eigenvalues and eigenvectors</a></li>
<li><a href="sec-eigen-diag.html" data-scroll="sec-eigen-diag">Diagonalization, similarity, and powers of a matrix</a></li>
<li><a href="sec-dynamical.html" data-scroll="sec-dynamical">Dynamical systems</a></li>
<li><a href="sec-stochastic.html" data-scroll="sec-stochastic">Markov chains and Google's PageRank algorithm</a></li>
</ul>
</li>
<li class="link">
<a href="chap5.html" data-scroll="chap5"><span class="codenumber">5</span> <span class="title">Linear algebra and computing</span></a><ul>
<li><a href="sec-gaussian-revisited.html" data-scroll="sec-gaussian-revisited">Gaussian elimination revisited</a></li>
<li><a href="sec-power-method.html" data-scroll="sec-power-method">Finding eigenvectors numerically</a></li>
</ul>
</li>
<li class="link">
<a href="chap6.html" data-scroll="chap6"><span class="codenumber">6</span> <span class="title">Orthogonality and Least Squares</span></a><ul>
<li><a href="sec-dot-product.html" data-scroll="sec-dot-product">The dot product</a></li>
<li><a href="sec-transpose.html" data-scroll="sec-transpose">Orthogonal complements and the matrix tranpose</a></li>
<li><a href="sec-orthogonal-bases.html" data-scroll="sec-orthogonal-bases">Orthogonal bases and projections</a></li>
<li><a href="sec-gram-schmidt.html" data-scroll="sec-gram-schmidt">Finding orthogonal bases</a></li>
<li><a href="sec-least-squares.html" data-scroll="sec-least-squares">Orthogonal least squares</a></li>
</ul>
</li>
<li class="link">
<a href="chap7.html" data-scroll="chap7"><span class="codenumber">7</span> <span class="title">The Spectral Theorem and singular value decompositions</span></a><ul>
<li><a href="sec-symmetric-matrices.html" data-scroll="sec-symmetric-matrices">Symmetric matrices and variance</a></li>
<li><a href="sec-quadratic-forms.html" data-scroll="sec-quadratic-forms">Quadratic forms</a></li>
<li><a href="sec-pca.html" data-scroll="sec-pca">Principal Component Analysis</a></li>
<li><a href="sec-svd-intro.html" data-scroll="sec-svd-intro">Singular Value Decompositions</a></li>
<li><a href="sec-svd-uses.html" data-scroll="sec-svd-uses" class="active">Using Singular Value Decompositions</a></li>
</ul>
</li>
<li class="link backmatter"><a href="backmatter.html" data-scroll="backmatter"><span class="title">Back Matter</span></a></li>
<li class="link"><a href="index-1.html" data-scroll="index-1"><span class="title">Index</span></a></li>
</ul></nav><div class="extras"><nav><a class="mathbook-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content"><section xmlns:svg="http://www.w3.org/2000/svg" class="section" id="sec-svd-uses"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">7.5</span> <span class="title">Using Singular Value Decompositions</span>
</h2>
<section class="introduction" id="introduction-38"><p id="p-7847">We've now seen what singular value decompositions are, how to construct them, and how they provide important information about a matrix such as orthonormal bases for the four fundamental subspaces.  This puts us in a good position to begin using singular value decompositions to solve a wide variety of problems.</p>
<p id="p-7848">Given the fact that singular value decompositions so immediately convey fundamental data about a matrix, it seems natural that some of our previous work can be reinterpreted in terms of singular value decompositions.  Therefore, we'll take some time in this section to revisit some familiar issues, such as least squares problems and principal component analysis, while also looking at some new applications.</p>
<article class="exploration project-like" id="exploration-30"><h6 class="heading">
<span class="type">Preview Activity</span><span class="space"> </span><span class="codenumber">7.5.1</span><span class="period">.</span>
</h6>
<p id="p-7849">Suppose that \(A = U\Sigma V^T\) where</p>
<div class="displaymath">
\begin{equation*}
\Sigma = \begin{bmatrix}
13 \amp 0 \amp 0 \amp 0 \\
0 \amp 8 \amp 0 \amp 0 \\
0 \amp 0 \amp 2 \amp 0 \\
0 \amp0 \amp 0 \amp 0 \\
0 \amp0 \amp 0 \amp 0
\end{bmatrix},
\end{equation*}
</div>
<p class="continuation">vectors \(\uvec_j\) form the columns of \(U\text{,}\) and vectors \(\vvec_j\) form the columns of \(V\text{.}\)</p>
<ol class="lower-alpha">
<li id="li-5429"><p id="p-7850">What are the dimensions of the matrices \(A\text{,}\) \(U\text{,}\) and \(V\text{?}\)</p></li>
<li id="li-5430"><p id="p-7851">What is the rank of \(A\text{?}\)</p></li>
<li id="li-5431"><p id="p-7852">Describe how to find an orthonormal basis for \(\col(A)\text{.}\)</p></li>
<li id="li-5432"><p id="p-7853">Describe how to find an orthonormal basis for \(\nul(A)\text{.}\)</p></li>
<li id="li-5433"><p id="p-7854">If the columns of \(Q\) form an orthonormal basis for \(\col(A)\text{,}\) what is \(Q^TQ\text{?}\)</p></li>
<li id="li-5434"><p id="p-7855">How would you form a matrix that projects vectors orthogonally onto \(\col(A)\text{?}\)</p></li>
</ol></article></section><section class="subsection" id="subsection-112"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">7.5.1</span> <span class="title">Least squares problems</span>
</h3>
<p id="p-7856">Least squares problems, which we explored in <a href="sec-least-squares.html" class="internal" title="Section 6.5: Orthogonal least squares">Section 6.5</a>, arise when we are confronted with an inconsistent linear system \(A\xvec=\bvec\text{.}\)  Since there is no solution to the system, we instead find the vector \(\xvec\) minimizing the distance between \(\bvec\) and \(A\xvec\text{.}\) That is, we find the vector \(\xhat\text{,}\) the least squares approximate solution, by solving \(A\xhat=\bhat\) where \(\bhat\) is the orthogonal projection of \(\bvec\) onto the column space of \(A\text{.}\)</p>
<p id="p-7857">If we have a singular value decomposition \(A=U\Sigma V^T\text{,}\) then the number of nonzero singular values \(r\) tells us the rank of \(A\text{,}\) and the first \(r\) columns of \(U\) form an orthonormal basis for \(\col(A)\text{.}\)  This basis may be used to project vectors onto \(\col(A)\) and hence to solve least squares problems.</p>
<p id="p-7858">Before exploring this connection further, we will introduce Sage as a tool for automating the construction of singular value decompositions.  One new feature is that we need to declare our matrix to consist of floating point entries.  We do this by including <code class="code-inline tex2jax_ignore">RDF</code> inside the matrix definition, as illustrated in the following cell. <div class="sagecell-sage" id="sage-238"><script type="text/x-sage">A = matrix(RDF, 3, 2, [1,0,-1,1,1,1])
U, Sigma, V = A.SVD()	  
print(U)
print('---------')
print(Sigma)
print('---------')
print(V)
</script></div></p>
<article class="activity project-like" id="activity-104"><h6 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.5.2</span><span class="period">.</span>
</h6>
<p id="p-7859">Consider the equation \(A\xvec=\bvec\) where</p>
<div class="displaymath">
\begin{equation*}
\begin{bmatrix}
1 \amp 0 \\
1 \amp 1 \\
1 \amp 2
\end{bmatrix}
\xvec = \threevec{-1}36
\end{equation*}
</div>
<ol class="lower-alpha">
<li id="li-5435"><p id="p-7860">Find a singular value decomposition for \(A\) using the Sage cell below.  What are singular values of \(A\text{?}\) <div class="sagecell-sage" id="sage-239"><script type="text/x-sage">
</script></div></p></li>
<li id="li-5436"><p id="p-7861">What is \(r\text{,}\) the rank of \(A\text{?}\)  How can we identify an orthonormal basis for \(\col(A)\text{?}\)</p></li>
<li id="li-5437">
<p id="p-7862">Form the reduced singular value decomposition \(U_r\Sigma_rV_r^T\) by constructing the matrix \(U_r\text{,}\) consisting of the first \(r\) columns of \(U\text{,}\) the matrix \(V_r\text{,}\) consisting of the first \(r\) columns of \(V\text{,}\) and \(\Sigma_r\text{,}\) an \(r\times r\) diagonal matrix. Verify that \(A=U_r\Sigma_r V_r^T\text{.}\)</p>
<p id="p-7863">You may find it convenient to remember that, if <code class="code-inline tex2jax_ignore">B</code> is a matrix defined in Sage, then <code class="code-inline tex2jax_ignore">B.matrix_from_columns( list )</code> and <code class="code-inline tex2jax_ignore">B.matrix_from_rows( list )</code> can be used to extract columns or rows from <code class="code-inline tex2jax_ignore">B</code>.  For instance, <code class="code-inline tex2jax_ignore">B.matrix_from_rows([0,1,2])</code> provides a matrix formed from the first three rows of <code class="code-inline tex2jax_ignore">B</code>. <div class="sagecell-sage" id="sage-240"><script type="text/x-sage">
</script></div></p>
</li>
<li id="li-5438"><p id="p-7864">How does the reduced singular value decomposition provide a matrix whose columns are an orthonormal basis for \(\col(A)\text{?}\)</p></li>
<li id="li-5439">
<p id="p-7865">Explain why a least squares approximate solution \(\xhat\) satisfies</p>
<div class="displaymath">
\begin{equation*}
A\xhat = U_rU_r^T\bvec.
\end{equation*}
</div>
</li>
<li id="li-5440"><p id="p-7866">What is the product \(V_r^TV_r\) and why does it have this form?</p></li>
<li id="li-5441">
<p id="p-7867">Explain why</p>
<div class="displaymath">
\begin{equation*}
\xhat = V_r\Sigma_r^{-1}U_r^T\bvec
\end{equation*}
</div>
<p class="continuation">is a least squares approximate solution by simplifying \(A\xhat = (U_r\Sigma_r V_r^T)(V_r\Sigma_r^{-1}U_r^T
\bvec)\text{.}\)</p>
<p id="p-7868">Now use this expression to find \(\xhat\text{.}\) <div class="sagecell-sage" id="sage-241"><script type="text/x-sage">
</script></div></p>
</li>
</ol></article><p id="p-7885">This activity demonstrates the power of a singular value decomposition to find a least squares approximate solution for an equation \(A\xvec = \bvec\text{.}\)  Because it immediately provides an orthonormal basis for \(\col(A)\text{,}\) something that we've had to construct by the Gram-Schmidt process in the past, we can easily project \(\bvec\) onto \(\col(A)\text{,}\) which results in a simple expression for \(\xhat\text{.}\)</p>
<article class="proposition theorem-like" id="prop-svd-ols"><h6 class="heading">
<span class="type">Proposition</span><span class="space"> </span><span class="codenumber">7.5.1</span><span class="period">.</span>
</h6>
<p id="p-7886">If \(A=U_r\Sigma_r V_r^T\) is a reduced singular value decomposition of \(A\text{,}\) then a least squares approximate solution to \(A\xvec=\bvec\) is given by</p>
<div class="displaymath">
\begin{equation*}
\xhat = V_r\Sigma_r^{-1}U_r^T\bvec.
\end{equation*}
</div></article><p id="p-7887">If the columns of \(A\) are linearly independent, then the equation \(A\xhat = \bhat\) has only one solution so there is a unique least squares approximate solution \(\xhat\text{.}\) Otherwise, the expression in <a class="xref" data-knowl="./knowl/prop-svd-ols.html" title="Proposition 7.5.1">Proposition 7.5.1</a> produces the solution to \(A\xhat=\bhat\) having the shortest length.</p>
<p id="p-7888">The matrix \(A^+ = V_r\Sigma_r^{-1}U_r^T\) is known as the <em class="emphasis">Moore-Penrose psuedoinverse</em> of \(A\text{.}\)  When \(A\) is invertible, \(A^{-1} = A^+\text{.}\)</p></section><section class="subsection" id="subsection-113"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">7.5.2</span> <span class="title">Rank \(k\) approximations</span>
</h3>
<p id="p-7889">If we have a singular value decomposition for a matrix \(A\text{,}\) we can form a sequence of matrices \(A_k\) that approximate \(A\) with increasing accuracy.  This may feel familiar to calculus students who have seen the way in which a function \(f(x)\) can be approximated by a linear function, a quadratic function, and so forth with increasing accuracy.</p>
<p id="p-7890">We'll begin with a singular value decomposition of a rank \(r\) matrix \(A\) so that \(A=U\Sigma V^T\text{.}\)  To create the approximating matrix \(A_k\text{,}\) we keep the first \(k\) singular values and set the others to zero.  For instance, if \(\Sigma = \begin{bmatrix}
22 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 14 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 3 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \\
\end{bmatrix}
\text{,}\) we can form matrices</p>
<div class="displaymath">
\begin{equation*}
\Sigma^{(1)} = \begin{bmatrix}
22 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \\
\end{bmatrix}, \hspace{24pt}
\Sigma^{(2)} = \begin{bmatrix}
22 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 14 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \\
0 \amp 0 \amp 0 \amp 0 \amp 0 \\
\end{bmatrix}
\end{equation*}
</div>
<p class="continuation">and define \(A_1 = U\Sigma^{(1)}V^T\) and \(A_2 =
U\Sigma^{(2)}V^T\text{.}\)  Because \(A_k\) has \(k\) nonzero singular values, we know that \(\rank(A_k) = k\text{.}\)  In fact, there is a sense in which \(A_k\) is the closest matrix to \(A\) among all rank \(k\) matrices.</p>
<article class="activity project-like" id="activity-105"><h6 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.5.3</span><span class="period">.</span>
</h6>
<p id="p-7891">Let's consider a matrix \(A=U\Sigma V^T\) where</p>
<div class="displaymath">
\begin{align*}
\amp U = \begin{bmatrix}
\frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \\
\frac{1}{2} \amp \frac{1}{2} \amp -\frac{1}{2} \amp -\frac{1}{2} \\
\frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2} \amp -\frac{1}{2} \\
\frac{1}{2} \amp -\frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2} \\
\end{bmatrix},\hspace{10pt}
\Sigma = \begin{bmatrix}
500 \amp 0 \amp 0 \amp 0 \\
0 \amp 100 \amp 0 \amp 0 \\
0 \amp 0 \amp 20 \amp 0  \\
0 \amp 0 \amp 0 \amp 4
\end{bmatrix}\\
\amp V = \begin{bmatrix}
\frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \\
\frac{1}{2} \amp -\frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2} \\
-\frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2} \amp \frac{1}{2} \\
-\frac{1}{2} \amp \frac{1}{2} \amp -\frac{1}{2} \amp \frac{1}{2}
\end{bmatrix}
\end{align*}
</div>
<p class="continuation">Evaluating the following cell will create the matrices <code class="code-inline tex2jax_ignore">U</code>, <code class="code-inline tex2jax_ignore">V</code>, and <code class="code-inline tex2jax_ignore">Sigma</code>.  Notice how the <code class="code-inline tex2jax_ignore">diagonal_matrix</code> command provides a convenient way to form the diagonal matrix \(\Sigma\text{.}\) <div class="sagecell-sage" id="sage-242"><script type="text/x-sage">h = 1/2
U = matrix(4,4,[h,h,h,h,  h,h,-h,-h,  h,-h,h,-h,  h,-h,-h,h])	    
V = matrix(4,4,[h,h,h,h,  h,-h,-h,h,  -h,-h,h,h,  -h,h,-h,h])
Sigma = diagonal_matrix([500, 100, 20, 4])
</script></div></p>
<ol class="lower-alpha">
<li id="li-5456"><p id="p-7892">Form the matrix \(A=U\Sigma V^T\text{.}\)  What is \(\rank(A)\text{?}\) <div class="sagecell-sage" id="sage-243"><script type="text/x-sage">
</script></div></p></li>
<li id="li-5457"><p id="p-7893">Now form the approximating matrix \(A_1=U\Sigma^{(1)}
V^T\text{.}\)  What is \(\rank(A_1)\text{?}\) <div class="sagecell-sage" id="sage-244"><script type="text/x-sage">
</script></div></p></li>
<li id="li-5458"><p id="p-7894">Find the error in the approximation \(A\approx
A_1\) by finding \(A-A_1\text{.}\)</p></li>
<li id="li-5459"><p id="p-7895">Now find \(A_2 = U\Sigma^{(2)} V^T\) and the error \(A-A_2\text{.}\)  What is \(\rank(A_2)\text{?}\) <div class="sagecell-sage" id="sage-245"><script type="text/x-sage">
</script></div></p></li>
<li id="li-5460"><p id="p-7896">Find \(A_3 = U\Sigma^{(3)} V^T\) and the error \(A-A_3\text{.}\)  What is \(\rank(A_3)\text{?}\)</p></li>
<li id="li-5461"><p id="p-7897">What would happen if we were to compute \(A_4\text{?}\)</p></li>
<li id="li-5462"><p id="p-7898">What do you notice about the error \(A-A_k\) as \(k\) increases?</p></li>
</ol></article><p id="p-7915">In this activity, the approximating matrix \(A_k\) has rank \(k\) because its singular value decomposition has \(k\) nonzero singular values.  We then saw how the difference between \(A\) and the approximations \(A_k\) decreases as \(k\) increases, which means that the sequence \(A_k\) forms better approximations as \(k\) increases.</p>
<p id="p-7916">Another way to represent \(A_k\) is with a reduced singular value decomposition so that \(A_k = U_k\Sigma_kV_k^T\) where</p>
<div class="displaymath">
\begin{equation*}
U_k = \begin{bmatrix}
\uvec_1 \amp \ldots \amp \uvec_k
\end{bmatrix},\hspace{10pt}
\Sigma_k = \begin{bmatrix}
\sigma_1 \amp 0 \amp \ldots \amp 0 \\
0 \amp \sigma_2 \amp \ldots \amp 0 \\
\vdots \amp \vdots \amp \ddots \amp \vdots \\
0 \amp 0 \amp \ldots \amp \sigma_k
\end{bmatrix},\hspace{10pt}
V_k = \begin{bmatrix}
\vvec_1 \amp \ldots \amp \vvec_k
\end{bmatrix}\text{.}
\end{equation*}
</div>
<p class="continuation">Notice that the rank \(1\) matrix \(A_1\) then has the form \(A_1 = \uvec_1\begin{bmatrix}\sigma_1\end{bmatrix}
\vvec_1^T = \sigma_1\uvec_1\vvec_1^T\) and that we can similarly write:</p>
<div class="displaymath">
\begin{align*}
A \approx A_1 \amp = \sigma_1\uvec_1\vvec_1^T\\
A \approx A_2 \amp = \sigma_1\uvec_1\vvec_1^T +
\sigma_2\uvec_2\vvec_2^T\\
A \approx A_3 \amp = \sigma_1\uvec_1\vvec_1^T +
\sigma_2\uvec_2\vvec_2^T + 
\sigma_3\uvec_3\vvec_3^T\\
\vdots \amp\\
A = A_r \amp = \sigma_1\uvec_1\vvec_1^T +
\sigma_2\uvec_2\vvec_2^T +
\sigma_3\uvec_3\vvec_3^T + 
\ldots +
\sigma_r\uvec_r\vvec_r^T\text{.}
\end{align*}
</div>
<p id="p-7917">Given two vectors \(\uvec\) and \(\vvec\text{,}\) the matrix \(\uvec~\vvec^T\) is called the <em class="emphasis">outer product</em> of \(\uvec\) and \(\vvec\text{.}\)  (The dot product \(\uvec\cdot\vvec=\uvec^T\vvec\) is sometimes called the <em class="emphasis">inner product</em>.)  An outer product will always be a rank \(1\) matrix so we see above how \(A_k\) is obtained by adding together \(k\) rank \(1\) matrices, each of which gets us one step closer to the original matrix \(A\text{.}\)</p></section><section class="subsection" id="subsection-114"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">7.5.3</span> <span class="title">Principal component analysis</span>
</h3>
<p id="p-7918">In <a href="sec-pca.html" class="internal" title="Section 7.3: Principal Component Analysis">Section 7.3</a>, we explored principal component analysis as a technique to reduce the dimension of a dataset. In particular, we constructed the covariance matrix \(C\) from a demeaned data matrix and saw that the eigenvalues and eigenvectors of \(C\) tell us about the variance of the dataset in different directions.  We referred to the eigenvectors of \(C\) as <em class="emphasis">principal components</em> and found that projecting the data onto a subspace defined by the first few principal components frequently gave us a way to visualize the dataset.  As we added more principal components, we retained more information about the original dataset.  This feels similar to the rank \(k\) approximations we have just seen so let's explore the connection.</p>
<p id="p-7919">Suppose that we have a dataset with \(N\) points, that \(A\) represents the demeaned data matrix, that \(A = U\Sigma V^T\) is a singular value decomposition, and that the singular values are \(A\) are denoted as \(\sigma_i\text{.}\) It follows that the covariance matrix</p>
<div class="displaymath">
\begin{equation*}
C = \frac1N AA^T = \frac1N (U\Sigma V^T) (U\Sigma V^T)^T =
U\left(\frac1N \Sigma \Sigma^T\right) U^T.
\end{equation*}
</div>
<p class="continuation">Notice that \(\frac1N \Sigma\Sigma^T\) is a diagonal matrix whose diagonal entries are \(\frac1N\sigma_i^2\text{.}\)  Therefore, it follows that</p>
<div class="displaymath">
\begin{equation*}
C = 
U\left(\frac1N \Sigma \Sigma^T\right) U^T
\end{equation*}
</div>
<p class="continuation">is an orthgonal diagonalization of \(C\) showing that</p>
<ul class="disc">
<li id="li-5477"><p id="p-7920">the principal components of the dataset, which are the eigenvectors of \(C\text{,}\) are given by the columns of \(U\text{.}\)  In other words, the left singular vectors of \(A\) are the principal components of the dataset.</p></li>
<li id="li-5478">
<p id="p-7921">the variance in the direction of a principal component is the associated eigenvalue of \(C\) and therefore</p>
<div class="displaymath">
\begin{equation*}
V_{\uvec_i} = \frac1N\sigma_i^2.
\end{equation*}
</div>
</li>
</ul>
<article class="activity project-like" id="activity-106"><h6 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.5.4</span><span class="period">.</span>
</h6>
<p id="p-7922">Let's revisit the iris data set that we studied in <a href="sec-pca.html" class="internal" title="Section 7.3: Principal Component Analysis">Section 7.3</a>.  Remember that there are four measurements given for each of 150 irises and that each iris belongs to one of three species.</p>
<p id="p-7923">Evaluating the following cell will load the dataset and define the demeaned data matrix \(A\) whose shape is \(4\times150\text{.}\) <div class="sagecell-sage" id="sage-246"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/pca_iris.py', globals())
df.T
</script></div></p>
<ol class="lower-alpha">
<li id="li-5479"><p id="p-7924">Find the singular values of \(A\) using the command <code class="code-inline tex2jax_ignore">A.singular_values()</code> and use them to determine the variance \(V_{\uvec_j}\) in the direction of each of the four principal components.  What is the fraction of variance retained by the first two principal components? <div class="sagecell-sage" id="sage-247"><script type="text/x-sage">
</script></div></p></li>
<li id="li-5480">
<p id="p-7925">We will now write the matrix \(\Gamma = \Sigma
V^T\) so that \(A = U\Gamma\text{.}\) Suppose that a demeaned data point, say, the 100th column of \(A\text{,}\) is written as a linear combination of principal components:</p>
<div class="displaymath">
\begin{equation*}
\xvec = c_1\uvec_1+c_2\uvec_2+c_3\uvec_3+c_4\uvec_4.
\end{equation*}
</div>
<p class="continuation">Explain why \(\fourvec{c_1}{c_2}{c_3}{c_4}\text{,}\) the vector of coordinates of \(\xvec\) in the basis of principal components, appears as 100th column of \(\Gamma\text{.}\)</p>
</li>
<li id="li-5481"><p id="p-7926">Suppose that we now project this demeaned data point \(\xvec\) orthogonally onto the subspace spanned by the first two principal components \(\uvec_1\) and \(\uvec_2\text{.}\)  What are the coordinates of the projected point in this basis and how can we find them in the matrix \(\Gamma\text{?}\)</p></li>
<li id="li-5482"><p id="p-7927">Alternatively, consider the approximation \(A_2=U_2\Sigma_2V_2^T\) of the demeaned data matrix \(A\text{.}\) Explain why the 100th column of \(A_2\) represents the projection of \(\xvec\) onto the two-dimensional subspace spanned by the first two principal components, \(\uvec_1\) and \(\uvec_2\text{.}\)  Then explain why the coefficients in that projection, \(c_1\uvec_1 + c_2\uvec_2\text{,}\) form the two-dimensional vector \(\twovec{c_1}{c_2}\) that is the 100th column of \(\Gamma_2=\Sigma_2
V_2^T\text{.}\)</p></li>
<li id="li-5483"><p id="p-7928">Now we've seen that the columns of \(\Gamma_2 =
\Sigma_2 V_2^T\) form the coordinates of the demeaned data points projected on to the two-dimensional subspace spanned by \(\uvec_1\) and \(\uvec_2\text{.}\) In the cell below, find a singular value decomposition of \(A\) and use it to form the matrix <code class="code-inline tex2jax_ignore">Gamma2</code>.  When you evaluate this cell, you will see a plot of the projected demeaned data plots, similar to the one we created in <a href="sec-pca.html" class="internal" title="Section 7.3: Principal Component Analysis">Section 7.3</a>. <div class="sagecell-sage" id="sage-248"><script type="text/x-sage"># Form the SVD of A and use it to form Gamma2

		    
Gamma2 = 

# The following will plot the projected demeaned data points
data = Gamma2.columns()
(list_plot(data[:50], color='blue', aspect_ratio=1) +
 list_plot(data[50:100], color='orange') +
 list_plot(data[100:], color='green'))
</script></div></p></li>
</ol></article><p id="p-7941">In our first encounter with principal component analysis, we began with a demeaned data matrix \(A\text{,}\) formed the covariance matrix \(C\text{,}\) and used the eigenvalues and eigenvectors of \(C\) to project the demeaned data onto a smaller dimensional subspace.  In this section, we have seen that a singular value decomposition of \(A\) provides a more direct route: the left singular vectors of \(A\) form the principal components and the approximating matrix \(A_k\) represents the data points projected onto the subspace spanned by the first \(k\) principal components.  The coordinates of a projected demeaned data point are given by the columns of \(\Gamma_k =
\Sigma_kV_k^T\text{.}\)</p></section><section class="subsection" id="subsection-115"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">7.5.4</span> <span class="title">Image compressing and denoising</span>
</h3>
<p id="p-7942">In addition to principal component analysis, the approximations \(A_k\) of a matrix \(A\) obtained from a singular value decomposition can be used in image processing. Remember that we studied the JPEG compression algorithm, whose foundation is the change of basis defined by the Discrete Cosine Transform, in <a href="sec-jpeg.html" class="internal" title="Section 3.3: Image compression">Section 3.3</a>. We will now see how a singular value decomposition provides another tool for both compressing images and removing noise in them.</p>
<article class="activity project-like" id="activity-107"><h6 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.5.5</span><span class="period">.</span>
</h6>
<p id="p-7943">Evaluating the following cell loads some data that we'll use in this activity.  To begin, it defines and displays a \(25\times15\) matrix \(A\text{.}\) <div class="sagecell-sage" id="sage-249"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/svd_compress.py', globals())
print(A)
</script></div></p>
<ol class="lower-alpha">
<li id="li-5494">
<p id="p-7944">If we interpret 0 as black and 1 as white, this matrix represents an image as shown below. <div class="sagecell-sage" id="sage-250"><script type="text/x-sage">display_matrix(A)
</script></div> We will explore how the singular value decomposition helps us to compress this image.</p>
<ol class="lower-roman">
<li id="li-5495"><p id="p-7945">By inspecting the image represented by \(A\text{,}\) identify a basis for \(\col(A)\) and determine \(\rank(A)\text{.}\)</p></li>
<li id="li-5496"><p id="p-7946">The following cell plots the singular values of \(A\text{.}\)  Explain how this plot verifies that the rank is what you found in the previous part. <div class="sagecell-sage" id="sage-251"><script type="text/x-sage">plot_sv(A)
</script></div></p></li>
<li id="li-5497"><p id="p-7947">There is a command <code class="code-inline tex2jax_ignore">approximate(A, k)</code> that creates the approximation \(A_k\text{.}\)  Use the cell below to define \(k\) and look at the images represented by the first few approximations.  What is the smallest value of \(k\) for which \(A=A_k\text{?}\) <div class="sagecell-sage" id="sage-252"><script type="text/x-sage">k = 
display_matrix(approximate(A, k))
</script></div></p></li>
<li id="li-5498">
<p id="p-7948">Now we can see how the singular value decomposition allows us to compress images. Since this is a \(25\times15\) matrix, we need \(25\cdot15=375\) numbers to represent the image.  However, we can also reconstruct the image using a small number of singular values and vectors:</p>
<div class="displaymath">
\begin{equation*}
A = A_k = \sigma_1\uvec_1\vvec_1^T +
\sigma_2\uvec_2\vvec_2^T + \ldots +
\sigma_k\uvec_k\vvec_k^T.
\end{equation*}
</div>
<p class="continuation">What are the dimensions of the singular vectors \(\uvec_i\) and \(\vvec_i\text{?}\)  Between the singular vectors and singular values, how many numbers do we need to reconstruct \(A_k\) for the smallest \(k\) for which \(A=A_k\text{?}\)  This is the compressed size of the image.</p>
</li>
<li id="li-5499"><p id="p-7949">The <em class="emphasis">compression ratio</em> is the ratio of the uncompressed size to the compressed size.  What compression ratio does this represent?</p></li>
</ol>
</li>
<li id="li-5500">
<p id="p-7950">Next we'll explore an example based on a photograph.</p>
<ol class="lower-roman">
<li id="li-5501">
<p id="p-7951">Consider the following image consisting of an array of \(316\times310\) pixels stored in the matrix \(A\text{.}\) <div class="sagecell-sage" id="sage-253"><script type="text/x-sage">A = matrix(RDF, image)
display_image(A)
</script></div></p>
<p id="p-7952">Plot the singular values of \(A\text{.}\) <div class="sagecell-sage" id="sage-254"><script type="text/x-sage">plot_sv(A)
</script></div></p>
</li>
<li id="li-5502">
<p id="p-7953">Use the cell below to study the approximations \(A_k\) for \(k=1, 10, 20, 50, 100\text{.}\) <div class="sagecell-sage" id="sage-255"><script type="text/x-sage">k = 1
display_image(approximate(A, k))
</script></div> Notice how the approximating image \(A_k\) more closely approximates the original image \(A\) as \(k\) increases.</p>
<p id="p-7954">What is the compression ratio when \(k=50\text{?}\) What is the compression ratio when \(k=100\text{?}\) Notice how a higher compression ratio leads to a lower quality reconstruction of the image.</p>
</li>
</ol>
</li>
<li id="li-5503">
<p id="p-7955">A second, related application of the singular value decomposition to image processing is called <em class="emphasis">denoising</em>.  For example, consider the image represented by the matrix \(A\) below. <div class="sagecell-sage" id="sage-256"><script type="text/x-sage">A = matrix(RDF, noise.values)		    
display_matrix(A)
</script></div> This image is similar to the image of the letter "O" we first studied in this activity, but there are splotchy regions in the background that result, perhaps, from scanning the image.  We think of the splotchy regions as noise, and our goal is to improve the quality of the image by reducing the noise.</p>
<ol class="lower-roman">
<li id="li-5504"><p id="p-7956">Plot the singular values below.  How are the singular values of this matrix similar to those represented by the clean image that we considered earlier and how are they different? <div class="sagecell-sage" id="sage-257"><script type="text/x-sage">plot_sv(A)
</script></div></p></li>
<li id="li-5505"><p id="p-7957">There is a natural point where the singular values dramatically decrease so it makes sense to think of the noise as being formed by the small singular values.  To denoise the image, we will therefore replace \(A\) by its approximation \(A_k\text{,}\) where \(k\) is the point at which the singular values drop off. This has the effect of setting the small singular values to zero and hence eliminating the noise.  Choose an appropriate value of \(k\) below and notice that the new image appears to be somewhat cleaned up as a result of removing the noise. <div class="sagecell-sage" id="sage-258"><script type="text/x-sage">k = 
display_matrix(approximate(A, k))
</script></div></p></li>
</ol>
</li>
</ol></article><p id="p-7984">Several examples illustrating how the singular value decomposition compresses images are available at this page from <a class="external" href="http://timbaumann.info/svd-image-compression-demo/" target="_blank">Tim Baumann.</a></p></section><section class="subsection" id="subsection-116"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">7.5.5</span> <span class="title">Analyzing Supreme Court cases</span>
</h3>
<p id="p-7985">As we've seen, a singular value decomposition concentrates the most important features of a matrix into the first singular values and singular vectors.  We will now use this observation to extract meaning from a large dataset giving the voting records of Supreme Court justices.  A similar analysis appears in the paper <a class="external" href="https://www.pnas.org/content/100/13/7432" target="_blank">A pattern analysis of the second Rehnquist U.S. Supreme Court</a> by Lawrence Sirovich.</p>
<p id="p-7986">The makeup of the Supreme Court was unusually stable during a period from 1994-2005 when it was led by Chief Justice William Rehnquist.  This is sometimes called the <em class="emphasis">second Rehnquist court</em>.  The justices during this period were:</p>
<ul class="disc">
<li id="li-5530"><p id="p-7987">William Rehnquist</p></li>
<li id="li-5531"><p id="p-7988">Antonin Scalia</p></li>
<li id="li-5532"><p id="p-7989">Clarence Thomas</p></li>
<li id="li-5533"><p id="p-7990">Anthony Kennedy</p></li>
<li id="li-5534"><p id="p-7991">Sandra Day O'Connor</p></li>
<li id="li-5535"><p id="p-7992">John Paul Stevens</p></li>
<li id="li-5536"><p id="p-7993">David Souter</p></li>
<li id="li-5537"><p id="p-7994">Ruth Bader Ginsburg</p></li>
<li id="li-5538"><p id="p-7995">Stephen Breyer</p></li>
</ul>
<p id="p-7996">During this time, there were 911 cases in which all nine judges voted.  We would like to understand patterns in their voting.</p>
<article class="activity project-like" id="activity-108"><h6 class="heading">
<span class="type">Activity</span><span class="space"> </span><span class="codenumber">7.5.6</span><span class="period">.</span>
</h6>
<p id="p-7997">Evaluating the following cell loads and displays a dataset describing the votes of each justice in these 911 cases. More specifically, an entry of +1 means that the justice represented by the row voted with the majority in the case represented by the column.  An entry of -1 means that justice was in the minority.  This information is also stored in the \(9\times911\) matrix \(A\text{.}\) <div class="sagecell-sage" id="sage-259"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/svd_supreme.py', globals())
A = matrix(RDF, cases.values)
cases
</script></div> The justices are listed, very roughly, in order from more conservative to more progressive.</p>
<p id="p-7998">In this activity, it will be helpful to visualize the entries in various matrices and vectors.  The next cell displays the first 50 columns of the matrix \(A\) with white representing an entry of +1, red representing -1, and black representing 0. <div class="sagecell-sage" id="sage-260"><script type="text/x-sage">display_matrix(A.matrix_from_columns(range(50)))
</script></div></p>
<ol class="lower-alpha">
<li id="li-5539"><p id="p-7999">Plot the singular values of \(A\) below.  Describe the significance of this plot, including the relative contributions from the singular values \(\sigma_k\) as \(k\) increases. <div class="sagecell-sage" id="sage-261"><script type="text/x-sage">plot_sv(A)
</script></div></p></li>
<li id="li-5540"><p id="p-8000">Form the singular value decomposition \(A=U\Sigma
V^T\) and the matrix of coefficients \(\Gamma\) so that \(A=U\Gamma\text{.}\) <div class="sagecell-sage" id="sage-262"><script type="text/x-sage">
</script></div></p></li>
<li id="li-5541"><p id="p-8001">We will now study a particular case, the second case appearing as the column of \(A\) indexed by <code class="code-inline tex2jax_ignore">1</code>. There is a command <code class="code-inline tex2jax_ignore">display_column(A, k)</code> that provides a visual display of the \(k^{th}\) column of a matrix \(A\text{.}\)  Describe the justices' votes in the second case. <div class="sagecell-sage" id="sage-263"><script type="text/x-sage">
</script></div></p></li>
<li id="li-5542"><p id="p-8002">Also, display the first left singular vector \(\uvec_1\text{,}\) the column of \(U\) indexed by \(0\text{,}\) and the column of \(\Gamma\) holding the coefficients that express the second case as a linear combination of left singular vectors. <div class="sagecell-sage" id="sage-264"><script type="text/x-sage">
</script></div> What does this tell us about how the second case is constructed as a linear combination of left singular vectors?  What is the significance of the first left singular vector \(\uvec_1\text{?}\)</p></li>
<li id="li-5543"><p id="p-8003">Let's now study the \(48^{th}\) case, which is represented by the column of \(A\) indexed by <code class="code-inline tex2jax_ignore">47</code>.  Describe the voting pattern in this case. <div class="sagecell-sage" id="sage-265"><script type="text/x-sage">
</script></div></p></li>
<li id="li-5544"><p id="p-8004">Display the second left singular vector \(\uvec_2\) and the vector of coefficients that express the \(48^{th}\) case as a linear combination of left singular vectors. <div class="sagecell-sage" id="sage-266"><script type="text/x-sage">
</script></div> Describe how this case is constructed as a linear combination of singular vectors.  What is the significance of the second left singular vector \(\uvec_2\text{?}\)</p></li>
<li id="li-5545"><p id="p-8005">The data in <a class="xref" data-knowl="./knowl/table-supreme-cases.html" title="Table 7.5.2: Number of cases by vote count">Table 7.5.2</a> describes the number of cases decided by each possible vote count. <figure class="table table-like" id="table-supreme-cases"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">7.5.2<span class="period">.</span></span><span class="space"> </span>Number of cases by vote count</figcaption><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="c m b1 r0 l0 t0 lines">Vote count</td>
<td class="c m b1 r0 l0 t0 lines"># of cases</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">9-0</td>
<td class="c m b0 r0 l0 t0 lines">405</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">8-1</td>
<td class="c m b0 r0 l0 t0 lines">89</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">7-2</td>
<td class="c m b0 r0 l0 t0 lines">111</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">6-3</td>
<td class="c m b0 r0 l0 t0 lines">118</td>
</tr>
<tr>
<td class="c m b0 r0 l0 t0 lines">5-4</td>
<td class="c m b0 r0 l0 t0 lines">188</td>
</tr>
</table></div></figure> How do the singular vectors \(\uvec_1\) and \(\uvec_2\) reflect this data?  Would you characterize the court as leaning toward the conservatives or progressives?  Use these singular vectors to explain your response.</p></li>
<li id="li-5546"><p id="p-8006">Cases decided by a 5-4 vote are often the most impactful as they represent a sharp divide among the justices and, often, society at large.  For that reason, we will now focus on the 5-4 decisions. Evaluating the next cell forms the \(9\times188\) matrix \(B\) consisting of 5-4 decisions. <div class="sagecell-sage" id="sage-267"><script type="text/x-sage">B = matrix(RDF, fivefour.values)
display_matrix(B.matrix_from_columns(range(50)))
</script></div> Form the singular value decomposition of \(B=U\Sigma
V^T\) along with the matrix \(\Gamma\) of coefficients so that \(B=U\Gamma\) and display the first left singular vector \(\uvec_1\text{.}\)  Study how the \(7^{th}\) case, indexed by <code class="code-inline tex2jax_ignore">6</code>, is constructed as a linear combination of left singular vectors. <div class="sagecell-sage" id="sage-268"><script type="text/x-sage">
</script></div> What does this singular vector tell us about the make up of the court and whether it leans towards the conservatives or progressives?</p></li>
<li id="li-5547"><p id="p-8007">Display the second left singular vector \(\uvec_2\) and study how the \(6^{th}\) case, indexed by <code class="code-inline tex2jax_ignore">5</code>, is constructed as a linear combination of left singular vectors. <div class="sagecell-sage" id="sage-269"><script type="text/x-sage">
</script></div> What does \(\uvec_2\) tell us about the relative importance of the justices' voting records?</p></li>
<li id="li-5548"><p id="p-8008">By a <em class="emphasis">swing vote</em>, we mean a justice who is less inclined to vote with a particular bloc of justices but instead swings from one bloc to another with the potential to sway close decisions. What do the singular vectors \(\uvec_1\) and \(\uvec_2\) tell us about the presence of voting blocs on the court and the presence of a swing vote? Which justice represents the swing vote?</p></li>
</ol></article></section><section class="subsection" id="subsection-117"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">7.5.6</span> <span class="title">Summary</span>
</h3>
<p id="p-8031">This section has demonstrated some uses of the singular value decomposition.  Because the singular values appear in decreasing order, the decomposition has the effect of concentrating the most important features of the matrix into the first singular values and singular vectors.</p>
<ul class="disc">
<li id="li-5569"><p id="p-8032">Because the first left singular vectors form an orthonormal basis for \(\col(A)\text{,}\) a singular value decomposition provides a convenient way to project vectors onto \(\col(A)\) and therefore to solve least squares problems.</p></li>
<li id="li-5570">
<p id="p-8033">A singular value decomposition of a rank \(r\) matrix \(A\) leads to a series of approximations \(A_k\) of \(A\) where</p>
<div class="displaymath">
\begin{align*}
A \approx A_1 \amp = \sigma_1\uvec_1\vvec_1^T\\
A \approx A_2 \amp = \sigma_1\uvec_1\vvec_1^T +
\sigma_2\uvec_2\vvec_2^T\\
A \approx A_3 \amp = \sigma_1\uvec_1\vvec_1^T +
\sigma_2\uvec_2\vvec_2^T + 
\sigma_3\uvec_3\vvec_3^T\\
\vdots \amp\\
A = A_r \amp = \sigma_1\uvec_1\vvec_1^T +
\sigma_2\uvec_2\vvec_2^T +
\sigma_3\uvec_3\vvec_3^T +
\ldots +
\sigma_r\uvec_r\vvec_r^T
\end{align*}
</div>
<p class="continuation">In each case, \(A_k\) is the rank \(k\) matrix that is closest to \(A\text{.}\)</p>
</li>
<li id="li-5571"><p id="p-8034">If \(A\) is a demeaned data matrix, the left singular vectors give the principal components of \(A\) and the variance in the direction of a principal component can be easily expressed in terms of the corresponding singular value.</p></li>
<li id="li-5572"><p id="p-8035">The singular value decomposition has many applications. In this section, we looked at how the decomposition is used in image processing through the techniques of compression and denoising.</p></li>
<li id="li-5573"><p id="p-8036">Because the first few left singular vectors contain the most important features of a matrix, we can use a singular value decomposition to extract meaning from a large dataset as we did when analyzing the voting patterns of the second Rehnquist court.</p></li>
</ul></section><section class="exercises" id="exercises-31"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">7.5.7</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-278"><h6 class="heading"><span class="codenumber">1<span class="period">.</span></span></h6>
<p id="p-8037">Suppose that</p>
<div class="displaymath">
\begin{equation*}
A = \begin{bmatrix}
2.1 \amp -1.9 \amp 0.1 \amp 3.7 \\
-1.5 \amp 2.7 \amp 0.9 \amp -0.6 \\
-0.4 \amp 2.8 \amp -1.5 \amp 4.2 \\
-0.4 \amp 2.4 \amp 1.9 \amp -1.8
\end{bmatrix}.
\end{equation*}
</div>
<p class="continuation"><div class="sagecell-sage" id="sage-270"><script type="text/x-sage">
</script></div></p>
<ol class="lower-alpha">
<li id="li-5574"><p id="p-8038">Find the singular values of \(A\text{.}\)  What is \(\rank(A)\text{?}\)</p></li>
<li id="li-5575"><p id="p-8039">Find the sequence of matrices \(A_1\text{,}\) \(A_2\text{,}\) \(A_3\text{,}\) and \(A_4\) where \(A_k\) is the rank \(k\) approximation of \(A\text{.}\)</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-279"><h6 class="heading"><span class="codenumber">2<span class="period">.</span></span></h6>
<p id="p-8048">Suppose we would like to find the best quadratic function</p>
<div class="displaymath">
\begin{equation*}
\beta_0 + \beta_1x + \beta_2x^2=y
\end{equation*}
</div>
<p class="continuation">fitting the points</p>
<div class="displaymath">
\begin{equation*}
(0,1), (1,0), (2,1.5), (3,4), (4,8).
\end{equation*}
</div>
<p class="continuation"><div class="sagecell-sage" id="sage-271"><script type="text/x-sage">
</script></div></p>
<ol class="lower-alpha">
<li id="li-5580"><p id="p-8049">Set up a linear system \(A\xvec = \bvec\) describing the coefficients \(\xvec =
\threevec{\beta_0}{\beta_1}{\beta_2}\text{.}\)</p></li>
<li id="li-5581"><p id="p-8050">Find the singular value decomposition of \(A\text{.}\)</p></li>
<li id="li-5582"><p id="p-8051">Use the singular value decomposition to find the least squares approximate solution \(\xhat\text{.}\)</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-280"><h6 class="heading"><span class="codenumber">3<span class="period">.</span></span></h6>
<p id="p-8060">Remember that the outer product of two vector \(\uvec\) and \(\vvec\) is the matrix \(\uvec~\vvec^T\text{.}\)</p>
<ol class="lower-alpha">
<li id="li-5589">
<p id="p-8061">Suppose that \(\uvec = \twovec{2}{-3}\) and \(\vvec=\threevec201\text{.}\)  Evaluate the outer product \(\uvec~\vvec^T\text{.}\)  To get a clearer sense of how this works, perform this operation without using technology.</p>
<p id="p-8062">How is each of the columns of \(\uvec~\vvec^T\) related to \(\uvec\text{?}\)</p>
</li>
<li id="li-5590"><p id="p-8063">Suppose \(\uvec\) and \(\vvec\) are general vectors.  What is \(\rank(\uvec~\vvec^T)\) and what is a basis for its column space \(\col(\uvec~\vvec^T)\text{?}\)</p></li>
<li id="li-5591"><p id="p-8064">Suppose that \(\uvec\) is a unit vector.  What is the effect of multiplying a vector by the matrix \(\uvec~\uvec^T\text{?}\)</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-281"><h6 class="heading"><span class="codenumber">4<span class="period">.</span></span></h6>
<p id="p-8073">Evaluating the following cell loads in a dataset recording some features of 1057 houses.  Notice how the lot size varies over a relatively small range compared to the other features. For this reason, in addition to demeaning the data, we'll scale each feature by dividing by its standard deviation so that the range of values is similar for each feature.  The matrix \(A\) holds the result. <div class="sagecell-sage" id="sage-272"><script type="text/x-sage">import pandas as pd
df = pd.read_csv('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/data/housing.csv', index_col=0)
df = df.fillna(df.mean())
std = (df-df.mean())/df.std()
A = matrix(std.values).T
df.T
</script></div></p>
<ol class="lower-alpha">
<li id="li-5598"><p id="p-8074">Find the singular values of \(A\) and use them to determine the variance in the direction of the principal components. <div class="sagecell-sage" id="sage-273"><script type="text/x-sage">
</script></div></p></li>
<li id="li-5599"><p id="p-8075">For what fraction of the variance do the first two principal components account?</p></li>
<li id="li-5600"><p id="p-8076">Find a singular value decomposition of \(A\) and construct the matrix \(2\times1057\) matrix \(B\) whose entries are the coordinates of the demeaned data points projected on to the two-dimensional subspace spanned by the first two principal components.  You can plot the projected data points using <code class="code-inline tex2jax_ignore">list_plot(B.columns())</code>. <div class="sagecell-sage" id="sage-274"><script type="text/x-sage">
</script></div></p></li>
<li id="li-5601"><p id="p-8077">Study the entries in the first two principal components \(\uvec_1\) and \(\uvec_2\text{.}\)  Would a more expensive house lie on the left, right, top, or bottom of the plot you constructed?</p></li>
<li id="li-5602"><p id="p-8078">In what ways does a house that lies on the far left of the plot you constructed differ from an average house? In what ways does a house that lies near the top of the plot you constructed differ from an average house?</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-282"><h6 class="heading"><span class="codenumber">5<span class="period">.</span></span></h6>
<p id="p-8092">Let's revisit the voting records of justices on the second Rehnquist court.  Evaluating the following cell will load the voting records of the justices in the 188 cases decided by a 5-4 vote and store them in the matrix \(A\text{.}\) <div class="sagecell-sage" id="sage-275"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/svd_supreme.py', globals())
A = matrix(RDF, fivefour.values)
v = vector(188*[1])
fivefour
</script></div></p>
<ol class="lower-alpha">
<li id="li-5613"><p id="p-8093">The cell above also defined the 188-dimensional vector \(\vvec\) whose entries are all 1.  What does the product \(A\vvec\) represent?  Use the following cell to evaluate this product. <div class="sagecell-sage" id="sage-276"><script type="text/x-sage">
</script></div></p></li>
<li id="li-5614"><p id="p-8094">How does the product \(A\vvec\) tell us which justice voted in the majority most frequently?  What does this say about the presence of a swing vote on the court?</p></li>
<li id="li-5615"><p id="p-8095">How does this product tell us whether we should characterize this court as leaning conservative or progressive?</p></li>
<li id="li-5616"><p id="p-8096">How does this product tell us about the presence of a second swing vote on the court?</p></li>
<li id="li-5617"><p id="p-8097">Study the left singular vector \(\uvec_3\) and describe how it reinforces the fact that there was a second swing vote.  Who was this second swing vote?</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-283"><h6 class="heading"><span class="codenumber">6<span class="period">.</span></span></h6>
<p id="p-8110">The following cell loads a dataset that describes the percentages with which justices on the second Rehnquist court agreed with one another.  For instance, the entry in the first row and second column is 72.78, which means that Justices Rehnquist and Scalia agreed with each other in 72.78% of the cases. <div class="sagecell-sage" id="sage-277"><script type="text/x-sage">sage.repl.load.load('https://raw.githubusercontent.com/davidaustinm/ula_modules/master/svd_supreme.py', globals())
A = 1/100*matrix(RDF, agreement.values)
agreement
</script></div></p>
<ol class="lower-alpha">
<li id="li-5628"><p id="p-8111">Examine the matrix \(A\text{.}\)  What special structure does this matrix have and why should we expect it to have this structure?</p></li>
<li id="li-5629"><p id="p-8112">Plot the singular values of \(A\) below.  For what value of \(k\) would the approximation \(A_k\) be a reasonable approximation of \(A\text{?}\) <div class="sagecell-sage" id="sage-278"><script type="text/x-sage">plot_sv(A)
</script></div></p></li>
<li id="li-5630"><p id="p-8113">Find a singular value decomposition \(A=U\Sigma V^T\) and examine the matrices \(U\) and \(V\) using, for instance, <code class="code-inline tex2jax_ignore">n(U, 3)</code>.  What do you notice about the relationship between \(U\) and \(V\) and why should we expect this relationship to hold? <div class="sagecell-sage" id="sage-279"><script type="text/x-sage">
</script></div></p></li>
<li id="li-5631"><p id="p-8114">The command <code class="code-inline tex2jax_ignore">approximate(A, k)</code> will form the approximating matrix \(A_k\text{.}\)  Study the matrix \(A_1\) using the <code class="code-inline tex2jax_ignore">display_matrix</code> command. Which justice or justices seem to be most agreeable, that is, most likely to agree with other justices? Which justice is least agreeable? <div class="sagecell-sage" id="sage-280"><script type="text/x-sage">
</script></div></p></li>
<li id="li-5632"><p id="p-8115">Examine the difference \(A_2-A_1\) and describe how this tells us about the presence of voting blocs and swing votes on the court. <div class="sagecell-sage" id="sage-281"><script type="text/x-sage">
</script></div></p></li>
</ol></article><article class="exercise exercise-like" id="exercise-284"><h6 class="heading"><span class="codenumber">7<span class="period">.</span></span></h6>
<p id="p-8128">Suppose that \(A=U_r\Sigma_rV_r^T\) is a reduced singular value decomposition of the \(m\times n\) matrix \(A\text{.}\) The matrix \(A^+ = V_r\Sigma_r^{-1}U_r^T\) is called the <em class="emphasis">Moore-Penrose inverse</em> of \(A\text{.}\)</p>
<ol class="lower-alpha">
<li id="li-5643"><p id="p-8129">Explain why \(A^+\) is an \(n\times m\) matrix.</p></li>
<li id="li-5644"><p id="p-8130">If \(A\) is an invertible, square matrix, explain why \(A^+=A^{-1}\text{.}\)</p></li>
<li id="li-5645"><p id="p-8131">Explain why \(AA^+\bvec=\bhat\text{,}\) the orthogonal projection of \(\bvec\) onto \(\col(A)\text{.}\)</p></li>
<li id="li-5646"><p id="p-8132">Explain why \(A^+A\xvec=\xhat\text{,}\) the orthogonal projection of \(\xvec\) onto \(\col(A^T)\text{.}\)</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-285"><h6 class="heading"><span class="codenumber">8<span class="period">.</span></span></h6>
<p id="p-8143">In <a href="sec-gaussian-revisited.html#subsec-partial-pivot" class="internal" title="Subsection 5.1.1: Partial pivoting">Subsection 5.1.1</a>, we saw how some linear algebraic computations are sensitive to round off error made by a computer.  A singular value decomposition can help us understand when this situation can occur.</p>
<p id="p-8144">For instance, consider the matrices</p>
<div class="displaymath">
\begin{equation*}
A = \begin{bmatrix}
1.0001 \amp 1 \\
1 \amp 1 \\
\end{bmatrix},\hspace{24pt}
B = \begin{bmatrix}
1 \amp 1 \\
1 \amp 1 \\
\end{bmatrix}.
\end{equation*}
</div>
<p class="continuation">The entries in these matrices are quite close to one another, but \(A\) is invertible while \(B\) is not.  It seems like \(A\) is <em class="emphasis">almost</em> singular. In fact, we can measure how close a matrix is to being singular by forming the <em class="emphasis">condition number</em>, \(\sigma_1/\sigma_n\text{,}\) the ratio of the largest to smallest singular value.  If \(A\) were singular, the condition number would be undefined because the singular value \(\sigma_n=0\text{.}\)  Therefore, we will think of matrices with large condition numbers as being close to singular.</p>
<ol class="lower-alpha">
<li id="li-5655"><p id="p-8145">Define the matrix \(A\) and find a singular value decomposition.  What is the condition number of \(A\text{?}\) <div class="sagecell-sage" id="sage-282"><script type="text/x-sage">
</script></div></p></li>
<li id="li-5656">
<p id="p-8146">Define the left singular vectors \(\uvec_1\) and \(\uvec_2\text{.}\)  Compare the results \(A^{-1}\bvec\) when</p>
<ol class="lower-roman">
<li id="li-5657"><p id="p-8147">\(\bvec=\uvec_1+\uvec_2\text{.}\)</p></li>
<li id="li-5658"><p id="p-8148">\(\bvec=2\uvec_1+\uvec_2\text{.}\)</p></li>
</ol>
<p class="continuation">Notice how a small change in the vector \(\bvec\) leads to a small change in \(A^{-1}\bvec\text{.}\)</p>
</li>
<li id="li-5659">
<p id="p-8149">Now compare the results \(A^{-1}\bvec\) when</p>
<ol class="lower-roman">
<li id="li-5660"><p id="p-8150">\(\bvec=\uvec_1+\uvec_2\text{.}\)</p></li>
<li id="li-5661"><p id="p-8151">\(\bvec=\uvec_1+2\uvec_2\text{.}\)</p></li>
</ol>
<p class="continuation">Notice now how a small change in \(\bvec\) leads to a large change in \(A^{-1}\bvec\text{.}\)</p>
</li>
<li id="li-5662">
<p id="p-8152">Previously, we saw that, if we write \(\xvec\) in terms of left singular vectors \(\xvec=c_1\vvec_1+c_2\vvec_2\text{,}\) then we have</p>
<div class="displaymath">
\begin{equation*}
\bvec=A\xvec = c_1\sigma_1\uvec_1 +
c_2\sigma_2\uvec_2.
\end{equation*}
</div>
<p class="continuation">If we write \(\bvec=d_1\uvec_1+d_2\uvec_2\text{,}\) explain why \(A^{-1}\bvec\) is sensitive to small changes in \(d_2\text{.}\)</p>
</li>
</ol>
<p class="continuation">Generally speaking, a square matrix \(A\) with a large condition number will demonstrate this type of behavior so that the computation of \(A^{-1}\) is likely to be affected by round off error.  We call such a matrix <em class="emphasis">ill-conditioned</em>.</p></article></section></section></div></main>
</div>
</body>
</html>
